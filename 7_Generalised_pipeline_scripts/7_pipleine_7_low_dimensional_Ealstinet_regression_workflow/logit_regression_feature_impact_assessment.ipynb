{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "raised-difference",
   "metadata": {},
   "source": [
    "# LR feature impact assessment module\n",
    "\n",
    "##### Ver:: A2_V4\n",
    "##### Author(s) : Issac Goh\n",
    "##### Date : 221210;YYMMDD\n",
    "### Author notes\n",
    "    - Added ability to process feature impact assesment for low-dim models\n",
    "### Features to add\n",
    "    - Feature assessment weighted by classifications made in query data based on bayes factor of variable expression \n",
    "### Modes to run in\n",
    "    - Should be automated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "difficult-deficit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "#from geosketch import gs\n",
    "from numpy import cov\n",
    "import scipy.cluster.hierarchy as spc\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import psutil\n",
    "import random\n",
    "import threading\n",
    "import tracemalloc\n",
    "import itertools\n",
    "import math\n",
    "import warnings\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "technological-shift",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "'pan_fetal':'/nfs/team205/ig7/resources/scripts_dont_modify/logit_regression_models/adifa_lr/celltypist_model.Pan_Fetal_Human.pkl',\n",
    "'pan_fetal_wget':'https://celltypist.cog.sanger.ac.uk/models/Pan_Fetal_Suo/v2/Pan_Fetal_Human.pkl',\n",
    "'adata_scvi':'/nfs/team205/ig7/mount/gdrive/g_cloud/projects/amniontic_fluid/scvi_low_dim_model.sav',\n",
    "'adata_ldvae':'/nfs/team205/ig7/mount/gdrive/g_cloud/projects/amniontic_fluid/ldvae_low_dim_model.sav',\n",
    "'adata_harmony':'/nfs/team205/ig7/work_backups/backup_210306/projects/amiotic_fluid/train_low_dim_model/organ_low_dim_model.sav',\n",
    "'test_low_dim_ipsc_ys':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/YS_030522_notebooks/Integrating_HM_data_030522/YS_logit/lr_model.sav',\n",
    "'YS_X':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/resources/YS_X_model_080922.sav',\n",
    "'YS_X_V3':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/rebuttal_figs_010922/train_YS_full_X_model/YS_X_A2_V12_lvl3_ELASTICNET_YS.sav',\n",
    "'SK_model':'/nfs/team205/ig7/resources/scripts_dont_modify/logit_regression_models/LR_app_format/hudaa_skin/for_hudaa_A1_V2',\n",
    "'Hudaa_model_trained':'/nfs/team298/hg6/Fetal_skin/LR_15012023/train-all_model.pkl',\n",
    "'low_dim_sk_model':'/nfs/team205/ig7/resources/scripts_dont_modify/logit_regression_models/LR_app_format/hudaa_skin/SK_model_lowdim',\n",
    "'low_dim_ldvae_model_YS_cross_organ':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/YS_data/YS_panf_gonads_brain_eliv_combined_060922/ldVAE_model_projections/YS_v3_macs_mod_ldvae_panf_model'\n",
    "}\n",
    "\n",
    "adatas_dict = {\n",
    "'Fetal_skin_raw': '/nfs/team298/hg6/Fetal_skin/data/FS_raw_sub.h5ad',\n",
    "'vascular_organoid': '/nfs/team298/hg6/Fetal_skin/data/vasc_org_raw.h5ad',\n",
    "'YS':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/YS_data/Submission_2_data/A2_V5_scvi_YS_integrated/A2_V5_scvi_YS_integrated_raw_qc_scr_umap.h5ad',\n",
    "'YS_test':'/nfs/team205/ig7/resources/scripts_dont_modify/logit_regression_models/LR_app_format/ys_test_data.h5ad',\n",
    "'YS_A2_V10_X_raw':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/YS_data/Submission_2_data/A2_V10_scvi_YS_integrated/A2_V10_raw_counts_full_no_obs.h5ad',\n",
    "'YS_A2_V10_X':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/YS_data/Submission_2_data/A2_V10_scvi_YS_integrated/A2_V10_qc_raw.h5ad',\n",
    "'pan_f_YS_A1_V10':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/YS_data/YS_panf_gonads_brain_eliv_combined_060922/A1_Vx_pan_organ_integrations/A1_V10_raw_scvi_YS_updated_panf_gonads_brain_build_donor_organ_corrected_031022.h5ad',\n",
    "'pan_f_YS_A1_V10_high_var_ldvae':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/YS_data/YS_panf_gonads_brain_eliv_combined_060922/ldVAE_model_projections/scvi_LDVAE_panf_pan_immune/A1_V10_raw_high_var_scvi_YS_updated_panf_gonads_brain_build_donor_organ_corrected_031022.h5ad',\n",
    "'pan_f_YS_A1_V12_high_var_ldvae':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/YS_data/YS_panf_gonads_brain_eliv_combined_060922/ldVAE_model_projections/scvi_LDVAE_panf_pan_immune/A1_V12_raw_high_var_scvi_YS_updated_panf_gonads_brain_build_donor_organ_corrected_031022.h5ad'\n",
    "}\n",
    "\n",
    "# Variable assignment\n",
    "feat_use = 'LVL3'\n",
    "model_key = 'ldvae_v3_panf_model'#'test_low_dim_ipsc_ys'# key for model of choice can be either url or local \n",
    "train_x_partition = 'X_scvi' # what partition was the data trained on? To keep simple, for now only accepts 'X'\n",
    "dyn_std = 1.96 # Dynamic cutoffs using std of the mean for each celltype probability, gives a column notifying user of uncertain labels 1 == 68Ci, 1.96 = 95CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sorted-taste",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(model_dict,model_run):\n",
    "    if (Path(model_dict[model_run])).is_file():\n",
    "        # Load data (deserialize)\n",
    "        model = pkl.load(open(model_dict[model_run], \"rb\"))\n",
    "        return model\n",
    "    elif 'http' in model_dict[model_run]:\n",
    "        print('Loading model from web source')\n",
    "        r_get = requests.get(model_dict[model_run])\n",
    "        fpath = './model_temp.sav'\n",
    "        open(fpath , 'wb').write(r_get.content)\n",
    "        model = pkl.load(open(fpath, \"rb\"))\n",
    "        return model\n",
    "\n",
    "def load_adatas(adatas_dict,data_merge, data_key_use,QC_normalise):\n",
    "    if data_merge == True:\n",
    "        # Read\n",
    "        gene_intersect = {} # unused here\n",
    "        adatas = {}\n",
    "        for dataset in adatas_dict.keys():\n",
    "            if 'https' in adatas_dict[dataset]:\n",
    "                print('Loading anndata from web source')\n",
    "                adatas[dataset] = sc.read('./temp_adata.h5ad',backup_url=adatas_dict[dataset])\n",
    "            adatas[dataset] = sc.read(data[dataset])\n",
    "            adatas[dataset].var_names_make_unique()\n",
    "            adatas[dataset].obs['dataset_merge'] = dataset\n",
    "            adatas[dataset].obs['dataset_merge'] = dataset\n",
    "            gene_intersect[dataset] = list(adatas[dataset].var.index)\n",
    "        adata = list(adatas.values())[0].concatenate(list(adatas.values())[1:],join='inner')\n",
    "        return adatas, adata\n",
    "    elif data_merge == False:\n",
    "        if 'https' in adatas_dict[data_key_use]:\n",
    "            print('Loading anndata from web source')\n",
    "            adata = sc.read('./temp_adata.h5ad',backup_url=adatas_dict[data_key_use])\n",
    "        else: \n",
    "            adata = sc.read(adatas_dict[data_key_use])\n",
    "    if QC_normalise == True:\n",
    "        print('option to apply standardisation to data detected, performing basic QC filtering')\n",
    "        sc.pp.filter_cells(adata, min_genes=200)\n",
    "        sc.pp.filter_genes(adata, min_cells=3)\n",
    "        sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4)\n",
    "        sc.pp.log1p(adata)\n",
    "        \n",
    "    return adata\n",
    "\n",
    "# resource usage logger\n",
    "class DisplayCPU(threading.Thread):\n",
    "    def run(self):\n",
    "        tracemalloc.start()\n",
    "        starting, starting_peak = tracemalloc.get_traced_memory()\n",
    "        self.running = True\n",
    "        self.starting = starting\n",
    "        currentProcess = psutil.Process()\n",
    "        cpu_pct = []\n",
    "        peak_cpu = 0\n",
    "        while self.running:\n",
    "            peak_cpu = 0\n",
    "#           time.sleep(3)\n",
    "#             print('CPU % usage = '+''+ str(currentProcess.cpu_percent(interval=1)))\n",
    "#             cpu_pct.append(str(currentProcess.cpu_percent(interval=1)))\n",
    "            cpu = currentProcess.cpu_percent()\n",
    "        # track the peak utilization of the process\n",
    "            if cpu > peak_cpu:\n",
    "                peak_cpu = cpu\n",
    "                peak_cpu_per_core = peak_cpu/psutil.cpu_count()\n",
    "        self.peak_cpu = peak_cpu\n",
    "        self.peak_cpu_per_core = peak_cpu_per_core\n",
    "        \n",
    "    def stop(self):\n",
    "#        cpu_pct = DisplayCPU.run(self)\n",
    "        self.running = False\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        tracemalloc.stop()\n",
    "        return current, peak\n",
    "    \n",
    "# projection module\n",
    "def reference_projection(adata, model, dyn_std,partial_scale):\n",
    "    \n",
    "    class adata_temp:\n",
    "        pass\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    print('Determining model flavour')\n",
    "    try:\n",
    "        model_lr =  model['Model']\n",
    "        print('Consuming celltypist model')\n",
    "    except:# hasattr(model, 'coef_'):\n",
    "        print('Consuming non-celltypist model')\n",
    "        model_lr =  model\n",
    "    print(model_lr)\n",
    "    \n",
    "#     model_lr =  model['Model']\n",
    "\n",
    "    if train_x_partition == 'X':\n",
    "        print('Matching reference genes in the model')\n",
    "        k_x = np.isin(list(adata.var.index), list(model_lr.features))\n",
    "        if k_x.sum() == 0:\n",
    "            raise ValueError(f\"🛑 No features overlap with the model. Please provide gene symbols\")\n",
    "        print(f\"🧬 {k_x.sum()} features used for prediction\")\n",
    "        #slicing adata\n",
    "        k_x_idx = np.where(k_x)[0]\n",
    "        # adata_temp = adata[:,k_x_idx]\n",
    "        adata_temp.var = adata[:,k_x_idx].var\n",
    "        adata_temp.X = adata[:,k_x_idx].X\n",
    "        adata_temp.obs = adata[:,k_x_idx].obs\n",
    "        lr_idx = pd.DataFrame(model_lr.features, columns=['features']).reset_index().set_index('features').loc[list(adata_temp.var.index)].values\n",
    "        # adata_arr = adata_temp.X[:,list(lr_idexes['index'])]\n",
    "\n",
    "        # slice and reorder model\n",
    "        ni, fs, cf = model_lr.n_features_in_, model_lr.features, model_lr.coef_\n",
    "        model_lr.n_features_in_ = lr_idx.size\n",
    "        model_lr.features = np.array(model_lr.features)[lr_idx]\n",
    "        model_lr.coef_ = np.squeeze(model_lr.coef_[:,lr_idx]) #model_lr.coef_[:, lr_idx]\n",
    "        \n",
    "        if partial_scale == True:\n",
    "            print('scaling input data, default option is to use incremental learning and fit in mini bulks!')\n",
    "            # Partial scaling alg\n",
    "            scaler = StandardScaler(with_mean=False)\n",
    "            n = adata_temp.X.shape[0]  # number of rows\n",
    "            # set dyn scale packet size\n",
    "            x_len = len(adata_temp.var)\n",
    "            y_len = len(adata.obs)\n",
    "            if y_len < 100000:\n",
    "                dyn_pack = int(x_len/10)\n",
    "                pack_size = dyn_pack\n",
    "            else:\n",
    "                # 10 pack for every 100,000\n",
    "                dyn_pack = int((y_len/100000)*10)\n",
    "                pack_size = int(x_len/dyn_pack)\n",
    "\n",
    "            batch_size =  1000#pack_size#500  # number of rows in each call to partial_fit\n",
    "            index = 0  # helper-var\n",
    "            while index < n:\n",
    "                partial_size = min(batch_size, n - index)  # needed because last loop is possibly incomplete\n",
    "                partial_x = adata_temp.X[index:index+partial_size]\n",
    "                scaler.partial_fit(partial_x)\n",
    "                index += partial_size\n",
    "            adata_temp.X = scaler.transform(adata_temp.X)\n",
    "    else:\n",
    "        adata_temp = adata[:]\n",
    "    # model projections\n",
    "    print('Starting reference projection!')\n",
    "    if train_x_partition == 'X':\n",
    "        train_x = adata_temp.X\n",
    "        pred_out = pd.DataFrame(model_lr.predict(train_x),columns = ['predicted'],index = list(adata.obs.index))\n",
    "        proba =  pd.DataFrame(model_lr.predict_proba(train_x),columns = model_lr.classes_,index = list(adata.obs.index))\n",
    "        pred_out = pred_out.join(proba)\n",
    "        \n",
    "    elif train_x_partition in list(adata_temp.obsm.keys()): \n",
    "        print('{low_dim: this partition modality is still under development!}')\n",
    "        train_x = adata_temp.obsm[train_x_partition][:]\n",
    "        pred_out = pd.DataFrame(model_lr.predict(train_x),columns = ['predicted'],index = list(adata.obs.index))\n",
    "        proba =  pd.DataFrame(model_lr.predict_proba(train_x),columns = model_lr.classes_,index = list(adata.obs.index))\n",
    "        pred_out = pred_out.join(proba)\n",
    "\n",
    "    \n",
    "    else:\n",
    "        print('{this partition modality is still under development!}')\n",
    "        print('Warning! No obsm partition detected! defaulting to PCA, if this is not self-projection, do not use the results!')\n",
    "        if not 'X_pca' in adata.obsm.keys():\n",
    "            print('performing highly variable gene selection')\n",
    "            sc.pp.highly_variable_genes(adata_temp, batch_key = batch_key, subset=False)\n",
    "            sc.pp.pca(adata_temp, n_comps=100, use_highly_variable=True, svd_solver='arpack')\n",
    "            sc.pl.pca_variance_ratio(adata_temp, log=True,n_pcs=100)\n",
    "            train_x = adata_temp.obsm['X_pca'][:]\n",
    "            pred_out = pd.DataFrame(model_lr.predict(train_x),columns = ['predicted'],index = list(adata.obs.index))\n",
    "            proba =  pd.DataFrame(model_lr.predict_proba(train_x),columns = model_lr.classes_,index = list(adata.obs.index))\n",
    "            pred_out = pred_out.join(proba)\n",
    "\n",
    "    # Simple dynamic confidence calling\n",
    "    pred_out['confident_calls'] = pred_out['predicted']\n",
    "    pred_out.loc[pred_out.max(axis=1)<(pred_out.mean(axis=1) + (1*pred_out.std(axis=1))),'confident_calls'] = pred_out.loc[pred_out.max(axis=1)<(pred_out.mean(axis=1) + (1*pred_out.std(axis=1))),'confident_calls'].astype(str) + '_uncertain'\n",
    "    # means_ = self.model.scaler.mean_[lr_idx] if self.model.scaler.with_mean else 0\n",
    "    return(pred_out,train_x,model_lr,adata_temp)\n",
    "\n",
    "def freq_redist_68CI(adata,clusters_reassign):\n",
    "    if freq_redist != False:\n",
    "        print('Frequency redistribution commencing')\n",
    "        cluster_prediction = \"consensus_clus_prediction\"\n",
    "        lr_predicted_col = 'predicted'\n",
    "        pred_out[clusters_reassign] = adata.obs[clusters_reassign].astype(str)\n",
    "        reassign_classes = list(pred_out[clusters_reassign].unique())\n",
    "        lm = 1 # lambda value\n",
    "        pred_out[cluster_prediction] = pred_out[clusters_reassign]\n",
    "        for z in pred_out[clusters_reassign][pred_out[clusters_reassign].isin(reassign_classes)].unique():\n",
    "            df = pred_out\n",
    "            df = df[(df[clusters_reassign].isin([z]))]\n",
    "            df_count = pd.DataFrame(df[lr_predicted_col].value_counts())\n",
    "            # Look for classificationds > 68CI\n",
    "            if len(df_count) > 1:\n",
    "                df_count_temp = df_count[df_count[lr_predicted_col]>int(int(df_count.mean()) + (df_count.std()*lm))]\n",
    "                if len(df_count_temp >= 1):\n",
    "                    df_count = df_count_temp\n",
    "            #print(df_count)     \n",
    "            freq_arranged = df_count.index\n",
    "            cat = freq_arranged[0]\n",
    "        #Make the cluster assignment first\n",
    "            pred_out[cluster_prediction] = pred_out[cluster_prediction].astype(str)\n",
    "            pred_out.loc[pred_out[clusters_reassign] == z, [cluster_prediction]] = cat\n",
    "        # Create assignments for any classification >68CI\n",
    "            for cats in freq_arranged:\n",
    "                #print(cats)\n",
    "                cats_assignment = cats#.replace(data1,'') + '_clus_prediction'\n",
    "                pred_out.loc[(pred_out[clusters_reassign] == z) & (pred_out[lr_predicted_col] == cats),[cluster_prediction]] = cats_assignment\n",
    "        min_counts = pd.DataFrame((pred_out[cluster_prediction].value_counts()))\n",
    "        reassign = list(min_counts.index[min_counts[cluster_prediction]<=2])\n",
    "        pred_out[cluster_prediction] = pred_out[cluster_prediction].str.replace(str(''.join(reassign)),str(''.join(pred_out.loc[pred_out[clusters_reassign].isin(list(pred_out.loc[(pred_out[cluster_prediction].isin(reassign)),clusters_reassign])),lr_predicted_col].value_counts().head(1).index.values)))\n",
    "        return pred_out\n",
    "\n",
    "### Feature importance notes\n",
    "#- If we increase the x feature one unit, then the prediction will change e to the power of its weight. We can apply this rule to the all weights to find the feature importance.\n",
    "#- We will calculate the Euler number to the power of its coefficient to find the importance.\n",
    "#- To sum up an increase of x feature by one unit increases the odds of being versicolor class by a factor of x[importance] when all other features remain the same.\n",
    "\n",
    "#- For low-dim, we look at the distribution of e^coef per class, we extract the \n",
    "\n",
    "\n",
    "# class coef_extract:\n",
    "#     def __init__(self, model,features, pos):\n",
    "# #         self.w = list(itertools.chain(*(model.coef_[pos]).tolist())) #model.coef_[pos]\n",
    "#         self.w = model.coef_[class_pred_pos]\n",
    "#         self.features = features \n",
    "\n",
    "def long_format_features(top_loadings):\n",
    "    p = top_loadings.loc[:, top_loadings.columns.str.endswith(\"_e^coef\")]\n",
    "    p = pd.melt(p)\n",
    "    n = top_loadings.loc[:, top_loadings.columns.str.endswith(\"_feature\")]\n",
    "    n = pd.melt(n)\n",
    "    l = top_loadings.loc[:, top_loadings.columns.str.endswith(\"_coef\")]\n",
    "    l = pd.melt(l)\n",
    "    n = n.replace(regex=r'_feature', value='')\n",
    "    n = n.rename(columns={\"variable\": \"class\", \"value\": \"feature\"})\n",
    "    p = (p.drop([\"variable\"],axis = 1)).rename(columns={ \"value\": \"e^coef\"})\n",
    "    l = (l.drop([\"variable\"],axis = 1)).rename(columns={ \"value\": \"coef\"})\n",
    "    concat = pd.concat([n,p,l],axis=1)\n",
    "    return concat\n",
    "\n",
    "def model_feature_sf(long_format_feature_importance, coef_use):\n",
    "        long_format_feature_importance[str(coef_use) + '_pval'] = 'NaN'\n",
    "        for class_lw in long_format_feature_importance['class'].unique():\n",
    "            df_loadings = long_format_feature_importance[long_format_feature_importance['class'].isin([class_lw])]\n",
    "            comps = coef_use #'e^coef'\n",
    "            U = np.mean(df_loadings[comps])\n",
    "            std = np.std(df_loadings[comps])\n",
    "            med =  np.median(df_loadings[comps])\n",
    "            mad = np.median(np.absolute(df_loadings[comps] - np.median(df_loadings[comps])))\n",
    "            # Survival function scaled by 1.4826 of MAD (approx norm)\n",
    "            pvals = scipy.stats.norm.sf(df_loadings[comps], loc=med, scale=1.4826*mad) # 95% CI of MAD <10,000 samples\n",
    "            #pvals = scipy.stats.norm.sf(df_loadings[comps], loc=U, scale=1*std)\n",
    "            df_loadings[str(comps) +'_pval'] = pvals\n",
    "            long_format_feature_importance.loc[long_format_feature_importance.index.isin(df_loadings.index)] = df_loadings\n",
    "        long_format_feature_importance['is_significant_sf'] = False\n",
    "        long_format_feature_importance.loc[long_format_feature_importance[coef_use+ '_pval']<0.05,'is_significant_sf'] = True\n",
    "        return long_format_feature_importance\n",
    "# Apply SF to e^coeff mat data\n",
    "#         pval_mat = pd.DataFrame(columns = mat.columns)\n",
    "#         for class_lw in mat.index:\n",
    "#             df_loadings = mat.loc[class_lw]\n",
    "#             U = np.mean(df_loadings)\n",
    "#             std = np.std(df_loadings)\n",
    "#             med =  np.median(df_loadings)\n",
    "#             mad = np.median(np.absolute(df_loadings - np.median(df_loadings)))\n",
    "#             pvals = scipy.stats.norm.sf(df_loadings, loc=med, scale=1.96*U)\n",
    "\n",
    "class estimate_important_features: # This calculates feature effect sizes of the model\n",
    "    def __init__(self, model, top_n):\n",
    "        print('Estimating feature importance')\n",
    "        classes =  list(model.classes_)\n",
    "         # get feature names\n",
    "        try:\n",
    "            model_features = list(itertools.chain(*list(model.features)))\n",
    "        except:\n",
    "            warnings.warn('no features recorded in data, naming features by position')\n",
    "            print('if low-dim lr was submitted, run linear decoding function to obtain true feature set')\n",
    "            model_features = list(range(0,model.coef_.shape[1]))\n",
    "            model.features = model_features\n",
    "        print('Calculating the Euler number to the power of coefficients')\n",
    "        impt_ = pow(math.e,model.coef_)\n",
    "        try:\n",
    "            self.euler_pow_mat = pd.DataFrame(impt_,columns = list(itertools.chain(*list(model.features))),index = list(model.classes_))\n",
    "        except:\n",
    "            self.euler_pow_mat = pd.DataFrame(impt_,columns = list(model.features),index = list(model.classes_))\n",
    "        self.top_n_features = pd.DataFrame(index = list(range(0,top_n)))\n",
    "        # estimate per class feature importance\n",
    "        \n",
    "        print('Estimating feature importance for each class')\n",
    "        mat = self.euler_pow_mat\n",
    "        for class_pred_pos in list(range(0,len(mat.T.columns))):\n",
    "            class_pred = list(mat.T.columns)[class_pred_pos]\n",
    "            #     print(class_pred)\n",
    "            temp_mat =  pd.DataFrame(mat.T[class_pred])\n",
    "            temp_mat['coef'] = model.coef_[class_pred_pos]\n",
    "            temp_mat = temp_mat.sort_values(by = [class_pred], ascending=False)\n",
    "            temp_mat = temp_mat.reset_index()\n",
    "            temp_mat.columns = ['feature','e^coef','coef']\n",
    "            temp_mat = temp_mat[['feature','e^coef','coef']]\n",
    "            temp_mat.columns =str(class_pred)+ \"_\" + temp_mat.columns\n",
    "            self.top_n_features = pd.concat([self.top_n_features,temp_mat.head(top_n)], join=\"inner\",ignore_index = False, axis=1)\n",
    "            self.to_n_features_long = model_feature_sf(long_format_features(self.top_n_features),'e^coef')\n",
    "            \n",
    "    \n",
    "    # plot class-wise features\n",
    "def model_class_feature_plots(top_loadings, classes, comps):\n",
    "    import matplotlib.pyplot as plt\n",
    "    for class_temp in classes:\n",
    "        class_lw = class_temp\n",
    "        long_format = top_loadings\n",
    "        df_loadings = long_format[long_format['class'].isin([class_lw])]\n",
    "        plt.hist(df_loadings[comps])\n",
    "        for i in ((df_loadings[comps][df_loadings[str(comps) +'_pval']<0.05]).unique()):\n",
    "            plt.axvline(x=i,color='red')\n",
    "        med = np.median(df_loadings[comps])\n",
    "        plt.axvline(x=med,color='blue')\n",
    "        plt.xlabel('feature_importance', fontsize=12)\n",
    "        plt.title(class_lw)\n",
    "        #plt.axvline(x=med,color='pink')\n",
    "        df_loadings[comps][df_loadings[str(comps) +'_pval']<0.05]\n",
    "        print(len(df_loadings[comps][df_loadings[str(comps) +'_pval']<0.05]))\n",
    "        #Plot feature ranking\n",
    "        plot_loading = pd.DataFrame(pd.DataFrame(df_loadings[comps][df_loadings[str(comps) +'_pval']<0.05]).iloc[:,0].sort_values(ascending=False))\n",
    "        table = plt.table(cellText=plot_loading.values,colWidths = [1]*len(plot_loading.columns),\n",
    "        rowLabels= list(df_loadings['feature'][df_loadings.index.isin(plot_loading.index)].reindex(plot_loading.index)), #plot_loading.index,\n",
    "        colLabels=plot_loading.columns,\n",
    "        cellLoc = 'center', rowLoc = 'center',\n",
    "        loc='right', bbox=[1.4, -0.05, 0.5,1])\n",
    "        table.scale(1, 2)\n",
    "        table.set_fontsize(10)\n",
    "        \n",
    "def report_f1(model,train_x, train_label):\n",
    "    ## Report accuracy score\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "    from sklearn import metrics\n",
    "    import seaborn as sn\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # cv = RepeatedStratifiedKFold(n_splits=2, n_repeats=2, random_state=1)\n",
    "    # # evaluate the model and collect the scores\n",
    "    # n_scores = cross_val_score(lr, train_x, train_label, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    # # report the model performance\n",
    "    # print('Mean Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))\n",
    "\n",
    "    # Report Precision score\n",
    "    metric = pd.DataFrame((metrics.classification_report(train_label, model.predict(train_x), digits=2,output_dict=True))).T\n",
    "    cm = confusion_matrix(train_label, model.predict(train_x))\n",
    "    #cm = confusion_matrix(train_label, model.predict_proba(train_x))\n",
    "    df_cm = pd.DataFrame(cm, index = model.classes_,columns = model.classes_)\n",
    "    df_cm = (df_cm / df_cm.sum(axis=0))*100\n",
    "    plt.figure(figsize = (20,20))\n",
    "    sn.set(font_scale=1) # for label size\n",
    "    pal = sns.diverging_palette(240, 10, n=10)\n",
    "    #plt.suptitle(('Mean Accuracy 5 fold: %.3f std: %.3f' % (np.mean(n_scores),  np.std(n_scores))), y=1.05, fontsize=18)\n",
    "    #Plot precision recall and recall\n",
    "    table = plt.table(cellText=metric.values,colWidths = [1]*len(metric.columns),\n",
    "    rowLabels=metric.index,\n",
    "    colLabels=metric.columns,\n",
    "    cellLoc = 'center', rowLoc = 'center',\n",
    "    loc='bottom', bbox=[0.25, -0.6, 0.5, 0.3])\n",
    "    table.scale(1, 2)\n",
    "    table.set_fontsize(5)\n",
    "    g = sn.heatmap(df_cm, annot=False, annot_kws={\"size\": 16},cmap=pal) # font size\n",
    "    print(metrics.classification_report(train_label, model.predict(train_x), digits=2))\n",
    "    plt.show()\n",
    "    return metric\n",
    "    \n",
    "    \n",
    "def subset_top_hvgs(adata_lognorm, n_top_genes):\n",
    "    dispersion_norm = adata_lognorm.var['dispersions_norm'].values.astype('float32')\n",
    "\n",
    "    dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]\n",
    "    dispersion_norm[\n",
    "                ::-1\n",
    "            ].sort()  # interestingly, np.argpartition is slightly slower\n",
    "\n",
    "    disp_cut_off = dispersion_norm[n_top_genes - 1]\n",
    "    gene_subset = adata_lognorm.var['dispersions_norm'].values >= disp_cut_off\n",
    "    return(adata_lognorm[:,gene_subset])\n",
    "\n",
    "def prep_scVI(adata, \n",
    "              n_hvgs = 5000,\n",
    "              remove_cc_genes = True,\n",
    "              remove_tcr_bcr_genes = False\n",
    "             ):\n",
    "    ## Remove cell cycle genes\n",
    "    if remove_cc_genes:\n",
    "        adata = panfetal_utils.remove_geneset(adata,genes.cc_genes)\n",
    "\n",
    "    ## Remove TCR/BCR genes\n",
    "    if remove_tcr_bcr_genes:\n",
    "        adata = panfetal_utils.remove_geneset(adata, genes.IG_genes)\n",
    "        adata = panfetal_utils.remove_geneset(adata, genes.TCR_genes)\n",
    "        \n",
    "    ## HVG selection\n",
    "    adata = subset_top_hvgs(adata, n_top_genes=n_hvgs)\n",
    "    return(adata)\n",
    "\n",
    "# Modified LR train module, does not work with low-dim by default anymore, please use low-dim adapter\n",
    "def LR_train(adata, train_x, train_label, penalty='elasticnet', sparcity=0.2,max_iter=200,l1_ratio =0.2,tune_hyper_params =False,n_splits=5, n_repeats=3,l1_grid = [0.05,0.2,0.5], c_grid = [0.05,0.2,0.4],sketch_obsm =None):\n",
    "    if tune_hyper_params == True:\n",
    "        train_labels = train_label\n",
    "        results,adata_tuned = tune_lr_model(adata, train_x_partition = train_x, random_state = 42,  train_labels = train_labels, n_splits=n_splits, n_repeats=n_repeats,l1_grid = l1_grid, c_grid = c_grid,sketch_obsm = sketch_obsm)\n",
    "        print('hyper_params tuned')\n",
    "        sparcity = results.best_params_['C']\n",
    "        l1_ratio = results.best_params_['l1_ratio']\n",
    "        \n",
    "    if not sketch_obsm == None:\n",
    "        #sketch data\n",
    "        try:\n",
    "            adata = sketch_data(adata, train_x_partition = train_x, random_state = 42,  train_labels = train_label,sketch_obsm = sketch_obsm)\n",
    "        except:\n",
    "            print()\n",
    "\n",
    "    lr = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  max_iter, n_jobs=thread_num)\n",
    "    if (penalty == \"l1\"):\n",
    "        lr = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  max_iter, dual = True, solver = 'liblinear',multi_class = 'ovr', n_jobs=thread_num ) # one-vs-rest\n",
    "    if (penalty == \"elasticnet\"):\n",
    "        lr = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  max_iter, dual=False,solver = 'saga',l1_ratio=l1_ratio,multi_class = 'ovr', n_jobs=thread_num)\n",
    "    if train_x == 'X':\n",
    "        subset_train = adata.obs.index\n",
    "        # Define training parameters\n",
    "        train_label = adata.obs[train_label].values\n",
    "#        predict_label = train_label[subset_predict]\n",
    "#        train_label = train_label[subset_train]\n",
    "        train_x = adata.X#[adata.obs.index.isin(list(adata.obs[subset_train].index))]\n",
    "#        predict_x = adata.X[adata.obs.index.isin(list(adata.obs[subset_predict].index))]\n",
    "        model = lr.fit(train_x, train_label)\n",
    "        model.features = np.array(adata.var.index)\n",
    "    elif train_x in adata.obsm.keys():\n",
    "        print('train with obsm')\n",
    "        # Define training parameters\n",
    "        train_label = adata.obs[train_label].values\n",
    "#        predict_label = train_label[subset_predict]\n",
    "#         train_label = train_label[subset_train]\n",
    "        train_x = adata.obsm[train_x]\n",
    "#        predict_x = train_x\n",
    "#        train_x = train_x[subset_train, :]\n",
    "        # Define prediction parameters\n",
    "#        predict_x = predict_x[subset_predict]\n",
    "#        predict_x = pd.DataFrame(predict_x)\n",
    "#        predict_x.index = adata.obs[subset_predict].index\n",
    "    # Train predictive model using user defined partition labels (train_x ,train_label, predict_x)\n",
    "        model = lr.fit(train_x, train_label)\n",
    "        model.features = list(pd.DataFrame(train_x).columns)\n",
    "        #model.features = list(range()) list(pd.DataFrame(adata.obsm[train_x]).columns)\n",
    "    return model\n",
    "\n",
    "def tune_lr_model(adata, train_x_partition = 'X', random_state = 42,  train_labels = None, n_splits=5, n_repeats=3,l1_grid = [0.05,0.2,0.5], c_grid = [0.05,0.2,0.4],sketch_obsm = None):\n",
    "    import bless as bless\n",
    "    from sklearn.gaussian_process.kernels import RBF\n",
    "    from numpy import arange\n",
    "    from sklearn.model_selection import RepeatedKFold\n",
    "    from sklearn.datasets import make_classification\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    # If latent rep is provided, randomly sample data in spatially aware manner for initialisation\n",
    "    r = np.random.RandomState(random_state)\n",
    "    if train_x_partition in adata.obsm.keys():\n",
    "        tune_train_x = adata.obsm[train_x_partition][:]\n",
    "        lvg = bless.bless(tune_train_x, RBF(length_scale=20), lam_final = 2, qbar = 10, random_state = r, H = 10, force_cpu=True)\n",
    "        adata_tuning = adata[lvg.idx]\n",
    "        if not train_labels == None:\n",
    "            tune_train_label = adata_tuning.obs[train_labels]\n",
    "            orig_labels = adata.obs[train_labels]\n",
    "            # check if all labels are preserved\n",
    "            non_int = len(list(set(tune_train_label)^set(orig_labels)))\n",
    "            if ((non_int)) > 0 :\n",
    "                print('{} labels lost in sketch, attempting re-sketch'.format(non_int))\n",
    "                counter = 1\n",
    "                while counter < 5:\n",
    "                    q_bar = 10 + (5*counter)\n",
    "                    lvg = bless.bless(tune_train_x, RBF(length_scale=20), lam_final = 2, qbar = q_bar, random_state = r, H = 10, force_cpu=True)\n",
    "                    adata_tuning = adata[lvg.idx]\n",
    "                    tune_train_label = adata_tuning.obs[train_labels]\n",
    "                    non_int = len(list(set(tune_train_label)^set(orig_labels)))\n",
    "                    if non_int > 0:\n",
    "                        counter = counter + 1\n",
    "                    else:\n",
    "                        counter = 5\n",
    "        tune_train_x = adata_tuning.obsm[sketch_obsm][:]\n",
    "        print('Sketched data is {} long'.format(len(adata_tuning.obs)))\n",
    "\n",
    "    elif sketch_obsm in adata.obsm.keys():\n",
    "        sketch_obsm_id = adata.obsm[sketch_obsm][:]\n",
    "        lvg = bless.bless(sketch_obsm_id, RBF(length_scale=20), lam_final = 2, qbar = 10, random_state = r, H = 10, force_cpu=True)\n",
    "        adata_tuning = adata[lvg.idx]\n",
    "        if not train_labels == None:\n",
    "            tune_train_label = adata_tuning.obs[train_labels]\n",
    "            orig_labels = adata.obs[train_labels]\n",
    "            # check if all labels are preserved\n",
    "            non_int = len(list(set(tune_train_label)^set(orig_labels)))\n",
    "            if ((non_int)) > 0 :\n",
    "                print('{} labels lost in sketch, attempting re-sketch'.format(non_int))\n",
    "                counter = 1\n",
    "                while counter < 5:\n",
    "                    q_bar = 10 + (5*counter)\n",
    "                    lvg = bless.bless(tune_train_x, RBF(length_scale=20), lam_final = 2, qbar = q_bar, random_state = r, H = 10, force_cpu=True)\n",
    "                    adata_tuning = adata[lvg.idx]\n",
    "                    tune_train_label = adata_tuning.obs[train_labels]\n",
    "                    non_int = len(list(set(tune_train_label)^set(orig_labels)))\n",
    "                    if non_int > 0:\n",
    "                        counter = counter + 1\n",
    "                    else:\n",
    "                        counter = 5          \n",
    "        print('Sketched data is {} long'.format(len(adata_tuning.obs)))\n",
    "        tune_train_x = adata_tuning.obsm[sketch_obsm][:]\n",
    "    #     try:\n",
    "    #         import cupy\n",
    "    #         lvg_2 = bless(adata.obsm[train_x_partition], RBF(length_scale=10), 10, 10, r, 10, force_cpu=False)\n",
    "    #     except ImportError:\n",
    "    #         print(\"cupy not found, defaulting to numpy\")\n",
    "    else:\n",
    "        print('no latent representation provided, random sampling instead')\n",
    "        prop = 0.1\n",
    "        random_vertices = []\n",
    "        n_ixs = int(len(adata.obs) * prop)\n",
    "        random_vertices = random.sample(list(range(len(adata.obs))), k=n_ixs)\n",
    "        adata_tuning = adata[random_vertices]\n",
    "        tune_train_x = adata_tuning.X\n",
    "        \n",
    "    if not train_labels == None:\n",
    "        tune_train_label = adata_tuning.obs[train_labels]\n",
    "    elif train_labels == None:\n",
    "        try:\n",
    "            print('no training labels provided, defaulting to unsuperived leiden clustering, updates will change this to voronoi greedy sampling')\n",
    "            sc.tl.leiden(adata_tuning)\n",
    "        except:\n",
    "            print('no training labels provided, no neighbors, defaulting to unsuperived leiden clustering, updates will change this to voronoi greedy sampling')\n",
    "            sc.pp.neighbors(adata_hm, n_neighbors=15, n_pcs=50)\n",
    "            sc.tl.leiden(adata_tuning)\n",
    "        tune_train_label = adata_tuning.obs['leiden']\n",
    "    ## tune regularization for multinomial logistic regression\n",
    "    print('starting tuning loops')\n",
    "    X = tune_train_x\n",
    "    y = tune_train_label\n",
    "    grid = dict()\n",
    "    # define model\n",
    "    cv = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=random_state)\n",
    "    #model = LogisticRegression(penalty = penalty, max_iter =  200, dual=False,solver = 'saga', multi_class = 'multinomial',)\n",
    "    model = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  500, n_jobs=4)\n",
    "    if (penalty == \"l1\"):\n",
    "        model = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  500, dual = True, solver = 'liblinear',multi_class = 'ovr', n_jobs=4 ) # one-vs-rest\n",
    "    if (penalty == \"elasticnet\"):\n",
    "        model = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  500, dual=False,solver = 'saga',l1_ratio=l1_ratio,multi_class = 'ovr', n_jobs=4) # use multinomial class if probabilities are descrete\n",
    "        grid['l1_ratio'] = l1_grid\n",
    "    grid['C'] = c_grid\n",
    "    # define search\n",
    "    search = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "    # perform the search\n",
    "    results = search.fit(X, y)\n",
    "    # summarize\n",
    "    print('MAE: %.3f' % results.best_score_)\n",
    "    print('Config: %s' % results.best_params_)\n",
    "    return results , adata_tuning\n",
    "\n",
    "def prep_training_data(adata_temp,feat_use,batch_key, model_key, batch_correction=False, var_length = 7500,penalty='elasticnet',sparcity=0.2,max_iter = 200,l1_ratio = 0.1,partial_scale=True,train_x_partition ='X',theta = 3,tune_hyper_params=False,sketch_obsm = None ):\n",
    "    model_name = model_key + '_lr_model'\n",
    "    print('performing highly variable gene selection')\n",
    "    #sc.pp.highly_variable_genes(adata_temp, batch_key = batch_key, subset=False)\n",
    "#     #temp inclusion\n",
    "#     sc.pp.pca(adata_temp, n_comps=100, use_highly_variable=True, svd_solver='arpack')\n",
    "#     sc.pl.pca_variance_ratio(adata_temp, log=True,n_pcs=100)\n",
    "        \n",
    "    adata_temp = adata_temp#subset_top_hvgs(adata_temp,var_length)\n",
    "    #scale the input data\n",
    "    if partial_scale == True:\n",
    "        print('scaling input data, default option is to use incremental learning and fit in mini bulks!')\n",
    "        # Partial scaling alg\n",
    "        #adata_temp.X = (adata_temp.X)\n",
    "        scaler = StandardScaler(with_mean=False)\n",
    "        n = adata_temp.X.shape[0]  # number of rows\n",
    "        # set dyn scale packet size\n",
    "        x_len = len(adata_temp.var)\n",
    "        y_len = len(adata_temp.obs)\n",
    "        if y_len < 100000:\n",
    "            dyn_pack = int(x_len/10)\n",
    "            pack_size = dyn_pack\n",
    "        else:\n",
    "            # 10 pack for every 100,000\n",
    "            dyn_pack = int((y_len/100000)*10)\n",
    "            pack_size = int(x_len/dyn_pack)\n",
    "        batch_size =  1000#pack_size#500  # number of rows in each call to partial_fit\n",
    "        index = 0  # helper-var\n",
    "        while index < n:\n",
    "            partial_size = min(batch_size, n - index)  # needed because last loop is possibly incomplete\n",
    "            partial_x = adata_temp.X[index:index+partial_size]\n",
    "            scaler.partial_fit(partial_x)\n",
    "            index += partial_size\n",
    "        adata_temp.X = scaler.transform(adata_temp.X)\n",
    "#     else:\n",
    "#         sc.pp.scale(adata_temp, zero_center=True, max_value=None, copy=False, layer=None, obsm=None)\n",
    "\n",
    "    if (train_x_partition != 'X') & (train_x_partition not in adata_temp.obsm.keys()):\n",
    "        print('train partition is not in OBSM, defaulting to PCA')\n",
    "        # Now compute PCA\n",
    "        sc.pp.pca(adata_temp, n_comps=100, use_highly_variable=True, svd_solver='arpack')\n",
    "        sc.pl.pca_variance_ratio(adata_temp, log=True,n_pcs=100)\n",
    "        \n",
    "        # Batch correction options\n",
    "        # The script will test later which Harmony values we should use \n",
    "        if(batch_correction == \"Harmony\"):\n",
    "            print(\"Commencing harmony\")\n",
    "            if len(batch_key) == 1:\n",
    "                adata_temp.obs['lr_batch'] = adata_temp.obs[batch_key]\n",
    "                batch_var = \"lr_batch\"\n",
    "            else:\n",
    "                batch_var = batch_key\n",
    "            # Create hm subset\n",
    "            adata_hm = adata_temp[:]\n",
    "            # Set harmony variables\n",
    "            data_mat = np.array(adata_hm.obsm[\"X_pca\"])\n",
    "            meta_data = adata_hm.obs\n",
    "            vars_use = [batch_var]\n",
    "            # Run Harmony\n",
    "            ho = hm.run_harmony(data_mat, meta_data, vars_use,theta=theta)\n",
    "            res = (pd.DataFrame(ho.Z_corr)).T\n",
    "            res.columns = ['X{}'.format(i + 1) for i in range(res.shape[1])]\n",
    "            # Insert coordinates back into object\n",
    "            adata_hm.obsm[\"X_pca_back\"]= adata_hm.obsm[\"X_pca\"][:]\n",
    "            adata_hm.obsm[\"X_pca\"] = np.array(res)\n",
    "            # Run neighbours\n",
    "            #sc.pp.neighbors(adata_hm, n_neighbors=15, n_pcs=50)\n",
    "            adata_temp = adata_hm[:]\n",
    "            del adata_hm\n",
    "        elif(batch_correction == \"BBKNN\"):\n",
    "            print(\"Commencing BBKNN\")\n",
    "            sc.external.pp.bbknn(adata_temp, batch_key=batch_var, approx=True, metric='angular', copy=False, n_pcs=50, trim=None, n_trees=10, use_faiss=True, set_op_mix_ratio=1.0, local_connectivity=15) \n",
    "\n",
    "    # train model\n",
    "#    train_x = adata_temp.X\n",
    "    #train_label = adata_temp.obs[feat_use]\n",
    "    print('proceeding to train model')\n",
    "    model = LR_train(adata_temp, train_x = train_x_partition, train_label=feat_use, penalty=penalty, sparcity=sparcity,max_iter=max_iter,l1_ratio = l1_ratio,tune_hyper_params = tune_hyper_params,sketch_obsm = sketch_obsm)\n",
    "#     model.features = list(adata_temp.var.index)\n",
    "    return model\n",
    "\n",
    "def regression_results(y_true, y_pred):\n",
    "    # Regression metrics\n",
    "    explained_variance=metrics.explained_variance_score(y_true, y_pred)\n",
    "    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n",
    "    mse=metrics.mean_squared_error(y_true, y_pred) \n",
    "    mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)\n",
    "    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n",
    "    r2=metrics.r2_score(y_true, y_pred)\n",
    "    print('explained_variance: ', round(explained_variance,4))    \n",
    "    print('mean_squared_log_error: ', round(mean_squared_log_error,4))\n",
    "    print('r2: ', round(r2,4))\n",
    "    print('MAE: ', round(mean_absolute_error,4))\n",
    "    print('MSE: ', round(mse,4))\n",
    "    print('RMSE: ', round(np.sqrt(mse),4))\n",
    "    \n",
    "def sketch_data(adata, train_x_partition = 'X', sketch_obsm = None, random_state = 42,  train_labels = None):\n",
    "    import bless as bless\n",
    "    from sklearn.gaussian_process.kernels import RBF\n",
    "    from numpy import arange\n",
    "    from sklearn.model_selection import RepeatedKFold\n",
    "    from sklearn.datasets import make_classification\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    # If latent rep is provided, randomly sample data in spatially aware manner for initialisation\n",
    "    r = np.random.RandomState(random_state)\n",
    "    if train_x_partition in adata.obsm.keys():\n",
    "        tune_train_x = adata.obsm[train_x_partition][:]\n",
    "    elif sketch_obsm in adata.obsm.keys():\n",
    "        tune_train_x = adata.obsm[sketch_obsm][:]\n",
    "    else:\n",
    "        print('No obsm partition detected! defaulting to PCA')\n",
    "        if not 'X_pca' in adata.obsm.keys():\n",
    "            print('performing highly variable gene selection')\n",
    "            sc.pp.highly_variable_genes(adata_temp, batch_key = batch_key, subset=False)\n",
    "            sc.pp.pca(adata_temp, n_comps=100, use_highly_variable=True, svd_solver='arpack')\n",
    "            sc.pl.pca_variance_ratio(adata_temp, log=True,n_pcs=100)\n",
    "        tune_train_x = adata.obsm['X_pca'][:]\n",
    "    lvg = bless.bless(tune_train_x, RBF(length_scale=20), lam_final = 2, qbar = 2, random_state = r, H = 10, force_cpu=True)\n",
    "    adata_tuning = adata[lvg.idx]\n",
    "    print('sketched partition is {}, original is {}'.format(len(lvg.idx)),len(adata.obs))\n",
    "    return adata_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dependent-milwaukee",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_key = 'YS'\n",
    "model_key = 'low_dim_ldvae_model_YS_cross_organ'\n",
    "out_dir = './A1_V8_YS_model_pan_organ_heatmaps'\n",
    "train_x_partition = 'X_scvi'\n",
    "feat_use = 'LVL3'\n",
    "# if low_fim, else all below == False\n",
    "use_varm = '/nfs/team205/ig7/work_backups/backup_210306/projects/YS/YS_data/YS_panf_gonads_brain_eliv_combined_060922/ldVAE_model_projections/scvi_LDVAE_panf_pan_immune/A1_V13_ldvae_scvi_YS_updated_panf_gonads_brain_build_donor_organ_corrected_120922/A1_V13_ldvae_X_scvi_weights.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "french-frost",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low_dim model detected, proceeding to translate linear components into feature sets\n"
     ]
    }
   ],
   "source": [
    "model = load_models(models,model_key)\n",
    "model_lr =  model\n",
    "# Estimate top model features for class descrimination\n",
    "feature_importance = estimate_important_features(model_lr, 100)\n",
    "mat = feature_importance.euler_pow_mat\n",
    "top_loadings = feature_importance.to_n_features_long\n",
    "\n",
    "# Extra layer for low_dim models\n",
    "if use_varm != False:\n",
    "    print('Low_dim model detected, proceeding to translate linear components into feature sets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "curious-resistance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of model and varm features are equal, proceeding\n"
     ]
    }
   ],
   "source": [
    "model_varm = pd.read_csv(use_varm,index_col = 0)\n",
    "if len(sorted(list((top_loadings['feature'].unique())))) == len(list(model_varm.columns)):\n",
    "    print(\"length of model and varm features are equal, proceeding\")\n",
    "    feature_set = dict(zip(sorted(list((top_loadings['feature'].unique()))),list(model_varm.columns)))\n",
    "else: \n",
    "    print('Lengths of model and varm features are unequal! proceeding but cannot be sure that ordering if preserved!')\n",
    "    model_varm = model_varm.iloc[0:len(sorted(list((top_loadings['feature'].unique()))))]\n",
    "    feature_set = dict(zip(sorted(list((top_loadings['feature'].unique()))),list(model_varm.columns)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "viral-director",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-165-61bc49a16624>:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  top_loadings_lw['feature'] = top_loadings_lw['feature'].map(feature_set)\n",
      "<ipython-input-165-61bc49a16624>:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_varm_sig['weighted_impact'] = temp_varm_sig['value'] * feature[1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMP\n",
      "DC2_CYCLING\n",
      "EARLY_ERYTHROID\n",
      "ELP\n",
      "ENDODERM\n",
      "ENDOTHELIUM_AEC\n",
      "ENDOTHELIUM_PROLIF_AEC\n",
      "ENDOTHELIUM_PROLIF_SINUSOIDAL_EC\n",
      "ENDOTHELIUM_SINUSOIDAL\n",
      "ENDOTHELIUM_VWF\n",
      "EOSINOPHIL_BASOPHIL\n",
      "EO_BASO_MAST_PRECURSOR\n",
      "ERYTHROID\n",
      "FIBROBLAST\n",
      "HE\n",
      "HSPC_1\n",
      "HSPC_2\n",
      "ILC_PRECURSOR\n",
      "IMMATURE_ENDOTHELIUM\n",
      "LMPP\n",
      "LYMPHOID B LIN\n",
      "MAC DC2\n",
      "MACROPHAGE_ERY\n",
      "MACROPHAGE_IRON_RECYCLING\n",
      "MACROPHAGE_KUPFFER_LIKE\n",
      "MACROPHAGE_LYVE1_HIGH\n",
      "MACROPHAGE_MHCII_HIGH\n",
      "MACROPHAGE_MICROGLIA\n",
      "MACROPHAGE_PERI\n",
      "MACROPHAGE_PRE_MAC\n",
      "MACROPHAGE_PROLIFERATING\n",
      "MAST_CELL\n",
      "MEMP\n",
      "MESOTHELIUM\n",
      "MID_ERYTHROID\n",
      "MK\n",
      "MONO MAC DC2\n",
      "MONO MAC PRE DC2\n",
      "MONOCYTE_0_YS\n",
      "MONOCYTE_1_YS\n",
      "MONOCYTE_MACROPHAGE\n",
      "MOP\n",
      "NEUTROPHIL_PRECURSOR\n",
      "NK\n",
      "OSTEOCLAST\n",
      "PDC PRECURSOR\n",
      "PRE DC2\n",
      "PROMONOCYTE\n",
      "SMOOTH_MUSCLE\n"
     ]
    }
   ],
   "source": [
    "# Top predictive features for each component\n",
    "varm_melt = pd.melt(model_varm.reset_index(), id_vars='index')\n",
    "varm_melt['pvals']= np.NAN\n",
    "for variable in varm_melt['variable'].unique():\n",
    "    varm_loadings = varm_melt[varm_melt['variable'].isin([variable])]\n",
    "    comps = 'value'\n",
    "    U = np.mean(varm_loadings[comps])\n",
    "    std = np.std(varm_loadings[comps])\n",
    "    med =  np.median(varm_loadings[comps])\n",
    "    mad = np.median(np.absolute(varm_loadings[comps] - np.median(varm_loadings[comps])))\n",
    "    # Survival function scaled by 1.4826 of MAD (approx norm)\n",
    "    pvals = scipy.stats.norm.sf(varm_loadings[comps], loc=med, scale=1.4826*mad)\n",
    "    varm_melt.loc[varm_melt['variable'].isin([variable]),'pvals'] = pvals\n",
    "varm_sig = varm_melt[varm_melt['pvals']<0.05]\n",
    "\n",
    "# varm sig is the reference for weighting, report top 10 weighted features per class\n",
    "\n",
    "top_loadings_lowdim = pd.DataFrame(columns =['class','feature', 'weighted_impact', 'e^coef_pval', 'e^coef',\n",
    "       'is_significant_sf'])\n",
    "top_loadings_lw = top_loadings.groupby(['class']).head(10)\n",
    "top_loadings_lw['feature'] = top_loadings_lw['feature'].map(feature_set)\n",
    "for classes in top_loadings_lw['class'].unique():\n",
    "    print(classes)\n",
    "    # linearise in next update!\n",
    "    for feature in top_loadings_lw.loc[top_loadings_lw['class'].isin([classes]) ,['feature','e^coef']].values:\n",
    "        # multiply value by impact! \n",
    "        temp_varm_sig = varm_sig[varm_sig['variable'].isin([feature[0]])]\n",
    "        temp_varm_sig['weighted_impact'] = temp_varm_sig['value'] * feature[1]\n",
    "        temp_varm_sig = temp_varm_sig[['index','weighted_impact']]\n",
    "        temp_varm_sig.columns = ['feature','weighted_impact']\n",
    "        temp_varm_sig['class'] = classes\n",
    "        temp_varm_sig['e^coef_pval'] = top_loadings_lw.loc[(top_loadings_lw['class'].isin([classes])) & (top_loadings_lw['feature'].isin([feature[0]])),'e^coef_pval'].values[0]\n",
    "        temp_varm_sig['e^coef'] = top_loadings_lw.loc[(top_loadings_lw['class'].isin([classes])) & (top_loadings_lw['feature'].isin([feature[0]])),'e^coef'].values[0]\n",
    "        temp_varm_sig['is_significant_sf'] = top_loadings_lw.loc[(top_loadings_lw['class'].isin([classes])) & (top_loadings_lw['feature'].isin([feature[0]])),'is_significant_sf'].values[0]\n",
    "        top_loadings_lowdim = pd.concat([top_loadings_lowdim,temp_varm_sig],ignore_index=True)\n",
    "top_loadings_lowdim = top_loadings_lowdim.sort_values(['weighted_impact'],ascending=False).groupby('class').head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "ignored-knitting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>weighted_impact</th>\n",
       "      <th>class</th>\n",
       "      <th>e^coef_pval</th>\n",
       "      <th>e^coef</th>\n",
       "      <th>is_significant_sf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>315000</th>\n",
       "      <td>A2M</td>\n",
       "      <td>0.373896</td>\n",
       "      <td>SMOOTH_MUSCLE</td>\n",
       "      <td>0.213745</td>\n",
       "      <td>1.595367</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315006</th>\n",
       "      <td>AADAT</td>\n",
       "      <td>0.118918</td>\n",
       "      <td>SMOOTH_MUSCLE</td>\n",
       "      <td>0.213745</td>\n",
       "      <td>1.595367</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315009</th>\n",
       "      <td>AARD</td>\n",
       "      <td>0.095255</td>\n",
       "      <td>SMOOTH_MUSCLE</td>\n",
       "      <td>0.213745</td>\n",
       "      <td>1.595367</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315018</th>\n",
       "      <td>ABCA13</td>\n",
       "      <td>0.097217</td>\n",
       "      <td>SMOOTH_MUSCLE</td>\n",
       "      <td>0.213745</td>\n",
       "      <td>1.595367</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315025</th>\n",
       "      <td>ABCB5</td>\n",
       "      <td>0.120334</td>\n",
       "      <td>SMOOTH_MUSCLE</td>\n",
       "      <td>0.213745</td>\n",
       "      <td>1.595367</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329725</th>\n",
       "      <td>ZNF488</td>\n",
       "      <td>0.173459</td>\n",
       "      <td>SMOOTH_MUSCLE</td>\n",
       "      <td>0.213745</td>\n",
       "      <td>1.595367</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329773</th>\n",
       "      <td>ZNF561-AS1</td>\n",
       "      <td>0.095590</td>\n",
       "      <td>SMOOTH_MUSCLE</td>\n",
       "      <td>0.213745</td>\n",
       "      <td>1.595367</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329856</th>\n",
       "      <td>ZNF683</td>\n",
       "      <td>0.202654</td>\n",
       "      <td>SMOOTH_MUSCLE</td>\n",
       "      <td>0.213745</td>\n",
       "      <td>1.595367</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329940</th>\n",
       "      <td>ZNF843</td>\n",
       "      <td>0.077536</td>\n",
       "      <td>SMOOTH_MUSCLE</td>\n",
       "      <td>0.213745</td>\n",
       "      <td>1.595367</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329959</th>\n",
       "      <td>ZNF99</td>\n",
       "      <td>0.087161</td>\n",
       "      <td>SMOOTH_MUSCLE</td>\n",
       "      <td>0.213745</td>\n",
       "      <td>1.595367</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1205 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           feature  weighted_impact          class  e^coef_pval    e^coef  \\\n",
       "315000         A2M         0.373896  SMOOTH_MUSCLE     0.213745  1.595367   \n",
       "315006       AADAT         0.118918  SMOOTH_MUSCLE     0.213745  1.595367   \n",
       "315009        AARD         0.095255  SMOOTH_MUSCLE     0.213745  1.595367   \n",
       "315018      ABCA13         0.097217  SMOOTH_MUSCLE     0.213745  1.595367   \n",
       "315025       ABCB5         0.120334  SMOOTH_MUSCLE     0.213745  1.595367   \n",
       "...            ...              ...            ...          ...       ...   \n",
       "329725      ZNF488         0.173459  SMOOTH_MUSCLE     0.213745  1.595367   \n",
       "329773  ZNF561-AS1         0.095590  SMOOTH_MUSCLE     0.213745  1.595367   \n",
       "329856      ZNF683         0.202654  SMOOTH_MUSCLE     0.213745  1.595367   \n",
       "329940      ZNF843         0.077536  SMOOTH_MUSCLE     0.213745  1.595367   \n",
       "329959       ZNF99         0.087161  SMOOTH_MUSCLE     0.213745  1.595367   \n",
       "\n",
       "        is_significant_sf  \n",
       "315000              False  \n",
       "315006              False  \n",
       "315009              False  \n",
       "315018              False  \n",
       "315025              False  \n",
       "...                   ...  \n",
       "329725              False  \n",
       "329773              False  \n",
       "329856              False  \n",
       "329940              False  \n",
       "329959              False  \n",
       "\n",
       "[1205 rows x 6 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_varm_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "naked-metabolism",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_class_feature_plots(top_loadings, classes, comps):\n",
    "    import matplotlib.pyplot as plt\n",
    "    for class_temp in classes:\n",
    "        class_lw = class_temp\n",
    "        long_format = top_loadings\n",
    "        df_loadings = long_format[long_format['class'].isin([class_lw])]\n",
    "        plt.hist(df_loadings[comps])\n",
    "        for i in ((df_loadings[comps][df_loadings[str(comps) +'_pval']<0.05]).unique()):\n",
    "            plt.axvline(x=i,color='red')\n",
    "        med = np.median(df_loadings[comps])\n",
    "        plt.axvline(x=med,color='blue')\n",
    "        plt.xlabel('feature_importance', fontsize=12)\n",
    "        plt.title(class_lw)\n",
    "        #plt.axvline(x=med,color='pink')\n",
    "        df_loadings[comps][df_loadings[str(comps) +'_pval']<0.05]\n",
    "        print(len(df_loadings[comps][df_loadings[str(comps) +'_pval']<0.05]))\n",
    "        #Plot feature ranking\n",
    "        plot_loading = pd.DataFrame(pd.DataFrame(df_loadings[comps][df_loadings[str(comps) +'_pval']<0.05]).iloc[:,0].sort_values(ascending=False))\n",
    "        table = plt.table(cellText=plot_loading.values,colWidths = [1]*len(plot_loading.columns),\n",
    "        rowLabels= list(df_loadings['feature'][df_loadings.index.isin(plot_loading.index)].reindex(plot_loading.index)), #plot_loading.index,\n",
    "        colLabels=plot_loading.columns,\n",
    "        cellLoc = 'center', rowLoc = 'center',\n",
    "        loc='right', bbox=[1.4, -0.05, 0.5,1])\n",
    "        table.scale(1, 2)\n",
    "        table.set_fontsize(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "breathing-potential",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>feature</th>\n",
       "      <th>weighted_impact</th>\n",
       "      <th>e^coef_pval</th>\n",
       "      <th>e^coef</th>\n",
       "      <th>is_significant_sf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81018</th>\n",
       "      <td>ENDOTHELIUM_VWF</td>\n",
       "      <td>ITM2A</td>\n",
       "      <td>43.100996</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>74.946595</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81148</th>\n",
       "      <td>ENDOTHELIUM_VWF</td>\n",
       "      <td>SYNGR3</td>\n",
       "      <td>39.569729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>74.946595</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81044</th>\n",
       "      <td>ENDOTHELIUM_VWF</td>\n",
       "      <td>MALAT1</td>\n",
       "      <td>39.554469</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>74.946595</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80889</th>\n",
       "      <td>ENDOTHELIUM_VWF</td>\n",
       "      <td>CHI3L2</td>\n",
       "      <td>39.204718</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>74.946595</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80924</th>\n",
       "      <td>ENDOTHELIUM_VWF</td>\n",
       "      <td>FABP7</td>\n",
       "      <td>38.842966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>74.946595</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9752</th>\n",
       "      <td>DC2_CYCLING</td>\n",
       "      <td>TLCD4</td>\n",
       "      <td>1.053568</td>\n",
       "      <td>0.022502</td>\n",
       "      <td>1.602236</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9683</th>\n",
       "      <td>DC2_CYCLING</td>\n",
       "      <td>PCDH17</td>\n",
       "      <td>1.053041</td>\n",
       "      <td>0.022502</td>\n",
       "      <td>1.602236</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9597</th>\n",
       "      <td>DC2_CYCLING</td>\n",
       "      <td>GRIA1</td>\n",
       "      <td>1.052548</td>\n",
       "      <td>0.022502</td>\n",
       "      <td>1.602236</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9563</th>\n",
       "      <td>DC2_CYCLING</td>\n",
       "      <td>EIF4E3</td>\n",
       "      <td>1.051947</td>\n",
       "      <td>0.022502</td>\n",
       "      <td>1.602236</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9521</th>\n",
       "      <td>DC2_CYCLING</td>\n",
       "      <td>CAMK2N2</td>\n",
       "      <td>1.048525</td>\n",
       "      <td>0.022502</td>\n",
       "      <td>1.602236</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4900 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 class  feature  weighted_impact  e^coef_pval     e^coef  \\\n",
       "81018  ENDOTHELIUM_VWF    ITM2A        43.100996     0.000000  74.946595   \n",
       "81148  ENDOTHELIUM_VWF   SYNGR3        39.569729     0.000000  74.946595   \n",
       "81044  ENDOTHELIUM_VWF   MALAT1        39.554469     0.000000  74.946595   \n",
       "80889  ENDOTHELIUM_VWF   CHI3L2        39.204718     0.000000  74.946595   \n",
       "80924  ENDOTHELIUM_VWF    FABP7        38.842966     0.000000  74.946595   \n",
       "...                ...      ...              ...          ...        ...   \n",
       "9752       DC2_CYCLING    TLCD4         1.053568     0.022502   1.602236   \n",
       "9683       DC2_CYCLING   PCDH17         1.053041     0.022502   1.602236   \n",
       "9597       DC2_CYCLING    GRIA1         1.052548     0.022502   1.602236   \n",
       "9563       DC2_CYCLING   EIF4E3         1.051947     0.022502   1.602236   \n",
       "9521       DC2_CYCLING  CAMK2N2         1.048525     0.022502   1.602236   \n",
       "\n",
       "      is_significant_sf  \n",
       "81018              True  \n",
       "81148              True  \n",
       "81044              True  \n",
       "80889              True  \n",
       "80924              True  \n",
       "...                 ...  \n",
       "9752               True  \n",
       "9683               True  \n",
       "9597               True  \n",
       "9563               True  \n",
       "9521               True  \n",
       "\n",
       "[4900 rows x 6 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_loadings_lowdim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "complicated-meter",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class_feature_plots(top_loadings_lowdim, ['MACROPHAGE_MICROGLIA'], 'e^coef')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workhorse",
   "language": "python",
   "name": "workhorse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
