{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "minus-return",
   "metadata": {},
   "source": [
    "# Extended figure 5h part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-match",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "#import pkg_resources\n",
    "#required = {'harmonypy','sklearn','scanpy','pandas', 'numpy', 'bbknn', 'scipy', 'matplotlib', 'seaborn' ,'scipy'}\n",
    "#installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "#missing = required - installed\n",
    "#if missing:\n",
    "#    print(\"Installing missing packages:\" )\n",
    "#    print(missing)\n",
    "#    python = sys.executable\n",
    "#    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
    "\n",
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from bbknn import bbknn\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from geosketch import gs\n",
    "from numpy import cov\n",
    "import scipy.cluster.hierarchy as spc\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "import harmonypy as hm\n",
    "from pathlib import Path\n",
    "\n",
    "sc.settings.verbosity = 3  # verbosity: errors (0), warnings (1), info (2), hints (3)\n",
    "sc.settings.set_figure_params(dpi=80, color_map='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-public",
   "metadata": {},
   "source": [
    "# LR script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-window",
   "metadata": {},
   "source": [
    "# Only block to edit in LR script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-event",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Introduce variables\n",
    "# Note that this script expects raw data to be in \"non-batch-corrected\" adata.raw.X. \n",
    "\n",
    "# Required: Introduce the path you'd like to save figures or data to \n",
    "save_path = \"/home/jovyan/YS_project/New_AGM_comparison/Endothelium_comparison_4_models_19112021/Eddie_AGM_output_23112021/\"\n",
    "\n",
    "run_date = \"23112021\" # import data time to auto this in future - see data prep script\n",
    "# add an option to add additional arguments to combined save object \n",
    "\n",
    "# Required: Name of second object\n",
    "data1 = \"_training\"\n",
    "data1_identifier = \"_YS_main_endo\"\n",
    "# Provide path to obj2 // prediction/projection data\n",
    "Object1 = \"/home/jovyan/YS_project/New_AGM_comparison/Endothelium_comparison_4_models_19112021/YS_main_for_endothlium_comparison_19112021.h5ad\"\n",
    "# Provide categorical to join between datasets\n",
    "cat1 = \"celltypes_for_comparison\"\n",
    "\n",
    "# Required: Name of first object\n",
    "data2 = \"_prediction\"\n",
    "data2_identifier = \"_Eddie_AGM\"\n",
    "# Provide path to obj1 // landscape/training data\n",
    "Object2 = \"/home/jovyan/YS_project/New_AGM_comparison/Endothelium_comparison_4_models_19112021/Eddie_AGM_for_endothlium_comparison_23112021.h5ad\"\n",
    "# Provide categorical to join between datasets\n",
    "cat2 = \"celltypes_for_comparison\"\n",
    "\n",
    "# Required: LR Model Options\n",
    "penalty='elasticnet' # can be [\"l1\",\"l2\",\"elasticnet\"]\n",
    "sparcity=0.2\n",
    "max_iter = 1000 #Increase if experiencing max iter issues\n",
    "l1_ratio = 0.5 #If using elasticnet, tis controls the ratio between l1 and l2\n",
    "\n",
    "# Optional: Batch correction options (this is for correction of eventual combined dataset for data1 and data2)\n",
    "# If you do not have a batch variable for either data1 or data2, please add a \"filler\" column in the relevent adata.obs\n",
    "# for the purposes of batch_correction and batch args below.\n",
    "# e.g., adata.obs[\"whatever\"] = \"something\"; batch=\"whatever\"\n",
    "batch_correction = \"Harmony\" # Will accept Harmony, BBKNN or False as options\n",
    "batch = [\"fetal.ids\",\"Batch_col\",] # Will accept any batch categorical. Comma space a batch categorical for each dataset. Position 1 is for data1, position 2 is for data2\n",
    "\n",
    "# Optional: miscellaneous Options.   \n",
    "subsample_train = False # Samples the training data to the smallest fraction (highly dependent on resolution of input celltype categorical). This corrects for proportional differences between celltype labels of interest in the training data. E.g., training data has 50,000 B cells, 20,000 T cells and 100 HSCs. This function will subsample all training to 100 cells per cell type. \n",
    "subsample_prop = 0.2 # Give this option a proprtion to subsample to(e.g 0.2), if NA given, will subsample to smallest population\n",
    "subsample_predict = False\n",
    "subsample_prop_predict = 0.5\n",
    "remove_non_high_var = False\n",
    "\n",
    "train_x = 'X_pca' # Define the resource to train and predict on, PCA, X or UMAP (#if you wish to use gene expression, train_x = 'X')\n",
    "remove_effect_of_custom_gene_list = 'NA' # \"./cell_cycle_genes.csv\" #remove a custom list of genes from just the variable genes to compute PCA from. Your .csv should have HGNC gene names in the first column to be read in as a vector, any column name is fine.\n",
    "use_raw = True # Do you want to use adata.raw.X (recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make our own clusters and run on own computed clusters then project back to own clusters not optional clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-officer",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Rest of LR script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-school",
   "metadata": {},
   "source": [
    "## Combining data and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-button",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if filepaths are good\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "    \n",
    "if (Path(Object1).is_file() & Path(Object2).is_file()):\n",
    "    print(\"adata file paths detetcted, proceeding to load\")\n",
    "    adata = sc.read(Object1)\n",
    "    adata2 =  sc.read(Object2)\n",
    "    del adata.uns\n",
    "    del adata2.uns\n",
    "else: \n",
    "    raise TypeError(\"one or more .h5ad paths cannot be accessed\")\n",
    "\n",
    "# altering scanpy setting so that we can save it to our defined directory\n",
    "sc._settings.ScanpyConfig(figdir=save_path)\n",
    "\n",
    "# Combine and pre-process data to match correlations across PCA\n",
    "\n",
    "# Module to detect shape mismatch and alternatively rebuild adata\n",
    "if(use_raw==True):\n",
    "    print('option detected to use raw data, proceeding to check if raw exists and if it matches data.X')\n",
    "    if (hasattr(adata.raw, \"X\")):\n",
    "        try: adata.X =  adata.raw.X  ; print('no mismatch in shape for adata detected')\n",
    "        except: print(\"adata.X shape mismatched with adata.raw.X, proceeding to re-build data\") ; adata = adata.raw.to_adata()\n",
    "    else:\n",
    "        print(\"no raw data detected in adata! proceeding to create raw partition from adata.X\")\n",
    "        adata.raw = adata\n",
    "        \n",
    "    if (hasattr(adata2.raw, \"X\")):\n",
    "        try: adata2.X = adata2.raw.X ; print('no mismatch in shape for adata2 detected')\n",
    "        except: print(\"adata2.X shape mismatched with adata.raw.X, proceeding to re-build data\") ; adata2 = adata2.raw.to_adata()\n",
    "    else:\n",
    "        print(\"no raw data detected in adata! proceeding to create raw partition from adata.X\")\n",
    "        adata2.raw = adata2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-sampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define intersecting genes between datasets\n",
    "adata_genes = list(adata.var.index)\n",
    "adata2_genes = list(adata2.var.index)\n",
    "keep_SC_genes = list(set(adata_genes) & set(adata2_genes))\n",
    "print(\"keep gene list = \" , len(keep_SC_genes), \"adata1 gene length = \", len(adata_genes) , \"adata2 gene length = \", len(adata2_genes) )\n",
    "\n",
    "# Remove non-intersecting genes (this step will remove cite-seq data if training data is pure RNA seq)\n",
    "adata_intersect1 = adata[:, keep_SC_genes]\n",
    "adata = adata_intersect1\n",
    "adata_intersect2 = adata2[:, keep_SC_genes]\n",
    "adata2 = adata_intersect2\n",
    "\n",
    "# Optional subsampling of training data to \n",
    "if(subsample_train == True):\n",
    "    \n",
    "    if not(subsample_prop==\"NA\"):\n",
    "        print(\"option to subsample by proportion chosen\")\n",
    "        prop = subsample_prop\n",
    "        data = adata.obs[:]\n",
    "        grouped = data.groupby(cat1)\n",
    "        df = grouped.apply(lambda x: x.sample(frac=prop))\n",
    "        df = df.droplevel(cat1)\n",
    "        keep = df.index\n",
    "        adata = adata[adata.obs.index.isin(keep)]\n",
    "    else:\n",
    "        print(\"subsample by smallest population\")\n",
    "        data = adata.obs\n",
    "        data = data.sample(frac=1).groupby(cat1).head(min(adata.obs.groupby(cat1).size()))\n",
    "        keep = data.index\n",
    "        adata = adata[adata.obs.index.isin(keep)]\n",
    "        \n",
    "# Optional subsampling of training data to \n",
    "if(subsample_predict == True):\n",
    "    if not(subsample_prop_predict==\"NA\"):\n",
    "        print(\"option to subsample by proportion chosen\")\n",
    "        prop = subsample_prop_predict\n",
    "        data = adata2.obs[:]\n",
    "        grouped = data.groupby(cat2)\n",
    "        df = grouped.apply(lambda x: x.sample(frac=prop))\n",
    "        df = df.droplevel(cat2)\n",
    "        keep = df.index\n",
    "        adata2 = adata2[adata2.obs.index.isin(keep)]\n",
    "    else:\n",
    "        print(\"subsample by smallest population\")\n",
    "        data = adata2.obs\n",
    "        data = data.sample(frac=1).groupby(cat2).head(min(adata.obs.groupby(cat2).size()))\n",
    "        keep = data.index\n",
    "        adata2 = adata2[adata2.obs.index.isin(keep)]\n",
    "\n",
    "# Create a common batch column and do simple sanity check for batch variables\n",
    "if not((batch_correction == \"False\") and (len(batch)>1)):\n",
    "    print(\"Batch correction option detected, proceeding to format batch variables\")\n",
    "    batch_var = \"lr_batch\"\n",
    "    adata.obs[\"lr_batch\"] = adata.obs[batch[0]]\n",
    "    adata2.obs[\"lr_batch\"] = adata2.obs[batch[1]]\n",
    "else: raise TypeError(\"Batch correction option detected but requires at least one categorical for each dataset!\")\n",
    "\n",
    "# Create a common obs column in both datasets containing the data origin tag\n",
    "common_cat = \"corr_concat\" \n",
    "adata.obs[common_cat] = adata.obs[cat1].astype(str) + data1\n",
    "adata2.obs[common_cat] = adata2.obs[cat2].astype(str) + data2\n",
    "adata.obs = adata.obs.astype('category')\n",
    "adata2.obs = adata2.obs.astype('category')\n",
    "\n",
    "sc.pp.scale(adata, zero_center=False, max_value=None, copy=False) #zero_center=True (densifies output)\n",
    "sc.pp.scale(adata2, zero_center=False, max_value=None, copy=False) #zero_center=True (densifies output)\n",
    "\n",
    "\n",
    "concat = adata2.concatenate(adata, join='inner',index_unique=None, batch_categories=None)\n",
    "adata = concat[:]\n",
    "sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4)\n",
    "sc.pp.log1p(adata)\n",
    "sc.pp.highly_variable_genes(adata, min_mean=0.1, max_mean=4)\n",
    "#sc.pp.scale(adata, zero_center=False, max_value=None, copy=False) #zero_center=True (densifies output)\n",
    "\n",
    "# Optionally remove genes of known confounding effect from variable list\n",
    "if not (Path(remove_effect_of_custom_gene_list).is_file()):\n",
    "    print(\"Custom gene list option is not selected or path is not readbale, proceeding with no variable removal\")\n",
    "else: \n",
    "    print(\"Custom gene removal list detected, proceeding to remove intersect from variable genes\")\n",
    "    regress_list = pd.read_csv(remove_effect_of_custom_gene_list)\n",
    "    regress_list = regress_list.iloc[:, 0]\n",
    "    adata.var[\"highly_variable\"][adata.var.index.isin(regress_list)] = \"False\"\n",
    "\n",
    "#Optionally remove genes that do not contribute to variance in combined data::Use only if training and predicting withsim reduced data    \n",
    "if(remove_non_high_var==True):\n",
    "    high_var = list(adata.var[\"highly_variable\"][adata.var[\"highly_variable\"]==True])\n",
    "    adata = adata[:, adata.var[\"highly_variable\"].isin(high_var)]   \n",
    "\n",
    "# Now compute PCA\n",
    "sc.pp.pca(adata, n_comps=50, use_highly_variable=True, svd_solver='arpack')\n",
    "\n",
    "# Batch correction options\n",
    "# The script will test later which Harmony values we should use\n",
    "if not(batch_correction == \"False\"):\n",
    "    sc.pp.neighbors(adata, n_neighbors=15, n_pcs=50)    \n",
    "if(batch_correction == \"Harmony\"):\n",
    "    print(\"Commencing harmony\")\n",
    "    # Create hm subset\n",
    "    adata_hm = adata[:]\n",
    "    # Set harmony variables\n",
    "    data_mat = np.array(adata_hm.obsm[\"X_pca\"])\n",
    "    meta_data = adata_hm.obs\n",
    "    vars_use = [batch_var]\n",
    "    # Run Harmony\n",
    "    ho = hm.run_harmony(data_mat, meta_data, vars_use)\n",
    "    res = (pd.DataFrame(ho.Z_corr)).T\n",
    "    res.columns = ['X{}'.format(i + 1) for i in range(res.shape[1])]\n",
    "    # Insert coordinates back into object\n",
    "    adata_hm.obsm[\"X_pca_back\"]= adata_hm.obsm[\"X_pca\"][:]\n",
    "    adata_hm.obsm[\"X_pca\"] = np.array(res)\n",
    "    # Run neighbours\n",
    "    sc.pp.neighbors(adata_hm, n_neighbors=5, n_pcs=50) # Reduced neighbours to get more granularity due to small cell numbers\n",
    "    adata = adata_hm[:]\n",
    "    del adata_hm\n",
    "elif(batch_correction == \"BBKNN\"):\n",
    "    print(\"Commencing BBKNN\")\n",
    "    sc.external.pp.bbknn(adata, batch_key=batch_var, approx=True, metric='angular', copy=False, n_pcs=50, trim=None, n_trees=10, use_faiss=True, set_op_mix_ratio=1.0, local_connectivity=15) \n",
    "    \n",
    "print(\"adata1 and adata2 are now combined and preprocessed in 'adata' obj - success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-dryer",
   "metadata": {},
   "source": [
    "## Logistic regression function to train data set and transfer labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-magnitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the separator category in the column of interest, this works by partial matches and enables a-symmetric \n",
    "# comparisons\n",
    "Data1_group = data1\n",
    "Data2_group = data2\n",
    "# Define the common .obs column between concatinated data\n",
    "common_cat = \"corr_concat\"\n",
    "\n",
    "# This block defines subset_predict and subset_train and also runs LR_compare function\n",
    "group1 = (adata.obs[common_cat][adata.obs[common_cat].str.contains(Data1_group)]).unique()\n",
    "group1 = list(group1)\n",
    "group2 = (adata.obs[common_cat][adata.obs[common_cat].str.contains(Data2_group)]).unique()\n",
    "group2 = list(group2)\n",
    "subset_predict = np.array(adata.obs[common_cat].isin(group2))\n",
    "subset_train = np.array(adata.obs[common_cat].isin(group1))\n",
    "train_label = (adata.obs[common_cat][adata.obs[common_cat].isin(group1)]).values\n",
    "\n",
    "#LR_compare(adata, train_x, train_label, subset_predict, subset_train, sparcity=sparcity, col_name='predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-louisville",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name='predicted'\n",
    "train_x = 'X_pca'\n",
    "\n",
    "if train_x in adata.obsm.keys():\n",
    "    # Define training parameters\n",
    "    train_label = adata.obs[common_cat].values\n",
    "    train_label = train_label[subset_train]\n",
    "    train_x = adata.obsm[train_x]\n",
    "    predict_x = train_x\n",
    "    train_x = train_x[subset_train, :]\n",
    "    # Define prediction parameters\n",
    "    predict_x = predict_x[subset_predict]\n",
    "    predict_x = pd.DataFrame(predict_x)\n",
    "    predict_x.index = adata.obs[subset_predict].index\n",
    "\n",
    "\n",
    "# Train predictive model using user defined partition labels (train_x ,train_label, predict_x)\n",
    "if (penalty == \"l1\"):\n",
    "    lr = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  max_iter, dual = True, solver = 'liblinear')\n",
    "if (penalty == \"elasticnet\"):\n",
    "    lr = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  max_iter, dual=False,solver = 'saga',l1_ratio=l1_ratio)\n",
    "else:\n",
    "    lr = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  max_iter)\n",
    "model = lr.fit(train_x, train_label)\n",
    "lr.fit(train_x, train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-killing",
   "metadata": {},
   "source": [
    "# Model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-scheme",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot heatmap by percentage\n",
    "def plot_df_heatmap(df, cmap='viridis', title=None, figsize=(7, 7), rotation=90, save=None, **kwargs):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    im = ax.imshow(df, cmap=cmap, aspect='auto', **kwargs)\n",
    "    if 0 < rotation < 90:\n",
    "        horizontalalignment = 'right'\n",
    "    else:\n",
    "        horizontalalignment = 'center'\n",
    "    plt.xticks(\n",
    "        range(len(df.columns)),\n",
    "        df.columns,\n",
    "        rotation=rotation,\n",
    "        horizontalalignment=horizontalalignment,\n",
    "    )\n",
    "    plt.yticks(range(len(df.index)), df.index)\n",
    "    if title:\n",
    "        fig.suptitle(title)\n",
    "    #fig.colorbar(im)\n",
    "    if save:\n",
    "        plt.savefig(fname=save, bbox_inches='tight', pad_inches=0.1)\n",
    "\n",
    "# Plot probability table by html\n",
    "def cross_table(adata, x, y, normalise=None, highlight=False, subset=None):                                                                                                                                                                                              \n",
    "    \"\"\"Make a cross table comparing two categorical annotations\n",
    "    \"\"\"\n",
    "    x_attr = adata.obs[x]\n",
    "    y_attr = adata.obs[y]\n",
    "    if subset is not None:\n",
    "        x_attr = x_attr[subset]\n",
    "        y_attr = y_attr[subset]\n",
    "    crs_tbl = pd.crosstab(x_attr, y_attr)\n",
    "    if normalise == 'x':\n",
    "        x_sizes = x_attr.groupby(x_attr).size().values\n",
    "        crs_tbl = (crs_tbl.T / x_sizes).round(2).T\n",
    "    elif normalise == 'y':\n",
    "        y_sizes = x_attr.groupby(y_attr).size().values\n",
    "        crs_tbl = (crs_tbl / y_sizes).round(2)\n",
    "    if highlight:\n",
    "        return crs_tbl.style.background_gradient(cmap='viridis', axis=0)\n",
    "    return crs_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-edgar",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Report accuracy score\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "#from sklearn import metrics\n",
    "#import seaborn as sn\n",
    "#import pandas as pd\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sn\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1) # ran 5 times with 10 splits of the data takren randomly ( 50 tests)\n",
    "# evaluate the model and collect the scores\n",
    "n_scores = cross_val_score(lr, train_x, train_label, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report the model performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))\n",
    "\n",
    "# Report Precision score\n",
    "metric = pd.DataFrame((metrics.classification_report(train_label, model.predict(train_x), digits=3,output_dict=True))).T\n",
    "\n",
    "cm = confusion_matrix(train_label, model.predict(train_x))\n",
    "#cm = confusion_matrix(train_label, model.predict_proba(train_x))\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index = lr.classes_,\n",
    "                  columns = lr.classes_)\n",
    "df_cm = (df_cm / df_cm.sum(axis=0))*100\n",
    "plt.figure(figsize = (20,15))\n",
    "sn.set(font_scale=1) # for label size\n",
    "pal = sns.diverging_palette(240, 10, n=10)\n",
    "plt.suptitle(('Mean Accuracy 5 fold: %.3f std: %.3f' % (np.mean(n_scores),  np.std(n_scores))), y=1.05, fontsize=18)\n",
    "\n",
    "#Plot precision recall and recall\n",
    "table = plt.table(cellText=metric.values,colWidths = [1]*len(metric.columns),\n",
    "rowLabels=metric.index,\n",
    "colLabels=metric.columns,\n",
    "cellLoc = 'center', rowLoc = 'center',\n",
    "loc='bottom', bbox=[0.25, -0.6, 0.5, 0.3])\n",
    "table.scale(1, 2)\n",
    "table.set_fontsize(16)\n",
    "\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16},cmap=pal) # font size\n",
    "print(metrics.classification_report(train_label, model.predict(train_x), digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-potential",
   "metadata": {},
   "source": [
    "## Plotting output of LR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_save = adata.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-consciousness",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-essay",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = adata.obs[common_cat].values\n",
    "predict_label = train_label[subset_predict]\n",
    "\n",
    "pred_out = pd.DataFrame(model.predict(predict_x),columns = ['predicted'],index = adata.obs.index[adata.obs[common_cat].isin(group2)])\n",
    "pred_out['orig_labels'] = predict_label\n",
    "proba = pd.DataFrame(model.predict_proba(predict_x),columns = lr.classes_,index = adata.obs.index[adata.obs[common_cat].isin(group2)])\n",
    "pred_out = pred_out.join(proba)\n",
    "\n",
    "pred_out.to_csv(save_path + '/pred_out_' + run_date + '.csv')\n",
    "\n",
    "\n",
    "\n",
    "model_mean_probs = pred_out.loc[:, pred_out.columns != 'predicted'].groupby('orig_labels').mean()\n",
    "#model_mean_probs = pred_out.loc[:, pred_out.columns != 'predicted'].groupby('orig_labels').median()\n",
    "model_mean_probs = model_mean_probs #*100\n",
    "model_mean_probs = model_mean_probs.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "crs_tbl = model_mean_probs.copy()\n",
    "# Sort df columns by rows\n",
    "crs_tbl = crs_tbl.sort_values(by =list(crs_tbl.index), axis=1,ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "# Plot_df_heatmap(crs_tbl, cmap='coolwarm', rotation=90, vmin=20, vmax=70)\n",
    "pal = sns.diverging_palette(240, 10, n=10)\n",
    "plt.figure(figsize=(20,15))\n",
    "\n",
    "sns.set(font_scale=0.8)\n",
    "g = sns.heatmap(crs_tbl, cmap=pal,  annot=True,vmin=0, vmax=1, linewidths=1, center=0.5, square=True, cbar_kws={\"shrink\": 0.5})\n",
    "plt.ylabel(\"Original labels\")\n",
    "plt.xlabel(\"Training labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-suspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(crs_tbl.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(crs_tbl.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-spencer",
   "metadata": {},
   "outputs": [],
   "source": [
    "crs_tbl= crs_tbl[[\n",
    "'Sinusoidal_EC_training',\n",
    "'Prolif_Sinusoidal_EC_training',\n",
    "'Immature_EC_training',\n",
    "'VWF_EC_training',\n",
    "'AEC_training',\n",
    "'HE_training',\n",
    "'Prolif_AEC_training'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "crs_tbl= crs_tbl.T[[\n",
    "'1_Endo1_GJAhi_prediction', '2_Endo2_APLNRhi_prediction'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-boring",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot_df_heatmap(crs_tbl, cmap='coolwarm', rotation=90, vmin=20, vmax=70)\n",
    "pal = sns.diverging_palette(240, 10, n=10)\n",
    "plt.figure(figsize=(20,15))\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "g = sns.heatmap(crs_tbl, cmap=pal,  annot=True,vmin=0, vmax=1, linewidths=1, center=0.5, square=True, cbar_kws={\"shrink\": 0.5})\n",
    "\n",
    "font1 = {'size':25}\n",
    "font2 = {'size':20}\n",
    "\n",
    "plt.ylabel(\"Original labels\", fontdict = font2)\n",
    "plt.xlabel(\"Training labels\", fontdict = font2)\n",
    "plt.title(\"Prediction heatmap\", fontdict = font1)\n",
    "plt.savefig(save_path + \"/LR_predictions_heatmap_with_predictions.pdf\")\n",
    "crs_tbl.to_csv(save_path + \"/pre-freq_LR_predictions_cross_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-clerk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot_df_heatmap(crs_tbl, cmap='coolwarm', rotation=90, vmin=20, vmax=70)\n",
    "pal = sns.diverging_palette(240, 10, n=10)\n",
    "plt.figure(figsize=(20,15))\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "g = sns.heatmap(crs_tbl, cmap=pal,  annot=False,vmin=0, vmax=1, linewidths=1, center=0.5, square=True, cbar_kws={\"shrink\": 0.5})\n",
    "\n",
    "font1 = {'size':25}\n",
    "font2 = {'size':20}\n",
    "\n",
    "plt.ylabel(\"Original labels\", fontdict = font2)\n",
    "plt.xlabel(\"Training labels\", fontdict = font2)\n",
    "plt.title(\"Prediction heatmap\", fontdict = font1)\n",
    "plt.savefig(save_path + \"/LR_predictions_heatmap.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-cause",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-nurse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-preview",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-island",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "agm = sc.read(\"/home/jovyan/YS_project/New_AGM_comparison/Endothelium_comparison_4_models_19112021/Eddie_AGM_for_endothlium_comparison_23112021.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-remainder",
   "metadata": {},
   "outputs": [],
   "source": [
    "agm.obs['cell_types'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-queue",
   "metadata": {},
   "outputs": [],
   "source": [
    "agm.obs.to_csv('AGM_eddie_metadata_for_sup_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-arbor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-mailman",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_env_Hlab",
   "language": "python",
   "name": "python_env_hlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
