{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-classic",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "#import pkg_resources\n",
    "#required = {'harmonypy','sklearn','scanpy','pandas', 'numpy', 'bbknn', 'scipy', 'matplotlib', 'seaborn' ,'scipy'}\n",
    "#installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "#missing = required - installed\n",
    "#if missing:\n",
    "#    print(\"Installing missing packages:\" )\n",
    "#    print(missing)\n",
    "#    python = sys.executable\n",
    "#    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
    "\n",
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from bbknn import bbknn\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from geosketch import gs\n",
    "from numpy import cov\n",
    "import scipy.cluster.hierarchy as spc\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn\n",
    "import harmonypy as hm\n",
    "from pathlib import Path\n",
    "\n",
    "sc.settings.verbosity = 3  # verbosity: errors (0), warnings (1), info (2), hints (3)\n",
    "sc.settings.set_figure_params(dpi=80, color_map='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-berry",
   "metadata": {},
   "source": [
    "# LR script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-feeling",
   "metadata": {},
   "source": [
    "# Only block to edit in LR script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-julian",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Introduce variables\n",
    "# Note that this script expects raw data to be in \"non-batch-corrected\" adata.raw.X. \n",
    "\n",
    "# Required: Introduce the path you'd like to save figures or data to \n",
    "save_path = \"/home/jovyan/mount_farm/nfs/ar32/YS/Cite_Seq/add_new_meta_from_issac/LR_outs_YS_against_YS_citeseq_new_anno_20220401/\"\n",
    "\n",
    "# Required: Name of first object\n",
    "data1 = \"YS_main_object_training\"\n",
    "# Provide path to obj1 // landscape/training data\n",
    "#Object1 = \"/home/jovyan/YS_project/YS/Data_objects/YS_object_14092021/YS_with_new_meta_20210919.h5ad\"\n",
    "Object1 = \"/home/jovyan/YS_project/YS/Data_objects/final_objects/A4_V7_YS_integrated_data_singlets_with_raw_counts_for_MS_plotting_20211111_with_obsp.h5ad\"\n",
    "# Provide categorical to join between datasets\n",
    "cat1 = \"broad_cell.labels\"\n",
    "\n",
    "# Required: Name of second object\n",
    "data2 = \"YS_CiteSeq_RNA_mito_removed\"\n",
    "# Provide path to obj2 // prediction/projection data\n",
    "Object2 = \"/home/jovyan/mount_farm/nfs/ar32/YS/Cite_Seq/add_new_meta_from_issac/cite_seq_rna_simple_raw_20220331.h5ad\"\n",
    "# Provide categorical to join between datasets\n",
    "cat2 = \"broad_anno\"\n",
    "\n",
    "\n",
    "# Required: LR Model Options\n",
    "penalty='l2' # can be [\"l1\",\"l2\",\"elasticnet\"]\n",
    "sparcity=0.2\n",
    "max_iter = 1000 #Increase if experiencing max iter issues\n",
    "l1_ratio = 0.5 #If using elasticnet, tis controls the ratio between l1 and l2\n",
    "\n",
    "# Optional: Batch correction options (this is for correction of eventual combined dataset for data1 and data2)\n",
    "# If you do not have a batch variable for either data1 or data2, please add a \"filler\" column in the relevent adata.obs\n",
    "# for the purposes of batch_correction and batch args below.\n",
    "# e.g., adata.obs[\"whatever\"] = \"something\"; batch=\"whatever\"\n",
    "batch_correction = \"Harmony\" # Will accept Harmony, BBKNN or False as options\n",
    "batch = [\"lanes\", \"orig.ident\"] # Will accept any batch categorical. Comma space a batch categorical for each dataset. Position 1 is for data1, position 2 is for data2\n",
    "\n",
    "# Optional: miscellaneous Options.   \n",
    "subsample_train = False # Samples the training data to the smallest fraction (highly dependent on resolution of input celltype categorical). This corrects for proportional differences between celltype labels of interest in the training data. E.g., training data has 50,000 B cells, 20,000 T cells and 100 HSCs. This function will subsample all training to 100 cells per cell type. \n",
    "subsample_prop = 0.2 # Give this option a proprtion to subsample to(e.g 0.2), if NA given, will subsample to smallest population\n",
    "subsample_predict = False\n",
    "subsample_prop_predict = 0.5\n",
    "remove_non_high_var = True\n",
    "\n",
    "train_x = 'X_pca' # Define the resource to train and predict on, PCA, X or UMAP (#if you wish to use gene expression, train_x = 'X')\n",
    "remove_effect_of_custom_gene_list = '' # \"./cell_cycle_genes.csv\" #remove a custom list of genes from just the variable genes to compute PCA from. Your .csv should have HGNC gene names in the first column to be read in as a vector, any column name is fine.\n",
    "use_raw = True # Do you want to use adata.raw.X (recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-summit",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Rest of LR script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-premium",
   "metadata": {},
   "source": [
    "## Combining data and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-oriental",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Check if filepaths are good\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "    \n",
    "if (Path(Object1).is_file() & Path(Object2).is_file()):\n",
    "    print(\"adata file paths detetcted, proceeding to load\")\n",
    "    adata = sc.read(Object1)\n",
    "    adata2 = sc.read(Object2)\n",
    "    del adata.uns\n",
    "    del adata2.uns\n",
    "else: \n",
    "    raise TypeError(\"one or more .h5ad paths cannot be accessed\")\n",
    "\n",
    "# altering scanpy setting so that we can save it to our defined directory\n",
    "sc._settings.ScanpyConfig(figdir=save_path)\n",
    "\n",
    "# Combine and pre-process data to match correlations across PCA\n",
    "\n",
    "# Module to detect shape mismatch and alternatively rebuild adata\n",
    "if(use_raw==True):\n",
    "    print('option detected to use raw data, proceeding to check if raw exists and if it matches data.X')\n",
    "    if (hasattr(adata.raw, \"X\")):\n",
    "        try: adata.X =  adata.raw.X  ; print('no mismatch in shape for adata detected')\n",
    "        except: print(\"adata.X shape mismatched with adata.raw.X, proceeding to re-build data\") ; adata = adata.raw.to_adata()\n",
    "    else:\n",
    "        print(\"no raw data detected in adata! proceeding to create raw partition from adata.X\")\n",
    "        adata.raw = adata\n",
    "        \n",
    "    if (hasattr(adata2.raw, \"X\")):\n",
    "        try: adata2.X = adata2.raw.X ; print('no mismatch in shape for adata2 detected')\n",
    "        except: print(\"adata2.X shape mismatched with adata.raw.X, proceeding to re-build data\") ; adata2 = adata2.raw.to_adata()\n",
    "    else:\n",
    "        print(\"no raw data detected in adata! proceeding to create raw partition from adata.X\")\n",
    "        adata2.raw = adata2\n",
    "           \n",
    "# Define intersecting genes between datasets\n",
    "adata_genes = list(adata.var.index)\n",
    "adata2_genes = list(adata2.var.index)\n",
    "keep_SC_genes = list(set(adata_genes) & set(adata2_genes))\n",
    "print(\"keep gene list = \" , len(keep_SC_genes), \"adata1 gene length = \", len(adata_genes) , \"adata2 gene length = \", len(adata2_genes) )\n",
    "\n",
    "# Remove non-intersecting genes (this step will remove cite-seq data if training data is pure RNA seq)\n",
    "adata_intersect1 = adata[:, keep_SC_genes]\n",
    "adata = adata_intersect1\n",
    "adata_intersect2 = adata2[:, keep_SC_genes]\n",
    "adata2 = adata_intersect2\n",
    "\n",
    "# Optional subsampling of training data to \n",
    "if(subsample_train == True):\n",
    "    \n",
    "    if not(subsample_prop==\"NA\"):\n",
    "        print(\"option to subsample by proportion chosen\")\n",
    "        prop = subsample_prop\n",
    "        data = adata.obs[:]\n",
    "        grouped = data.groupby(cat1)\n",
    "        df = grouped.apply(lambda x: x.sample(frac=prop))\n",
    "        df = df.droplevel(cat1)\n",
    "        keep = df.index\n",
    "        adata = adata[adata.obs.index.isin(keep)]\n",
    "    else:\n",
    "        print(\"subsample by smallest population\")\n",
    "        data = adata.obs\n",
    "        data = data.sample(frac=1).groupby(cat1).head(min(adata.obs.groupby(cat1).size()))\n",
    "        keep = data.index\n",
    "        adata = adata[adata.obs.index.isin(keep)]\n",
    "        \n",
    "# Optional subsampling of training data to \n",
    "if(subsample_predict == True):\n",
    "    if not(subsample_prop_predict==\"NA\"):\n",
    "        print(\"option to subsample by proportion chosen\")\n",
    "        prop = subsample_prop_predict\n",
    "        data = adata2.obs[:]\n",
    "        grouped = data.groupby(cat2)\n",
    "        df = grouped.apply(lambda x: x.sample(frac=prop))\n",
    "        df = df.droplevel(cat2)\n",
    "        keep = df.index\n",
    "        adata2 = adata2[adata2.obs.index.isin(keep)]\n",
    "    else:\n",
    "        print(\"subsample by smallest population\")\n",
    "        data = adata2.obs\n",
    "        data = data.sample(frac=1).groupby(cat2).head(min(adata.obs.groupby(cat2).size()))\n",
    "        keep = data.index\n",
    "        adata2 = adata2[adata2.obs.index.isin(keep)]\n",
    "\n",
    "# Create a common batch column and do simple sanity check for batch variables\n",
    "if not((batch_correction == \"False\") and (len(batch)>1)):\n",
    "    print(\"Batch correction option detected, proceeding to format batch variables\")\n",
    "    batch_var = \"lr_batch\"\n",
    "    adata.obs[\"lr_batch\"] = adata.obs[batch[0]]\n",
    "    adata2.obs[\"lr_batch\"] = adata2.obs[batch[1]]\n",
    "else: raise TypeError(\"Batch correction option detected but requires at least one categorical for each dataset!\")\n",
    "\n",
    "# Create a common obs column in both datasets containing the data origin tag\n",
    "common_cat = \"corr_concat\" \n",
    "adata.obs[common_cat] = adata.obs[cat1].astype(str) + data1\n",
    "adata2.obs[common_cat] = adata2.obs[cat2].astype(str) + data2\n",
    "adata.obs = adata.obs.astype('category')\n",
    "adata2.obs = adata2.obs.astype('category')\n",
    "concat = adata2.concatenate(adata, join='inner',index_unique=None, batch_categories=None)\n",
    "adata = concat[:]\n",
    "sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4)\n",
    "sc.pp.log1p(adata)\n",
    "sc.pp.highly_variable_genes(adata, min_mean=0.1, max_mean=4)\n",
    "sc.pp.scale(adata, zero_center=False, max_value=None, copy=False) #zero_center=True (densifies output)\n",
    "\n",
    "# Optionally remove genes of known confounding effect from variable list\n",
    "if not (Path(remove_effect_of_custom_gene_list).is_file()):\n",
    "    print(\"Custom gene list option is not selected or path is not readbale, proceeding with no variable removal\")\n",
    "else: \n",
    "    print(\"Custom gene removal list detected, proceeding to remove intersect from variable genes\")\n",
    "    regress_list = pd.read_csv(remove_effect_of_custom_gene_list)\n",
    "    regress_list = regress_list.iloc[:, 0]\n",
    "    adata.var[\"highly_variable\"][adata.var.index.isin(regress_list)] = \"False\"\n",
    "\n",
    "#Optionally remove genes that do not contribute to variance in combined data::Use only if training and predicting withsim reduced data    \n",
    "if(remove_non_high_var==True):\n",
    "    high_var = list(adata.var[\"highly_variable\"][adata.var[\"highly_variable\"]==True])\n",
    "    adata = adata[:, adata.var[\"highly_variable\"].isin(high_var)]   \n",
    "\n",
    "# Now compute PCA\n",
    "sc.pp.pca(adata, n_comps=50, use_highly_variable=True, svd_solver='arpack')\n",
    "\n",
    "# Batch correction options\n",
    "# The script will test later which Harmony values we should use\n",
    "if not(batch_correction == \"False\"):\n",
    "    sc.pp.neighbors(adata, n_neighbors=15, n_pcs=50)    \n",
    "if(batch_correction == \"Harmony\"):\n",
    "    print(\"Commencing harmony\")\n",
    "    # Create hm subset\n",
    "    adata_hm = adata[:]\n",
    "    # Set harmony variables\n",
    "    data_mat = np.array(adata_hm.obsm[\"X_pca\"])\n",
    "    meta_data = adata_hm.obs\n",
    "    vars_use = [batch_var]\n",
    "    # Run Harmony\n",
    "    ho = hm.run_harmony(data_mat, meta_data, vars_use)\n",
    "    res = (pd.DataFrame(ho.Z_corr)).T\n",
    "    res.columns = ['X{}'.format(i + 1) for i in range(res.shape[1])]\n",
    "    # Insert coordinates back into object\n",
    "    adata_hm.obsm[\"X_pca_back\"]= adata_hm.obsm[\"X_pca\"][:]\n",
    "    adata_hm.obsm[\"X_pca\"] = np.array(res)\n",
    "    # Run neighbours\n",
    "    sc.pp.neighbors(adata_hm, n_neighbors=15, n_pcs=50)\n",
    "    adata = adata_hm[:]\n",
    "    del adata_hm\n",
    "elif(batch_correction == \"BBKNN\"):\n",
    "    print(\"Commencing BBKNN\")\n",
    "    sc.external.pp.bbknn(adata, batch_key=batch_var, approx=True, metric='angular', copy=False, n_pcs=50, trim=None, n_trees=10, use_faiss=True, set_op_mix_ratio=1.0, local_connectivity=15) \n",
    "    \n",
    "print(\"adata1 and adata2 are now combined and preprocessed in 'adata' obj - success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-consciousness",
   "metadata": {},
   "source": [
    "## Logistic regression function to train data set and transfer labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-sheffield",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function require compute power, will take a while \n",
    "\n",
    "#def LR_compare(adata, train_x, train_label, subset_predict, subset_train, penalty=penalty, sparcity=sparcity, \n",
    "#               col_name='predicted'):\n",
    "#\n",
    "#    # adata - training+prediction adata object (combined). Pre-processed already\n",
    "#    # sparsity - larger sparsity, more bins, more conservative predictions, less accurate. Low sparist for clean output\n",
    "#                # A value of 0.2 is reasonable for L2 ridge regression\n",
    "#    # penalty - acts as buffer for assigning bins too harshly\n",
    "#    # train_x - arg refers to where you would like to derive your training reference from, i.e., GEX (X) or/elif.\n",
    "#                # PCA/UMAP in obsm. The two 'if' statements below handle train_x differently based on this\n",
    "#                # Based on train_x, the loops below compute 'train_label' (cell type values in training/landscape data) \n",
    "#                # and 'predict_x'(prediction data equivalent of train_x)\n",
    "#    # train_label - cell type values in training/landscape data\n",
    "#    # subset_predict - mandatory subset of predict_x which contains metadata for expression\n",
    "#    # subset_train - mandatory subset of train_x which contains metadata for expression\n",
    "#    \n",
    "#    # Redefine LR parameters 'penalty' and 'sparsity' if you would like to deviate from defaults set above\n",
    "#    \n",
    "#    # Assign 'lr' as sklearn logistic regression func, with penalty and sparsity defined above\n",
    "#    lr = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  max_iter)\n",
    "#    \n",
    "#    if (penalty == \"l1\"):\n",
    "#        lr = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  max_iter, dual = True, solver = 'liblinear')\n",
    "#    if (penalty == \"elasticnet\"):\n",
    "#        lr = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  max_iter, dual=False,solver = 'saga',l1_ratio=l1_ratio)\n",
    "#\n",
    "#    if train_x == 'X':\n",
    "#        # Define training parameters\n",
    "#        train_label = adata.obs[common_cat].values\n",
    "#        train_label = train_label[subset_train]\n",
    "#        #train_x = adata.X,\n",
    "#        # Define prediction parameters\n",
    "#        #predict_x = train_x\n",
    "#        #train_x = train_x[subset_train, :] # issue line! subset_train = np.array(adata.obs[common_cat].isin(group1))\n",
    "#                                           # group1 = (adata.obs[common_cat][adata.obs[common_cat].str.contains(Data1_group)]).unique()\n",
    "#                                           # Data1_group = data1 = healthy skin data , adata containing subsetting data to get metadata for expression prediction\n",
    "#                                           # adata.X = adata.X[np.array(adata.obs[common_cat].isin(group1)), :]\n",
    "#                                           # train_x = train_x[adata.obs[common_cat].isin(group1)]\n",
    "#                        \n",
    "#        #predict_x = train_x\n",
    "#        #predict_x = predict_x[subset_predict]\n",
    "#        train_x = adata.X[adata.obs.index.isin(list(adata.obs[subset_train].index))]\n",
    "#        predict_x = adata.X[adata.obs.index.isin(list(adata.obs[subset_predict].index))]\n",
    "#\n",
    "#    elif train_x in adata.obsm.keys():\n",
    "#        # Define training parameters\n",
    "#        train_label = adata.obs[common_cat].values\n",
    "#        train_label = train_label[subset_train]\n",
    "#        train_x = adata.obsm[train_x]\n",
    "#        predict_x = train_x\n",
    "#        train_x = train_x[subset_train, :]\n",
    "#        # Define prediction parameters\n",
    "#        predict_x = predict_x[subset_predict]\n",
    "#        predict_x = pd.DataFrame(predict_x)\n",
    "#        predict_x.index = adata.obs[subset_predict].index\n",
    "#\n",
    "#    # Train predictive model using user defined partition labels (train_x ,train_label, predict_x)\n",
    "#    model = lr.fit(train_x, train_label)\n",
    "#    lr.fit(train_x, train_label)\n",
    "#    predict = lr.predict_proba(predict_x)\n",
    "#\n",
    "#    # Create prediction table and map to adata.obs (in adata.obs[\"predict\"] in the combined object), for the cells that\n",
    "#    # are in predict dataset\n",
    "#    predict = lr.predict(predict_x)\n",
    "#    predict = pd.DataFrame(predict)\n",
    "#    predict.index = adata.obs[subset_predict].index\n",
    "#    adata.obs[col_name] = adata.obs.index\n",
    "#    adata.obs[col_name] = adata.obs[col_name].map(predict[0])\n",
    "\n",
    "# Function to plot heatmap by percentage\n",
    "def plot_df_heatmap(df, cmap='viridis', title=None, figsize=(7, 7), rotation=90, save=None, **kwargs):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    im = ax.imshow(df, cmap=cmap, aspect='auto', **kwargs)\n",
    "    if 0 < rotation < 90:\n",
    "        horizontalalignment = 'right'\n",
    "    else:\n",
    "        horizontalalignment = 'center'\n",
    "    plt.xticks(\n",
    "        range(len(df.columns)),\n",
    "        df.columns,\n",
    "        rotation=rotation,\n",
    "        horizontalalignment=horizontalalignment,\n",
    "    )\n",
    "    plt.yticks(range(len(df.index)), df.index)\n",
    "    if title:\n",
    "        fig.suptitle(title)\n",
    "    #fig.colorbar(im)\n",
    "    if save:\n",
    "        plt.savefig(fname=save, bbox_inches='tight', pad_inches=0.1)\n",
    "\n",
    "# Plot probability table by html\n",
    "def cross_table(adata, x, y, normalise=None, highlight=False, subset=None):                                                                                                                                                                                              \n",
    "    \"\"\"Make a cross table comparing two categorical annotations\n",
    "    \"\"\"\n",
    "    x_attr = adata.obs[x]\n",
    "    y_attr = adata.obs[y]\n",
    "    if subset is not None:\n",
    "        x_attr = x_attr[subset]\n",
    "        y_attr = y_attr[subset]\n",
    "    crs_tbl = pd.crosstab(x_attr, y_attr)\n",
    "    if normalise == 'x':\n",
    "        x_sizes = x_attr.groupby(x_attr).size().values\n",
    "        crs_tbl = (crs_tbl.T / x_sizes).round(2).T\n",
    "    elif normalise == 'y':\n",
    "        y_sizes = x_attr.groupby(y_attr).size().values\n",
    "        crs_tbl = (crs_tbl / y_sizes).round(2)\n",
    "    if highlight:\n",
    "        return crs_tbl.style.background_gradient(cmap='viridis', axis=0)\n",
    "    return crs_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-planning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the separator category in the column of interest, this works by partial matches and enables a-symmetric \n",
    "# comparisons\n",
    "Data1_group = data1\n",
    "Data2_group = data2\n",
    "# Define the common .obs column between concatinated data\n",
    "common_cat = \"corr_concat\"\n",
    "\n",
    "# This block defines subset_predict and subset_train and also runs LR_compare function\n",
    "group1 = (adata.obs[common_cat][adata.obs[common_cat].str.contains(Data1_group)]).unique()\n",
    "group1 = list(group1)\n",
    "group2 = (adata.obs[common_cat][adata.obs[common_cat].str.contains(Data2_group)]).unique()\n",
    "group2 = list(group2)\n",
    "subset_predict = np.array(adata.obs[common_cat].isin(group2))\n",
    "subset_train = np.array(adata.obs[common_cat].isin(group1))\n",
    "train_label = (adata.obs[common_cat][adata.obs[common_cat].isin(group1)]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-festival",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign 'lr' as sklearn logistic regression func, with penalty and sparsity defined above\n",
    "lr = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  max_iter)\n",
    "\n",
    "if (penalty == \"l1\"):\n",
    "    lr = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  max_iter, dual = True, solver = 'liblinear',multi_class = 'ovr' ) # one-vs-rest\n",
    "if (penalty == \"elasticnet\"):\n",
    "    lr = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  max_iter, dual=False,solver = 'saga',l1_ratio=l1_ratio,multi_class = 'multinomial')\n",
    "if train_x == 'X':\n",
    "    # Define training parameters\n",
    "    train_label = adata.obs[common_cat].values\n",
    "    predict_label = train_label[subset_predict]\n",
    "    train_label = train_label[subset_train]\n",
    "    train_x = adata.X[adata.obs.index.isin(list(adata.obs[subset_train].index))]\n",
    "    predict_x = adata.X[adata.obs.index.isin(list(adata.obs[subset_predict].index))]\n",
    "elif train_x in adata.obsm.keys():\n",
    "    # Define training parameters\n",
    "    train_label = adata.obs[common_cat].values\n",
    "    predict_label = train_label[subset_predict]\n",
    "    train_label = train_label[subset_train]\n",
    "    train_x = adata.obsm[train_x]\n",
    "    predict_x = train_x\n",
    "    train_x = train_x[subset_train, :]\n",
    "    # Define prediction parameters\n",
    "    predict_x = predict_x[subset_predict]\n",
    "    predict_x = pd.DataFrame(predict_x)\n",
    "    predict_x.index = adata.obs[subset_predict].index\n",
    "# Train predictive model using user defined partition labels (train_x ,train_label, predict_x)\n",
    "lr.fit(train_x, train_label)\n",
    "predict_proba = lr.predict_proba(predict_x)\n",
    "# Create prediction table and map to adata.obs (in adata.obs[\"predict\"] in the combined object), for the cells that\n",
    "# are in predict dataset\n",
    "predict = lr.predict(predict_x)\n",
    "predict = pd.DataFrame(predict)\n",
    "predict.index = adata.obs[subset_predict].index\n",
    "col_name='predicted'\n",
    "adata.obs[col_name] = adata.obs.index\n",
    "adata.obs[col_name] = adata.obs[col_name].map(predict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-turner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-potter",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/home/jovyan/mount_farm/nfs/ar32/YS/Cite_Seq/add_new_meta_from_issac/LR_outs_YS_against_YS_citeseq_new_anno_20220401/YS_training_against_YS_citeseq_model_20220401.sav'\n",
    "pickle.dump(lr, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-convertible",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save combined object\n",
    "adata.write('Combined_YS_main_CiteSeq_RNA_object_mito_removed_probability_comparison_20220401.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = adata.obs[common_cat].values\n",
    "predict_label = train_label[subset_predict]\n",
    "\n",
    "pred_out = pd.DataFrame(model.predict(predict_x),columns = ['predicted'],index = adata.obs.index[adata.obs[common_cat].isin(group2)])\n",
    "pred_out['orig_labels'] = predict_label\n",
    "proba = pd.DataFrame(model.predict_proba(predict_x),columns = lr.classes_,index = adata.obs.index[adata.obs[common_cat].isin(group2)])\n",
    "pred_out = pred_out.join(proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-latino",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_date = '20220401'\n",
    "pred_out.to_csv(save_path + '/pred_out_' + run_date + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-antigua",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mean_probs = pred_out.loc[:, pred_out.columns != 'predicted'].groupby('orig_labels').mean()\n",
    "#model_mean_probs = pred_out.loc[:, pred_out.columns != 'predicted'].groupby('orig_labels').median()\n",
    "model_mean_probs = model_mean_probs #*100\n",
    "model_mean_probs = model_mean_probs.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "crs_tbl = model_mean_probs.copy()\n",
    "# Sort df columns by rows\n",
    "crs_tbl = crs_tbl.sort_values(by =list(crs_tbl.index), axis=1,ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "# Plot_df_heatmap(crs_tbl, cmap='coolwarm', rotation=90, vmin=20, vmax=70)\n",
    "pal = sns.diverging_palette(240, 10, n=10)\n",
    "plt.figure(figsize=(20,15))\n",
    "\n",
    "sns.set(font_scale=0.8)\n",
    "g = sns.heatmap(crs_tbl, cmap=pal,  annot=True,vmin=0, vmax=1, linewidths=1, center=0.5, square=True, cbar_kws={\"shrink\": 0.5})\n",
    "plt.ylabel(\"Original labels\")\n",
    "plt.xlabel(\"Training labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-beginning",
   "metadata": {},
   "outputs": [],
   "source": [
    "crs_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-cliff",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(crs_tbl.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-exercise",
   "metadata": {},
   "outputs": [],
   "source": [
    "crs_tbl2 = crs_tbl.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-premium",
   "metadata": {},
   "outputs": [],
   "source": [
    "crs_tbl2= crs_tbl2[['ProgenitorsYS_main_object_training',\n",
    "'LymphoidYS_main_object_training',\n",
    "'DCYS_main_object_training',\n",
    "'MonocyteYS_main_object_training',\n",
    "'MacrophageYS_main_object_training',\n",
    "'MicrogliaYS_main_object_training',\n",
    "'Granulocyte_precursorsYS_main_object_training',\n",
    "'Mast_cellYS_main_object_training',\n",
    "'MKYS_main_object_training',\n",
    "'ErythroidYS_main_object_training',\n",
    "'EndotheliumYS_main_object_training',\n",
    "'FibroblastYS_main_object_training',\n",
    "'Smooth_MuscleYS_main_object_training',\n",
    "'MesotheliumYS_main_object_training',\n",
    "'EndodermYS_main_object_training']]\n",
    "crs_tbl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-chosen",
   "metadata": {},
   "outputs": [],
   "source": [
    "crs_tbl2 = crs_tbl2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-finder",
   "metadata": {},
   "outputs": [],
   "source": [
    "crs_tbl2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-indonesian",
   "metadata": {},
   "outputs": [],
   "source": [
    "crs_tbl2 = crs_tbl2[['ProgenitorsYS_CiteSeq_RNA_mito_removed',\n",
    "'LymphoidYS_CiteSeq_RNA_mito_removed',\n",
    "'pDC precursorYS_CiteSeq_RNA_mito_removed',\n",
    "'MonocyteYS_CiteSeq_RNA_mito_removed',\n",
    "'MacrophageYS_CiteSeq_RNA_mito_removed',\n",
    "'MicrogliaYS_CiteSeq_RNA_mito_removed',\n",
    "'Mast_cellYS_CiteSeq_RNA_mito_removed',\n",
    "'MKYS_CiteSeq_RNA_mito_removed',\n",
    "'ErythroidYS_CiteSeq_RNA_mito_removed',\n",
    "'EndotheliumYS_CiteSeq_RNA_mito_removed',\n",
    "'FibroblastYS_CiteSeq_RNA_mito_removed',\n",
    "'Smooth_MuscleYS_CiteSeq_RNA_mito_removed',\n",
    "'MesotheliumYS_CiteSeq_RNA_mito_removed',\n",
    "'EndodermYS_CiteSeq_RNA_mito_removed']]\n",
    "crs_tbl2 = crs_tbl2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-antenna",
   "metadata": {},
   "outputs": [],
   "source": [
    "crs_tbl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-intellectual",
   "metadata": {},
   "outputs": [],
   "source": [
    "crs_tbl2 = crs_tbl2.T #(so YS is on left, cite-seq on bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-steps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot_df_heatmap(crs_tbl, cmap='coolwarm', rotation=90, vmin=20, vmax=70)\n",
    "pal = sns.diverging_palette(240, 10, n=10)\n",
    "plt.figure(figsize=(20,15))\n",
    "\n",
    "sns.set(font_scale=0.8)\n",
    "g = sns.heatmap(crs_tbl2, cmap=pal,  annot=False,vmin=0, vmax=1, linewidths=1, center=0.5, square=True, cbar_kws={\"shrink\": 0.5})\n",
    "plt.ylabel(\"Original labels\")\n",
    "plt.xlabel(\"Training labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-doubt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot_df_heatmap(crs_tbl, cmap='coolwarm', rotation=90, vmin=20, vmax=70)\n",
    "pal = sns.diverging_palette(240, 10, n=10)\n",
    "plt.figure(figsize=(20,15))\n",
    "\n",
    "sns.set(font_scale=0.8)\n",
    "g = sns.heatmap(crs_tbl2, cmap=pal,  annot=False,vmin=0, vmax=1, linewidths=1, center=0.5, square=True, cbar_kws={\"shrink\": 0.5})\n",
    "plt.ylabel(\"Original labels\")\n",
    "plt.xlabel(\"Training labels\")\n",
    "crs_tbl.to_csv(save_path + \"/crs_tbl_20220401.csv\")\n",
    "plt.savefig(save_path + \"/marker_comparison_heatmap_20220401.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_env_Hlab",
   "language": "python",
   "name": "python_env_hlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
