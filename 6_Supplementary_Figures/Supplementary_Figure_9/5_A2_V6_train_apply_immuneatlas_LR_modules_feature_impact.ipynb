{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "meaningful-incident",
   "metadata": {},
   "source": [
    "# Train model from dominiguez immune atlas in myeloid compartment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-springer",
   "metadata": {},
   "source": [
    "# LR multi-tissue cross-comparison\n",
    "\n",
    "##### Ver:: A1_V5\n",
    "##### Author(s) : Issac Goh\n",
    "##### Date : 220823;YYMMDD\n",
    "### Author notes\n",
    "    - Current defaults scrpae data from web, so leave as default and run\n",
    "    - slices model and anndata to same feature shape, scales anndata object\n",
    "    - added some simple benchmarking\n",
    "    - creates dynamic cutoffs for probability score (x*sd of mean) in place of more memory intensive confidence scoring\n",
    "    - Does not have majority voting set on as default, but module does exist\n",
    "    - Multinomial logistic relies on the (not always realistic) assumption of independence of irrelevant alternatives whereas a series of binary logistic predictions does not. collinearity is assumed to be relatively low, as it becomes difficult to differentiate between the impact of several variables if this is not the case\n",
    "    \n",
    "### Features to add\n",
    "    - Add ability to consume anndata zar format for sequential learning\n",
    "### Modes to run in\n",
    "    - Run in training mode\n",
    "    - Run in projection mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# import pkg_resources\n",
    "# required = {'harmonypy','sklearn','scanpy','pandas', 'numpy', 'scipy', 'matplotlib', 'seaborn' ,'scipy'}\n",
    "# installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "# missing = required - installed\n",
    "# if missing:\n",
    "#    print(\"Installing missing packages:\" )\n",
    "#    print(missing)\n",
    "#    python = sys.executable\n",
    "#    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "#from geosketch import gs\n",
    "from numpy import cov\n",
    "import scipy.cluster.hierarchy as spc\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import psutil\n",
    "import random\n",
    "import threading\n",
    "import tracemalloc\n",
    "import itertools\n",
    "import math\n",
    "import warnings\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-librarian",
   "metadata": {},
   "source": [
    "# Train ABM model & validate, project onto adult Mye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-samoa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "'pan_fetal':'/nfs/team205/ig7/resources/scripts_dont_modify/logit_regression_models/adifa_lr/celltypist_model.Pan_Fetal_Human.pkl',\n",
    "'pan_fetal_wget':'https://celltypist.cog.sanger.ac.uk/models/Pan_Fetal_Suo/v2/Pan_Fetal_Human.pkl',\n",
    "'adata_scvi':'/nfs/team205/ig7/mount/gdrive/g_cloud/projects/amniontic_fluid/scvi_low_dim_model.sav',\n",
    "'adata_ldvae':'/nfs/team205/ig7/mount/gdrive/g_cloud/projects/amniontic_fluid/ldvae_low_dim_model.sav',\n",
    "'adata_harmony':'/nfs/team205/ig7/work_backups/backup_210306/projects/amiotic_fluid/train_low_dim_model/organ_low_dim_model.sav',\n",
    "'test_low_dim_ipsc_ys':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/YS_030522_notebooks/Integrating_HM_data_030522/YS_logit/lr_model.sav',\n",
    "'YS_X':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/resources/YS_X_model_080922.sav',\n",
    "'YS_X_V3':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/rebuttal_figs_010922/train_YS_full_X_model/YS_X_A2_V12_lvl3_ELASTICNET_YS.sav',\n",
    "'SK_model':'/nfs/team205/ig7/resources/scripts_dont_modify/logit_regression_models/LR_app_format/hudaa_skin/for_hudaa_A1_V2',\n",
    "'Hudaa_model_trained':'/nfs/team298/hg6/Fetal_skin/LR_15012023/train-all_model.pkl',\n",
    "'A1_V1_LUNG_MYE_model_IG':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/rebuttal_figs_010922/ling_adult_macs/LR_transfer_Lung_brain/A1_V1_LUNG_MYE_model_IG',\n",
    "'A1_V1_immuneatlas_model_IG':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/rebuttal_figs_010922/ling_adult_macs/LR_transfer_Lung_brain/A1_V1_immuneatlas_model_IG'\n",
    "}\n",
    "\n",
    "adatas_dict = {\n",
    "'human_lung_mye':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/YS_data/Theiss_lung_atlas/Myeloid_subset_HLCA_lung_processed.h5ad',\n",
    "'Fetal_skin_raw': '/nfs/team298/hg6/Fetal_skin/data/FS_raw_sub.h5ad',\n",
    "'vascular_organoid': '/nfs/team298/hg6/Fetal_skin/data/vasc_org_raw.h5ad',\n",
    "'YS':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/YS_data/Submission_2_data/A2_V5_scvi_YS_integrated/A2_V5_scvi_YS_integrated_raw_qc_scr_umap.h5ad',\n",
    "'YS_test':'/nfs/team205/ig7/resources/scripts_dont_modify/logit_regression_models/LR_app_format/ys_test_data.h5ad',\n",
    "'YS_A2_V10_X_raw':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/YS_data/Submission_2_data/A2_V10_scvi_YS_integrated/A2_V10_raw_counts_full_no_obs.h5ad',\n",
    "'YS_A2_V10_X':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/YS_data/Submission_2_data/A2_V10_scvi_YS_integrated/A2_V10_qc_raw.h5ad',\n",
    "'ABM':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/rebuttal_figs_010922/ling_adult_macs/LR_transfer_Lung_brain/ABM_re_constructed_IG.h5ad',\n",
    "'mye_immune_atlas':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/YS_data/tissue_immune_atlas_dominiguez/CountAdded_PIP_myeloid_object_for_cellxgene.h5ad',\n",
    "'global_immune_atlas':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/YS_data/tissue_immune_atlas_dominiguez/CountAdded_PIP_global_object_for_cellxgene.h5ad',\n",
    "'pan_organ_mye':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/rebuttal_figs_010922/ling_adult_macs/A1_V1_LING_ADULT_IG_annot.h5ad',\n",
    "}\n",
    "\n",
    "# Variable assignment\n",
    "train_model = True\n",
    "feat_use = 'Manually_curated_celltype'\n",
    "adata_key = 'global_immune_atlas'#'fliv_wget_test' # key for dictionary entry containing local or web path to adata/s can be either url or local \n",
    "data_merge = False # read and merge multiple adata (useful, but keep false for now)\n",
    "model_key = 'A1_V1_immuneatlas_model_IG'#'test_low_dim_ipsc_ys'# key for model of choice can be either url or local \n",
    "train_x_partition = 'X' # what partition was the data trained on? To keep simple, for now only accepts 'X'\n",
    "dyn_std = 1.96 # Dynamic cutoffs using std of the mean for each celltype probability, gives a column notifying user of uncertain labels 1 == 68Ci, 1.96 = 95CI\n",
    "freq_redist = 'Manually_curated_celltype'#'cell.labels'#'False#'cell.labels'#False # False or key of column in anndata object which contains labels/clusters // not currently implemented\n",
    "partial_scale = False # should data be scaled in batches?\n",
    "QC_normalise = True # should data be normalised?\n",
    "\n",
    "# training variables\n",
    "penalty='elasticnet' # can be [\"l1\",\"l2\",\"elasticnet\"]\n",
    "sparcity=0.5 # C penalty for degree of regularisation\n",
    "thread_num = -1\n",
    "l1_ratio = 0.5 # ratio between L1 and L2 regulrisatiuon depending on penatly method\n",
    "\n",
    "#\n",
    "\n",
    "# If low dim & not in keys\n",
    "batch_key = 'Donor'\n",
    "batch_correction = False#'Harmony' #or bbknn\n",
    "theta = 3 #harmony specifc\n",
    "\n",
    "#Sketch training?\n",
    "sketch_obsm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-technical",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from anndata._io.specs import read_elem\n",
    "\n",
    "with h5py.File(\"/nfs/team205/ig7/work_backups/backup_210306/projects/YS/YS_data/tissue_immune_atlas_dominiguez/CountAdded_PIP_global_object_for_cellxgene.h5ad\") as f:\n",
    "    cell_types = read_elem(f[\"obs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-federation",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(cell_types['Manually_curated_celltype'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(cell_types['Manually_curated_celltype'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-gates",
   "metadata": {},
   "source": [
    "# Partial scaling ver\n",
    "- scale across 10 mini bulks/every 100,000 cells\n",
    "- sequential learning for scaling\n",
    "- sequential application of scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-husband",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "#from geosketch import gs\n",
    "from numpy import cov\n",
    "import scipy.cluster.hierarchy as spc\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import psutil\n",
    "import random\n",
    "import threading\n",
    "import tracemalloc\n",
    "import itertools\n",
    "import math\n",
    "import warnings\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "def load_models(model_dict,model_run):\n",
    "    if (Path(model_dict[model_run])).is_file():\n",
    "        # Load data (deserialize)\n",
    "        model = pkl.load(open(model_dict[model_run], \"rb\"))\n",
    "        return model\n",
    "    elif 'http' in model_dict[model_run]:\n",
    "        print('Loading model from web source')\n",
    "        r_get = requests.get(model_dict[model_run])\n",
    "        fpath = './model_temp.sav'\n",
    "        open(fpath , 'wb').write(r_get.content)\n",
    "        model = pkl.load(open(fpath, \"rb\"))\n",
    "        return model\n",
    "\n",
    "def load_adatas(adatas_dict,data_merge, data_key_use,QC_normalise):\n",
    "    if data_merge == True:\n",
    "        # Read\n",
    "        gene_intersect = {} # unused here\n",
    "        adatas = {}\n",
    "        for dataset in adatas_dict.keys():\n",
    "            if 'https' in adatas_dict[dataset]:\n",
    "                print('Loading anndata from web source')\n",
    "                adatas[dataset] = sc.read('./temp_adata.h5ad',backup_url=adatas_dict[dataset])\n",
    "            adatas[dataset] = sc.read(data[dataset])\n",
    "            adatas[dataset].var_names_make_unique()\n",
    "            adatas[dataset].obs['dataset_merge'] = dataset\n",
    "            adatas[dataset].obs['dataset_merge'] = dataset\n",
    "            gene_intersect[dataset] = list(adatas[dataset].var.index)\n",
    "        adata = list(adatas.values())[0].concatenate(list(adatas.values())[1:],join='inner')\n",
    "        return adatas, adata\n",
    "    elif data_merge == False:\n",
    "        if 'https' in adatas_dict[data_key_use]:\n",
    "            print('Loading anndata from web source')\n",
    "            adata = sc.read('./temp_adata.h5ad',backup_url=adatas_dict[data_key_use])\n",
    "        else: \n",
    "            adata = sc.read(adatas_dict[data_key_use])\n",
    "    if QC_normalise == True:\n",
    "        print('option to apply standardisation to data detected, performing basic QC filtering')\n",
    "        sc.pp.filter_cells(adata, min_genes=200)\n",
    "        sc.pp.filter_genes(adata, min_cells=3)\n",
    "        sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4)\n",
    "        sc.pp.log1p(adata)\n",
    "        \n",
    "    return adata\n",
    "\n",
    "# resource usage logger\n",
    "class DisplayCPU(threading.Thread):\n",
    "    def run(self):\n",
    "        tracemalloc.start()\n",
    "        starting, starting_peak = tracemalloc.get_traced_memory()\n",
    "        self.running = True\n",
    "        self.starting = starting\n",
    "        currentProcess = psutil.Process()\n",
    "        cpu_pct = []\n",
    "        peak_cpu = 0\n",
    "        while self.running:\n",
    "            peak_cpu = 0\n",
    "#           time.sleep(3)\n",
    "#             print('CPU % usage = '+''+ str(currentProcess.cpu_percent(interval=1)))\n",
    "#             cpu_pct.append(str(currentProcess.cpu_percent(interval=1)))\n",
    "            cpu = currentProcess.cpu_percent()\n",
    "        # track the peak utilization of the process\n",
    "            if cpu > peak_cpu:\n",
    "                peak_cpu = cpu\n",
    "                peak_cpu_per_core = peak_cpu/psutil.cpu_count()\n",
    "        self.peak_cpu = peak_cpu\n",
    "        self.peak_cpu_per_core = peak_cpu_per_core\n",
    "        \n",
    "    def stop(self):\n",
    "#        cpu_pct = DisplayCPU.run(self)\n",
    "        self.running = False\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        tracemalloc.stop()\n",
    "        return current, peak\n",
    "    \n",
    "# projection module\n",
    "def reference_projection(adata, model, dyn_std,partial_scale):\n",
    "    \n",
    "    class adata_temp:\n",
    "        pass\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    print('Determining model flavour')\n",
    "    try:\n",
    "        model_lr =  model['Model']\n",
    "        print('Consuming celltypist model')\n",
    "    except:# hasattr(model, 'coef_'):\n",
    "        print('Consuming non-celltypist model')\n",
    "        model_lr =  model\n",
    "    print(model_lr)\n",
    "    \n",
    "#     model_lr =  model['Model']\n",
    "\n",
    "    if train_x_partition == 'X':\n",
    "        print('Matching reference genes in the model')\n",
    "        k_x = np.isin(list(adata.var.index), list(model_lr.features))\n",
    "        if k_x.sum() == 0:\n",
    "            raise ValueError(f\"ðŸ›‘ No features overlap with the model. Please provide gene symbols\")\n",
    "        print(f\"ðŸ§¬ {k_x.sum()} features used for prediction\")\n",
    "        #slicing adata\n",
    "        k_x_idx = np.where(k_x)[0]\n",
    "        # adata_temp = adata[:,k_x_idx]\n",
    "        adata_temp.var = adata[:,k_x_idx].var\n",
    "        adata_temp.X = adata[:,k_x_idx].X\n",
    "        adata_temp.obs = adata[:,k_x_idx].obs\n",
    "        lr_idx = pd.DataFrame(model_lr.features, columns=['features']).reset_index().set_index('features').loc[list(adata_temp.var.index)].values\n",
    "        # adata_arr = adata_temp.X[:,list(lr_idexes['index'])]\n",
    "\n",
    "        # slice and reorder model\n",
    "        ni, fs, cf = model_lr.n_features_in_, model_lr.features, model_lr.coef_\n",
    "        model_lr.n_features_in_ = lr_idx.size\n",
    "        model_lr.features = np.array(model_lr.features)[lr_idx]\n",
    "        model_lr.coef_ = np.squeeze(model_lr.coef_[:,lr_idx]) #model_lr.coef_[:, lr_idx]\n",
    "        \n",
    "        if partial_scale == True:\n",
    "            print('scaling input data, default option is to use incremental learning and fit in mini bulks!')\n",
    "            # Partial scaling alg\n",
    "            scaler = StandardScaler(with_mean=False)\n",
    "            n = adata_temp.X.shape[0]  # number of rows\n",
    "            # set dyn scale packet size\n",
    "            x_len = len(adata_temp.var)\n",
    "            y_len = len(adata.obs)\n",
    "            if y_len < 100000:\n",
    "                dyn_pack = int(x_len/10)\n",
    "                pack_size = dyn_pack\n",
    "            else:\n",
    "                # 10 pack for every 100,000\n",
    "                dyn_pack = int((y_len/100000)*10)\n",
    "                pack_size = int(x_len/dyn_pack)\n",
    "\n",
    "            batch_size =  1000#pack_size#500  # number of rows in each call to partial_fit\n",
    "            index = 0  # helper-var\n",
    "            while index < n:\n",
    "                partial_size = min(batch_size, n - index)  # needed because last loop is possibly incomplete\n",
    "                partial_x = adata_temp.X[index:index+partial_size]\n",
    "                scaler.partial_fit(partial_x)\n",
    "                index += partial_size\n",
    "            adata_temp.X = scaler.transform(adata_temp.X)\n",
    "    \n",
    "    # model projections\n",
    "    print('Starting reference projection!')\n",
    "    if train_x_partition == 'X':\n",
    "        train_x = adata_temp.X\n",
    "        pred_out = pd.DataFrame(model_lr.predict(train_x),columns = ['predicted'],index = list(adata.obs.index))\n",
    "        proba =  pd.DataFrame(model_lr.predict_proba(train_x),columns = model_lr.classes_,index = list(adata.obs.index))\n",
    "        pred_out = pred_out.join(proba)\n",
    "        \n",
    "    elif train_x_partition in list(adata.obsm.keys()): \n",
    "        print('{low_dim: this partition modality is still under development!}')\n",
    "        train_x = adata.obsm[train_x_partition]\n",
    "        pred_out = pd.DataFrame(model_lr.predict(train_x),columns = ['predicted'],index = list(adata.obs.index))\n",
    "        proba =  pd.DataFrame(model_lr.predict_proba(train_x),columns = model_lr.classes_,index = list(adata.obs.index))\n",
    "        pred_out = pred_out.join(proba)\n",
    "    \n",
    "    else:\n",
    "        print('{this partition modality is still under development!}')\n",
    "    ## insert modules for low dim below\n",
    "\n",
    "    # Simple dynamic confidence calling\n",
    "    pred_out['confident_calls'] = pred_out['predicted']\n",
    "    pred_out.loc[pred_out.max(axis=1)<(pred_out.mean(axis=1) + (1*pred_out.std(axis=1))),'confident_calls'] = pred_out.loc[pred_out.max(axis=1)<(pred_out.mean(axis=1) + (1*pred_out.std(axis=1))),'confident_calls'].astype(str) + '_uncertain'\n",
    "    # means_ = self.model.scaler.mean_[lr_idx] if self.model.scaler.with_mean else 0\n",
    "    return(pred_out,train_x,model_lr,adata_temp)\n",
    "\n",
    "def freq_redist_68CI(adata,clusters_reassign):\n",
    "    if freq_redist != False:\n",
    "        print('Frequency redistribution commencing')\n",
    "        cluster_prediction = \"consensus_clus_prediction\"\n",
    "        lr_predicted_col = 'predicted'\n",
    "        pred_out[clusters_reassign] = adata.obs[clusters_reassign].astype(str)\n",
    "        reassign_classes = list(pred_out[clusters_reassign].unique())\n",
    "        lm = 1 # lambda value\n",
    "        pred_out[cluster_prediction] = pred_out[clusters_reassign]\n",
    "        for z in pred_out[clusters_reassign][pred_out[clusters_reassign].isin(reassign_classes)].unique():\n",
    "            df = pred_out\n",
    "            df = df[(df[clusters_reassign].isin([z]))]\n",
    "            df_count = pd.DataFrame(df[lr_predicted_col].value_counts())\n",
    "            # Look for classificationds > 68CI\n",
    "            if len(df_count) > 1:\n",
    "                df_count_temp = df_count[df_count[lr_predicted_col]>int(int(df_count.mean()) + (df_count.std()*lm))]\n",
    "                if len(df_count_temp >= 1):\n",
    "                    df_count = df_count_temp\n",
    "            #print(df_count)     \n",
    "            freq_arranged = df_count.index\n",
    "            cat = freq_arranged[0]\n",
    "        #Make the cluster assignment first\n",
    "            pred_out[cluster_prediction] = pred_out[cluster_prediction].astype(str)\n",
    "            pred_out.loc[pred_out[clusters_reassign] == z, [cluster_prediction]] = cat\n",
    "        # Create assignments for any classification >68CI\n",
    "            for cats in freq_arranged:\n",
    "                #print(cats)\n",
    "                cats_assignment = cats#.replace(data1,'') + '_clus_prediction'\n",
    "                pred_out.loc[(pred_out[clusters_reassign] == z) & (pred_out[lr_predicted_col] == cats),[cluster_prediction]] = cats_assignment\n",
    "        min_counts = pd.DataFrame((pred_out[cluster_prediction].value_counts()))\n",
    "        reassign = list(min_counts.index[min_counts[cluster_prediction]<=2])\n",
    "        pred_out[cluster_prediction] = pred_out[cluster_prediction].str.replace(str(''.join(reassign)),str(''.join(pred_out.loc[pred_out[clusters_reassign].isin(list(pred_out.loc[(pred_out[cluster_prediction].isin(reassign)),clusters_reassign])),lr_predicted_col].value_counts().head(1).index.values)))\n",
    "        return pred_out\n",
    "\n",
    "### Feature importance notes\n",
    "#- If we increase the x feature one unit, then the prediction will change e to the power of its weight. We can apply this rule to the all weights to find the feature importance.\n",
    "#- We will calculate the Euler number to the power of its coefficient to find the importance.\n",
    "#- To sum up an increase of x feature by one unit increases the odds of being versicolor class by a factor of x[importance] when all other features remain the same.\n",
    "\n",
    "#- For low-dim, we look at the distribution of e^coef per class, we extract the \n",
    "\n",
    "\n",
    "# class coef_extract:\n",
    "#     def __init__(self, model,features, pos):\n",
    "# #         self.w = list(itertools.chain(*(model.coef_[pos]).tolist())) #model.coef_[pos]\n",
    "#         self.w = model.coef_[class_pred_pos]\n",
    "#         self.features = features \n",
    "\n",
    "def long_format_features(top_loadings):\n",
    "    p = top_loadings.loc[:, top_loadings.columns.str.endswith(\"_e^coef\")]\n",
    "    p = pd.melt(p)\n",
    "    n = top_loadings.loc[:, top_loadings.columns.str.endswith(\"_feature\")]\n",
    "    n = pd.melt(n)\n",
    "    l = top_loadings.loc[:, top_loadings.columns.str.endswith(\"_coef\")]\n",
    "    l = pd.melt(l)\n",
    "    n = n.replace(regex=r'_feature', value='')\n",
    "    n = n.rename(columns={\"variable\": \"class\", \"value\": \"feature\"})\n",
    "    p = (p.drop([\"variable\"],axis = 1)).rename(columns={ \"value\": \"e^coef\"})\n",
    "    l = (l.drop([\"variable\"],axis = 1)).rename(columns={ \"value\": \"coef\"})\n",
    "    concat = pd.concat([n,p,l],axis=1)\n",
    "    return concat\n",
    "\n",
    "def model_feature_sf(long_format_feature_importance, coef_use):\n",
    "        long_format_feature_importance[str(coef_use) + '_pval'] = 'NaN'\n",
    "        for class_lw in long_format_feature_importance['class'].unique():\n",
    "            df_loadings = long_format_feature_importance[long_format_feature_importance['class'].isin([class_lw])]\n",
    "            comps = coef_use #'e^coef'\n",
    "            U = np.mean(df_loadings[comps])\n",
    "            std = np.std(df_loadings[comps])\n",
    "            med =  np.median(df_loadings[comps])\n",
    "            mad = np.median(np.absolute(df_loadings[comps] - np.median(df_loadings[comps])))\n",
    "            # Survival function scaled by 1.4826 of MAD (approx norm)\n",
    "            pvals = scipy.stats.norm.sf(df_loadings[comps], loc=med, scale=1.4826*mad) # 95% CI of MAD <10,000 samples\n",
    "            #pvals = scipy.stats.norm.sf(df_loadings[comps], loc=U, scale=1*std)\n",
    "            df_loadings[str(comps) +'_pval'] = pvals\n",
    "            long_format_feature_importance.loc[long_format_feature_importance.index.isin(df_loadings.index)] = df_loadings\n",
    "        long_format_feature_importance['is_significant_sf'] = False\n",
    "        long_format_feature_importance.loc[long_format_feature_importance[coef_use+ '_pval']<0.05,'is_significant_sf'] = True\n",
    "        return long_format_feature_importance\n",
    "# Apply SF to e^coeff mat data\n",
    "#         pval_mat = pd.DataFrame(columns = mat.columns)\n",
    "#         for class_lw in mat.index:\n",
    "#             df_loadings = mat.loc[class_lw]\n",
    "#             U = np.mean(df_loadings)\n",
    "#             std = np.std(df_loadings)\n",
    "#             med =  np.median(df_loadings)\n",
    "#             mad = np.median(np.absolute(df_loadings - np.median(df_loadings)))\n",
    "#             pvals = scipy.stats.norm.sf(df_loadings, loc=med, scale=1.96*U)\n",
    "\n",
    "class estimate_important_features: # This calculates feature effect sizes of the model\n",
    "    def __init__(self, model, top_n):\n",
    "        print('Estimating feature importance')\n",
    "        classes =  list(model.classes_)\n",
    "         # get feature names\n",
    "        try:\n",
    "            model_features = list(itertools.chain(*list(model.features)))\n",
    "        except:\n",
    "            warnings.warn('no features recorded in data, naming features by position')\n",
    "            print('if low-dim lr was submitted, run linear decoding function to obtain true feature set')\n",
    "            model_features = list(range(0,model.coef_.shape[1]))\n",
    "            model.features = model_features\n",
    "        print('Calculating the Euler number to the power of coefficients')\n",
    "        impt_ = pow(math.e,model.coef_)\n",
    "        try:\n",
    "            self.euler_pow_mat = pd.DataFrame(impt_,columns = list(itertools.chain(*list(model.features))),index = list(model.classes_))\n",
    "        except:\n",
    "            self.euler_pow_mat = pd.DataFrame(impt_,columns = list(model.features),index = list(model.classes_))\n",
    "        self.top_n_features = pd.DataFrame(index = list(range(0,top_n)))\n",
    "        # estimate per class feature importance\n",
    "        \n",
    "        print('Estimating feature importance for each class')\n",
    "        mat = self.euler_pow_mat\n",
    "        for class_pred_pos in list(range(0,len(mat.T.columns))):\n",
    "            class_pred = list(mat.T.columns)[class_pred_pos]\n",
    "            #     print(class_pred)\n",
    "            temp_mat =  pd.DataFrame(mat.T[class_pred])\n",
    "            temp_mat['coef'] = model.coef_[class_pred_pos]\n",
    "            temp_mat = temp_mat.sort_values(by = [class_pred], ascending=False)\n",
    "            temp_mat = temp_mat.reset_index()\n",
    "            temp_mat.columns = ['feature','e^coef','coef']\n",
    "            temp_mat = temp_mat[['feature','e^coef','coef']]\n",
    "            temp_mat.columns =str(class_pred)+ \"_\" + temp_mat.columns\n",
    "            self.top_n_features = pd.concat([self.top_n_features,temp_mat.head(top_n)], join=\"inner\",ignore_index = False, axis=1)\n",
    "            self.to_n_features_long = model_feature_sf(long_format_features(self.top_n_features),'e^coef')\n",
    "            \n",
    "    \n",
    "    # plot class-wise features\n",
    "def model_class_feature_plots(top_loadings, classes, comps):\n",
    "    import matplotlib.pyplot as plt\n",
    "    for class_temp in classes:\n",
    "        class_lw = class_temp\n",
    "        long_format = top_loadings\n",
    "        df_loadings = long_format[long_format['class'].isin([class_lw])]\n",
    "        plt.hist(df_loadings[comps])\n",
    "        for i in ((df_loadings[comps][df_loadings[str(comps) +'_pval']<0.05]).unique()):\n",
    "            plt.axvline(x=i,color='red')\n",
    "        med = np.median(df_loadings[comps])\n",
    "        plt.axvline(x=med,color='blue')\n",
    "        plt.xlabel('feature_importance', fontsize=12)\n",
    "        plt.title(class_lw)\n",
    "        #plt.axvline(x=med,color='pink')\n",
    "        df_loadings[comps][df_loadings[str(comps) +'_pval']<0.05]\n",
    "        print(len(df_loadings[comps][df_loadings[str(comps) +'_pval']<0.05]))\n",
    "        #Plot feature ranking\n",
    "        plot_loading = pd.DataFrame(pd.DataFrame(df_loadings[comps][df_loadings[str(comps) +'_pval']<0.05]).iloc[:,0].sort_values(ascending=False))\n",
    "        table = plt.table(cellText=plot_loading.values,colWidths = [1]*len(plot_loading.columns),\n",
    "        rowLabels= list(df_loadings['feature'][df_loadings.index.isin(plot_loading.index)].reindex(plot_loading.index)), #plot_loading.index,\n",
    "        colLabels=plot_loading.columns,\n",
    "        cellLoc = 'center', rowLoc = 'center',\n",
    "        loc='right', bbox=[1.4, -0.05, 0.5,1])\n",
    "        table.scale(1, 2)\n",
    "        table.set_fontsize(10)\n",
    "        \n",
    "def report_f1(model,train_x, train_label):\n",
    "    ## Report accuracy score\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "    from sklearn import metrics\n",
    "    import seaborn as sn\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # cv = RepeatedStratifiedKFold(n_splits=2, n_repeats=2, random_state=1)\n",
    "    # # evaluate the model and collect the scores\n",
    "    # n_scores = cross_val_score(lr, train_x, train_label, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    # # report the model performance\n",
    "    # print('Mean Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))\n",
    "\n",
    "    # Report Precision score\n",
    "    metric = pd.DataFrame((metrics.classification_report(train_label, model.predict(train_x), digits=2,output_dict=True))).T\n",
    "    cm = confusion_matrix(train_label, model.predict(train_x))\n",
    "    #cm = confusion_matrix(train_label, model.predict_proba(train_x))\n",
    "    df_cm = pd.DataFrame(cm, index = model.classes_,columns = model.classes_)\n",
    "    df_cm = (df_cm / df_cm.sum(axis=0))*100\n",
    "    plt.figure(figsize = (20,15))\n",
    "    sn.set(font_scale=1) # for label size\n",
    "    pal = sns.diverging_palette(240, 10, n=10)\n",
    "    #plt.suptitle(('Mean Accuracy 5 fold: %.3f std: %.3f' % (np.mean(n_scores),  np.std(n_scores))), y=1.05, fontsize=18)\n",
    "    #Plot precision recall and recall\n",
    "    table = plt.table(cellText=metric.values,colWidths = [1]*len(metric.columns),\n",
    "    rowLabels=metric.index,\n",
    "    colLabels=metric.columns,\n",
    "    cellLoc = 'center', rowLoc = 'center',\n",
    "    loc='bottom', bbox=[0.25, -0.6, 0.5, 0.3])\n",
    "    table.scale(1, 2)\n",
    "    table.set_fontsize(10)\n",
    "\n",
    "    sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16},cmap=pal) # font size\n",
    "    print(metrics.classification_report(train_label, model.predict(train_x), digits=2))\n",
    "\n",
    "def subset_top_hvgs(adata_lognorm, n_top_genes):\n",
    "    dispersion_norm = adata_lognorm.var['dispersions_norm'].values.astype('float32')\n",
    "\n",
    "    dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]\n",
    "    dispersion_norm[\n",
    "                ::-1\n",
    "            ].sort()  # interestingly, np.argpartition is slightly slower\n",
    "\n",
    "    disp_cut_off = dispersion_norm[n_top_genes - 1]\n",
    "    gene_subset = adata_lognorm.var['dispersions_norm'].values >= disp_cut_off\n",
    "    return(adata_lognorm[:,gene_subset])\n",
    "\n",
    "def prep_scVI(adata, \n",
    "              n_hvgs = 5000,\n",
    "              remove_cc_genes = True,\n",
    "              remove_tcr_bcr_genes = False\n",
    "             ):\n",
    "    ## Remove cell cycle genes\n",
    "    if remove_cc_genes:\n",
    "        adata = panfetal_utils.remove_geneset(adata,genes.cc_genes)\n",
    "\n",
    "    ## Remove TCR/BCR genes\n",
    "    if remove_tcr_bcr_genes:\n",
    "        adata = panfetal_utils.remove_geneset(adata, genes.IG_genes)\n",
    "        adata = panfetal_utils.remove_geneset(adata, genes.TCR_genes)\n",
    "        \n",
    "    ## HVG selection\n",
    "    adata = subset_top_hvgs(adata, n_top_genes=n_hvgs)\n",
    "    return(adata)\n",
    "\n",
    "# Modified LR train module, does not work with low-dim by default anymore, please use low-dim adapter\n",
    "def LR_train(adata, train_x, train_label, penalty='elasticnet', sparcity=0.2,max_iter=200,l1_ratio =0.2,tune_hyper_params =False,n_splits=5, n_repeats=3,l1_grid = [0.1,0.2,0.5,0.8], c_grid = [0.1,0.2,0.4,0.6],sketch_obsm =None):\n",
    "    if tune_hyper_params == True:\n",
    "        train_labels = train_label\n",
    "        results,adata_tuned = tune_lr_model(adata, train_x_partition = train_x, random_state = 42,  train_labels = train_labels, n_splits=n_splits, n_repeats=n_repeats,l1_grid = l1_grid, c_grid = c_grid,sketch_obsm = sketch_obsm)\n",
    "        print('hyper_params tuned')\n",
    "        sparcity = results.best_params_['C']\n",
    "        l1_ratio = results.best_params_['l1_ratio']\n",
    "        \n",
    "    if not sketch_obsm == None:\n",
    "        #sketch data\n",
    "        try:\n",
    "            adata = sketch_data(adata, train_x_partition = train_x, random_state = 42,  train_labels = train_label,sketch_obsm = sketch_obsm)\n",
    "        except:\n",
    "            print()\n",
    "\n",
    "    lr = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  max_iter, n_jobs=thread_num)\n",
    "    if (penalty == \"l1\"):\n",
    "        lr = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  max_iter, dual = True, solver = 'liblinear',multi_class = 'ovr', n_jobs=thread_num ) # one-vs-rest\n",
    "    if (penalty == \"elasticnet\"):\n",
    "        lr = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  max_iter, dual=False,solver = 'saga',l1_ratio=l1_ratio,multi_class = 'ovr', n_jobs=thread_num)\n",
    "    if train_x == 'X':\n",
    "        subset_train = adata.obs.index\n",
    "        # Define training parameters\n",
    "        train_label = adata.obs[train_label].values\n",
    "#        predict_label = train_label[subset_predict]\n",
    "#        train_label = train_label[subset_train]\n",
    "        train_x = adata.X#[adata.obs.index.isin(list(adata.obs[subset_train].index))]\n",
    "#        predict_x = adata.X[adata.obs.index.isin(list(adata.obs[subset_predict].index))]\n",
    "    elif train_x in adata.obsm.keys():\n",
    "        # Define training parameters\n",
    "        train_label = adata.obs[train_label].values\n",
    "#        predict_label = train_label[subset_predict]\n",
    "#         train_label = train_label[subset_train]\n",
    "        train_x = adata.obsm[train_x]\n",
    "#        predict_x = train_x\n",
    "#        train_x = train_x[subset_train, :]\n",
    "        # Define prediction parameters\n",
    "#        predict_x = predict_x[subset_predict]\n",
    "#        predict_x = pd.DataFrame(predict_x)\n",
    "#        predict_x.index = adata.obs[subset_predict].index\n",
    "    # Train predictive model using user defined partition labels (train_x ,train_label, predict_x)\n",
    "    model = lr.fit(train_x, train_label)\n",
    "    model.features = np.array(adata.var.index)\n",
    "    return model\n",
    "\n",
    "def tune_lr_model(adata, train_x_partition = 'X', random_state = 42,  train_labels = None, n_splits=5, n_repeats=3,l1_grid = [0.05,0.2,0.5,0.8], c_grid = [0.05,0.2,0.4,0.6],sketch_obsm = None):\n",
    "    import bless as bless\n",
    "    from sklearn.gaussian_process.kernels import RBF\n",
    "    from numpy import arange\n",
    "    from sklearn.model_selection import RepeatedKFold\n",
    "    from sklearn.datasets import make_classification\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    # If latent rep is provided, randomly sample data in spatially aware manner for initialisation\n",
    "    r = np.random.RandomState(random_state)\n",
    "    if train_x_partition in adata.obsm.keys():\n",
    "        tune_train_x = adata.obsm[train_x_partition][:]\n",
    "        lvg = bless.bless(tune_train_x, RBF(length_scale=20), lam_final = 2, qbar = 2, random_state = r, H = 10, force_cpu=True)\n",
    "        adata_tuning = adata[lvg.idx]\n",
    "        print('Sketched data is {} long'.format(len(adata_tuning.obs)))\n",
    "\n",
    "    elif sketch_obsm in adata.obsm.keys():\n",
    "        sketch_obsm_id = adata.obsm[sketch_obsm][:]\n",
    "        lvg = bless.bless(sketch_obsm_id, RBF(length_scale=20), lam_final = 2, qbar = 2, random_state = r, H = 10, force_cpu=True)\n",
    "        adata_tuning = adata[lvg.idx]\n",
    "        print('Sketched data is {} long'.format(len(adata_tuning.obs)))\n",
    "        tune_train_x = adata_tuning.X\n",
    "    #     try:\n",
    "    #         import cupy\n",
    "    #         lvg_2 = bless(adata.obsm[train_x_partition], RBF(length_scale=10), 10, 10, r, 10, force_cpu=False)\n",
    "    #     except ImportError:\n",
    "    #         print(\"cupy not found, defaulting to numpy\")\n",
    "    else:\n",
    "        print('no latent representation provided, random sampling instead')\n",
    "        prop = 0.1\n",
    "        random_vertices = []\n",
    "        n_ixs = int(len(adata.obs) * prop)\n",
    "        random_vertices = random.sample(list(range(len(adata.obs))), k=n_ixs)\n",
    "        adata_tuning = adata[random_vertices]\n",
    "        tune_train_x = adata_tuning.X\n",
    "        \n",
    "    if not train_labels == None:\n",
    "        tune_train_label = adata_tuning.obs[train_labels]\n",
    "    elif train_labels == None:\n",
    "        try:\n",
    "            print('no training labels provided, defaulting to unsuperived leiden clustering, updates will change this to voronoi greedy sampling')\n",
    "            sc.tl.leiden(adata_tuning)\n",
    "        except:\n",
    "            print('no training labels provided, no neighbors, defaulting to unsuperived leiden clustering, updates will change this to voronoi greedy sampling')\n",
    "            sc.pp.neighbors(adata_hm, n_neighbors=15, n_pcs=50)\n",
    "            sc.tl.leiden(adata_tuning)\n",
    "        tune_train_label = adata_tuning.obs['leiden']\n",
    "    ## tune regularization for multinomial logistic regression\n",
    "    print('starting tuning loops')\n",
    "    X = tune_train_x\n",
    "    y = tune_train_label\n",
    "    grid = dict()\n",
    "    # define model\n",
    "    cv = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=random_state)\n",
    "    #model = LogisticRegression(penalty = penalty, max_iter =  200, dual=False,solver = 'saga', multi_class = 'multinomial',)\n",
    "    model = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  100, n_jobs=4)\n",
    "    if (penalty == \"l1\"):\n",
    "        model = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  100, dual = True, solver = 'liblinear',multi_class = 'ovr', n_jobs=4 ) # one-vs-rest\n",
    "    if (penalty == \"elasticnet\"):\n",
    "        model = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  100, dual=False,solver = 'saga',l1_ratio=l1_ratio,multi_class = 'ovr', n_jobs=4) # use multinomial class if probabilities are descrete\n",
    "        grid['l1_ratio'] = l1_grid\n",
    "    grid['C'] = c_grid\n",
    "    # define search\n",
    "    search = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "    # perform the search\n",
    "    results = search.fit(X, y)\n",
    "    # summarize\n",
    "    print('MAE: %.3f' % results.best_score_)\n",
    "    print('Config: %s' % results.best_params_)\n",
    "    return results , adata_tuning\n",
    "\n",
    "def prep_training_data(adata_temp,feat_use,batch_key, model_key, batch_correction=False, var_length = 7500,penalty='elasticnet',sparcity=0.2,max_iter = 200,l1_ratio = 0.1,partial_scale=True,train_x_partition ='X',theta = 3,tune_hyper_params=False,sketch_obsm = None ):\n",
    "    model_name = model_key + '_lr_model'\n",
    "    print('performing highly variable gene selection')\n",
    "    sc.pp.highly_variable_genes(adata_temp, batch_key = batch_key, subset=False)\n",
    "    #temp inclusion\n",
    "    sc.pp.pca(adata_temp, n_comps=100, use_highly_variable=True, svd_solver='arpack')\n",
    "    sc.pl.pca_variance_ratio(adata_temp, log=True,n_pcs=100)\n",
    "        \n",
    "    adata_temp = subset_top_hvgs(adata_temp,var_length)\n",
    "    #scale the input data\n",
    "    if partial_scale == True:\n",
    "        print('scaling input data, default option is to use incremental learning and fit in mini bulks!')\n",
    "        # Partial scaling alg\n",
    "        #adata_temp.X = (adata_temp.X)\n",
    "        scaler = StandardScaler(with_mean=False)\n",
    "        n = adata_temp.X.shape[0]  # number of rows\n",
    "        # set dyn scale packet size\n",
    "        x_len = len(adata_temp.var)\n",
    "        y_len = len(adata_temp.obs)\n",
    "        if y_len < 100000:\n",
    "            dyn_pack = int(x_len/10)\n",
    "            pack_size = dyn_pack\n",
    "        else:\n",
    "            # 10 pack for every 100,000\n",
    "            dyn_pack = int((y_len/100000)*10)\n",
    "            pack_size = int(x_len/dyn_pack)\n",
    "        batch_size =  1000#pack_size#500  # number of rows in each call to partial_fit\n",
    "        index = 0  # helper-var\n",
    "        while index < n:\n",
    "            partial_size = min(batch_size, n - index)  # needed because last loop is possibly incomplete\n",
    "            partial_x = adata_temp.X[index:index+partial_size]\n",
    "            scaler.partial_fit(partial_x)\n",
    "            index += partial_size\n",
    "        adata_temp.X = scaler.transform(adata_temp.X)\n",
    "#     else:\n",
    "#         sc.pp.scale(adata_temp, zero_center=True, max_value=None, copy=False, layer=None, obsm=None)\n",
    "\n",
    "    if (train_x_partition != 'X') & (train_x_partition in adata_temp.obsm.keys()):\n",
    "        print('train partition is not in OBSM, defaulting to PCA')\n",
    "        # Now compute PCA\n",
    "        sc.pp.pca(adata_temp, n_comps=100, use_highly_variable=True, svd_solver='arpack')\n",
    "        sc.pl.pca_variance_ratio(adata_temp, log=True,n_pcs=100)\n",
    "        \n",
    "        # Batch correction options\n",
    "        # The script will test later which Harmony values we should use \n",
    "        if(batch_correction == \"Harmony\"):\n",
    "            print(\"Commencing harmony\")\n",
    "            if len(batch_key) == 1:\n",
    "                adata_temp.obs['lr_batch'] = adata_temp.obs[batch_key]\n",
    "                batch_var = \"lr_batch\"\n",
    "            else:\n",
    "                batch_var = batch_key\n",
    "            # Create hm subset\n",
    "            adata_hm = adata_temp[:]\n",
    "            # Set harmony variables\n",
    "            data_mat = np.array(adata_hm.obsm[\"X_pca\"])\n",
    "            meta_data = adata_hm.obs\n",
    "            vars_use = [batch_var]\n",
    "            # Run Harmony\n",
    "            ho = hm.run_harmony(data_mat, meta_data, vars_use,theta=theta)\n",
    "            res = (pd.DataFrame(ho.Z_corr)).T\n",
    "            res.columns = ['X{}'.format(i + 1) for i in range(res.shape[1])]\n",
    "            # Insert coordinates back into object\n",
    "            adata_hm.obsm[\"X_pca_back\"]= adata_hm.obsm[\"X_pca\"][:]\n",
    "            adata_hm.obsm[\"X_pca\"] = np.array(res)\n",
    "            # Run neighbours\n",
    "            #sc.pp.neighbors(adata_hm, n_neighbors=15, n_pcs=50)\n",
    "            adata_temp = adata_hm[:]\n",
    "            del adata_hm\n",
    "        elif(batch_correction == \"BBKNN\"):\n",
    "            print(\"Commencing BBKNN\")\n",
    "            sc.external.pp.bbknn(adata_temp, batch_key=batch_var, approx=True, metric='angular', copy=False, n_pcs=50, trim=None, n_trees=10, use_faiss=True, set_op_mix_ratio=1.0, local_connectivity=15) \n",
    "        print(\"adata1 and adata2 are now combined and preprocessed in 'adata' obj - success!\")\n",
    "\n",
    "\n",
    "    # train model\n",
    "#    train_x = adata_temp.X\n",
    "    #train_label = adata_temp.obs[feat_use]\n",
    "    print('proceeding to train model')\n",
    "    model = LR_train(adata_temp, train_x = train_x_partition, train_label=feat_use, penalty=penalty, sparcity=sparcity,max_iter=max_iter,l1_ratio = l1_ratio,tune_hyper_params = tune_hyper_params,sketch_obsm = sketch_obsm)\n",
    "    model.features = list(adata_temp.var.index)\n",
    "    return model\n",
    "\n",
    "def regression_results(y_true, y_pred):\n",
    "    # Regression metrics\n",
    "    explained_variance=metrics.explained_variance_score(y_true, y_pred)\n",
    "    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n",
    "    mse=metrics.mean_squared_error(y_true, y_pred) \n",
    "    mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)\n",
    "    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n",
    "    r2=metrics.r2_score(y_true, y_pred)\n",
    "    print('explained_variance: ', round(explained_variance,4))    \n",
    "    print('mean_squared_log_error: ', round(mean_squared_log_error,4))\n",
    "    print('r2: ', round(r2,4))\n",
    "    print('MAE: ', round(mean_absolute_error,4))\n",
    "    print('MSE: ', round(mse,4))\n",
    "    print('RMSE: ', round(np.sqrt(mse),4))\n",
    "    \n",
    "def sketch_data(adata, train_x_partition = 'X', sketch_obsm = None, random_state = 42,  train_labels = None):\n",
    "    import bless as bless\n",
    "    from sklearn.gaussian_process.kernels import RBF\n",
    "    from numpy import arange\n",
    "    from sklearn.model_selection import RepeatedKFold\n",
    "    from sklearn.datasets import make_classification\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    # If latent rep is provided, randomly sample data in spatially aware manner for initialisation\n",
    "    r = np.random.RandomState(random_state)\n",
    "    if train_x_partition in adata.obsm.keys():\n",
    "        tune_train_x = adata.obsm[train_x_partition][:]\n",
    "    elif sketch_obsm in adata.obsm.keys():\n",
    "        tune_train_x = adata.obsm[sketch_obsm][:]\n",
    "    else:\n",
    "        print('No obsm partition detected! defaulting to PCA')\n",
    "        if not 'X_pca' in adata.obsm.keys():\n",
    "            print('performing highly variable gene selection')\n",
    "            sc.pp.highly_variable_genes(adata_temp, batch_key = batch_key, subset=False)\n",
    "            sc.pp.pca(adata_temp, n_comps=100, use_highly_variable=True, svd_solver='arpack')\n",
    "            sc.pl.pca_variance_ratio(adata_temp, log=True,n_pcs=100)\n",
    "        tune_train_x = adata.obsm['X_pca'][:]\n",
    "    lvg = bless.bless(tune_train_x, RBF(length_scale=20), lam_final = 2, qbar = 2, random_state = r, H = 10, force_cpu=True)\n",
    "    adata_tuning = adata[lvg.idx]\n",
    "    print('sketched partition is {}, original is {}'.format(len(lvg.idx)),len(adata.obs))\n",
    "    return adata_tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-liberia",
   "metadata": {},
   "source": [
    "# Read in query data for projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-conditions",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-kingston",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model == True:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    adata =  load_adatas(adatas_dict, data_merge, adata_key,QC_normalise)\n",
    "    print('adata_loaded')\n",
    "    import time\n",
    "    t0 = time.time()\n",
    "    display_cpu = DisplayCPU()\n",
    "    display_cpu.start()\n",
    "    try:\n",
    "        model_trained = prep_training_data(feat_use = feat_use,\n",
    "        adata_temp = adata,\n",
    "        train_x_partition = train_x_partition,\n",
    "        model_key = model_key + '_lr_model',\n",
    "        batch_correction = False,\n",
    "        var_length = 7500,\n",
    "        batch_key = None, #batch_key,\n",
    "        penalty='elasticnet', # can be [\"l1\",\"l2\",\"elasticnet\"],\n",
    "        sparcity=sparcity, #If using LR without optimisation, this controls the sparsity in model\n",
    "        max_iter = 1000, #Increase if experiencing max iter issues\n",
    "        l1_ratio = l1_ratio, #If using elasticnet without optimisation, this controls the ratio between l1 and l2)\n",
    "        partial_scale = False, #partial_scale,\n",
    "        tune_hyper_params = True, # Current implementation is very expensive, intentionally made rigid for now\n",
    "        sketch_obsm = 'X_pca'\n",
    "        )\n",
    "        filename = model_key\n",
    "        pkl.dump(model_trained, open(filename, 'wb'))\n",
    "        models[model_key] = model_key\n",
    "    finally: #\n",
    "        current, peak = display_cpu.stop()\n",
    "        t1 = time.time()\n",
    "        time_s = t1-t0\n",
    "        print('training complete!')\n",
    "        time.sleep(3)\n",
    "        print('projection time was ' + str(time_s) + ' seconds')\n",
    "        print(f\"Current memory usage is {current / 10**6}MB; Peak was {peak / 10**6}MB\")\n",
    "        print(f\"starting memory usage is\" +'' + str(display_cpu.starting))\n",
    "        print('peak CPU % usage = '+''+ str(display_cpu.peak_cpu))\n",
    "        print('peak CPU % usage/core = '+''+ str(display_cpu.peak_cpu_per_core))\n",
    "    model_lr= model_trained\n",
    "    adata =  load_adatas(adatas_dict, data_merge, adata_key,QC_normalise)\n",
    "else:\n",
    "    adata =  load_adatas(adatas_dict, data_merge, adata_key,QC_normalise)\n",
    "    model = load_models(models,model_key)\n",
    "    model_lr =  model\n",
    "# run with usage logger\n",
    "import time\n",
    "t0 = time.time()\n",
    "display_cpu = DisplayCPU()\n",
    "display_cpu.start()\n",
    "try: #code here ##\n",
    "    pred_out,train_x,model_lr,adata_temp = reference_projection(adata, model_lr, dyn_std,partial_scale)\n",
    "    if freq_redist != False:\n",
    "        pred_out = freq_redist_68CI(adata,freq_redist)\n",
    "        pred_out['orig_labels'] = adata.obs[freq_redist]\n",
    "        adata.obs['consensus_clus_prediction'] = pred_out['consensus_clus_prediction']\n",
    "    adata.obs['predicted'] = pred_out['predicted']\n",
    "    adata_temp.obs = adata.obs\n",
    "    \n",
    "    # Estimate top model features for class descrimination\n",
    "    feature_importance = estimate_important_features(model_lr, 100)\n",
    "    mat = feature_importance.euler_pow_mat\n",
    "    top_loadings = feature_importance.to_n_features_long\n",
    "    \n",
    "    # Estimate dataset specific feature impact\n",
    "#     for classes in ['pDC precursor_ys_HL','AEC_ys_HL']:\n",
    "#         model_class_feature_plots(top_loadings, [str(classes)], 'e^coef')\n",
    "#         plt.show()\n",
    "finally: #\n",
    "    current, peak = display_cpu.stop()\n",
    "t1 = time.time()\n",
    "time_s = t1-t0\n",
    "print('projection complete!')\n",
    "time.sleep(3)\n",
    "print('projection time was ' + str(time_s) + ' seconds')\n",
    "print(f\"Current memory usage is {current / 10**6}MB; Peak was {peak / 10**6}MB\")\n",
    "print(f\"starting memory usage is\" +'' + str(display_cpu.starting))\n",
    "print('peak CPU % usage = '+''+ str(display_cpu.peak_cpu))\n",
    "print('peak CPU % usage/core = '+''+ str(display_cpu.peak_cpu_per_core))\n",
    "\n",
    "# regression summary\n",
    "idx_map = dict(zip(  list(adata.obs[feat_use].unique()),list(range(0,len(list(adata.obs[feat_use].unique()))))))\n",
    "regression_results(adata.obs[feat_use].map(idx_map), adata.obs['predicted'].map(idx_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-flesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mean_probs = pred_out.loc[:, pred_out.columns != 'predicted'].groupby('orig_labels').median()\n",
    "model_mean_probs = model_mean_probs*100\n",
    "model_mean_probs = model_mean_probs.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "crs_tbl = model_mean_probs.copy()\n",
    "# Sort df columns by rows\n",
    "index_order = list(crs_tbl.max(axis=1).sort_values(ascending=False).index)\n",
    "col_order = list(crs_tbl.max(axis=0).sort_values(ascending=False).index)\n",
    "crs_tbl = crs_tbl.loc[index_order]\n",
    "crs_tbl = crs_tbl[col_order]\n",
    "# Plot_df_heatmap(crs_tbl, cmap='coolwarm', rotation=90, vmin=20, vmax=70)\n",
    "pal = sns.diverging_palette(240, 10, n=10)\n",
    "plt.figure(figsize=(20,15))\n",
    "sns.set(font_scale=0.5)\n",
    "g = sns.heatmap(crs_tbl, cmap='viridis_r',  annot=False,vmin=0, vmax=max(np.max(crs_tbl)), linewidths=1, center=max(np.max(crs_tbl))/2, square=True, cbar_kws={\"shrink\": 0.5})\n",
    "    \n",
    "plt.ylabel(\"Original labels\")\n",
    "plt.xlabel(\"Training labels\")\n",
    "plt.savefig((model_key+'_X_lr_model_means_subclusters.pdf'),dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-stanley",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report F1 metrics\n",
    "k_x = np.isin(list(adata.var.index), list(model_lr.features))\n",
    "k_x_idx = np.where(k_x)[0]\n",
    "X = adata[:,k_x_idx].X\n",
    "report_f1(model_lr,X, list(adata.obs[feat_use]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-heavy",
   "metadata": {},
   "source": [
    "# Project onto Mye cells in adult mye atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable assignment\n",
    "train_model = False\n",
    "feat_use = 'IG_annot'\n",
    "adata_key = 'pan_organ_mye'#'fliv_wget_test' # key for dictionary entry containing local or web path to adata/s can be either url or local \n",
    "data_merge = False # read and merge multiple adata (useful, but keep false for now)\n",
    "model_key = model_key#'test_low_dim_ipsc_ys'# key for model of choice can be either url or local \n",
    "train_x_partition = 'X' # what partition was the data trained on? To keep simple, for now only accepts 'X'\n",
    "dyn_std = 1.96 # Dynamic cutoffs using std of the mean for each celltype probability, gives a column notifying user of uncertain labels 1 == 68Ci, 1.96 = 95CI\n",
    "freq_redist = 'leiden_res_3_IG'#'cell.labels'#'False#'cell.labels'#False # False or key of column in anndata object which contains labels/clusters // not currently implemented\n",
    "partial_scale = False # should data be scaled in batches?\n",
    "QC_normalise = False # should data be normalised?\n",
    "\n",
    "# training variables\n",
    "penalty='elasticnet' # can be [\"l1\",\"l2\",\"elasticnet\"]\n",
    "sparcity=0.5 # C penalty for degree of regularisation\n",
    "thread_num = -1\n",
    "l1_ratio = 0.5 # ratio between L1 and L2 regulrisatiuon depending on penatly method\n",
    "\n",
    "#\n",
    "\n",
    "# If low dim & not in keys\n",
    "batch_key = ['kit','donor'] #organ\n",
    "batch_correction = 'Harmony'#'Harmony' #or bbknn\n",
    "theta = 3 #harmony specifc\n",
    "\n",
    "\n",
    "create_sketch_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-punishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read('/nfs/team205/ig7/work_backups/backup_210306/projects/YS/rebuttal_figs_010922/ling_adult_macs/A1_V1_LING_ADULT_IG_annot.h5ad')\n",
    "sc.pp.highly_variable_genes(adata, batch_key = batch_key[0], subset=False)\n",
    "sc.pp.pca(adata, n_comps=100, use_highly_variable=True, svd_solver='arpack')\n",
    "sc.pl.pca_variance_ratio(adata, log=True,n_pcs=100)\n",
    "\n",
    "import harmonypy as hm\n",
    "# Batch correction options\n",
    "# The script will test later which Harmony values we should use \n",
    "if(batch_correction == \"Harmony\"):\n",
    "    print(\"Commencing harmony\")\n",
    "    if len(batch_key) == 1:\n",
    "        adata.obs['lr_batch'] = adata.obs[batch_key]\n",
    "        batch_var = \"lr_batch\"\n",
    "    else:\n",
    "        batch_var = (batch_key)\n",
    "    # Create hm subset\n",
    "    adata_hm = adata[:]\n",
    "    # Set harmony variables\n",
    "    data_mat = np.array(adata_hm.obsm[\"X_pca\"])\n",
    "    meta_data = adata_hm.obs\n",
    "#     vars_use = [batch_var]\n",
    "    # Run Harmony\n",
    "    ho = hm.run_harmony(data_mat, meta_data, batch_var,theta=theta)\n",
    "    res = (pd.DataFrame(ho.Z_corr)).T\n",
    "    res.columns = ['X{}'.format(i + 1) for i in range(res.shape[1])]\n",
    "    # Insert coordinates back into object\n",
    "    adata_hm.obsm[\"X_pca_back\"]= adata_hm.obsm[\"X_pca\"][:]\n",
    "    adata_hm.obsm[\"X_pca\"] = np.array(res)\n",
    "    # Run neighbours\n",
    "    sc.pp.neighbors(adata_hm, n_neighbors=15, n_pcs=100)\n",
    "    sc.tl.umap(adata_hm)\n",
    "    sc.tl.leiden(adata_hm,resolution = 5,key_added = 'leiden_res_3_IG')\n",
    "    adata = adata_hm[:]\n",
    "    del adata_hm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-hands",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.write('/nfs/team205/ig7/work_backups/backup_210306/projects/YS/rebuttal_figs_010922/ling_adult_macs/A1_V2_LING_ADULT_IG_annot.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-silver",
   "metadata": {},
   "outputs": [],
   "source": [
    "adatas_dict[adata_key] = '/nfs/team205/ig7/work_backups/backup_210306/projects/YS/rebuttal_figs_010922/ling_adult_macs/A1_V2_LING_ADULT_IG_annot.h5ad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-consciousness",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model == True:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    adata =  load_adatas(adatas_dict, data_merge, adata_key,QC_normalise)\n",
    "    print('adata_loaded')\n",
    "    import time\n",
    "    t0 = time.time()\n",
    "    display_cpu = DisplayCPU()\n",
    "    display_cpu.start()\n",
    "    \n",
    "    if create_sketch_data == True:\n",
    "        #sketch data\n",
    "        adata = sketch_data(adata, train_x_partition = 'X_scanvi_emb', random_state = 42,  train_labels = None)\n",
    "        \n",
    "    try:\n",
    "        model_trained = prep_training_data(feat_use = feat_use,\n",
    "        adata_temp = adata,\n",
    "        train_x_partition = train_x_partition,\n",
    "        model_key = model_key + '_lr_model',\n",
    "        batch_correction = False,\n",
    "        var_length = 7500,\n",
    "        batch_key = batch_key,\n",
    "        penalty='elasticnet', # can be [\"l1\",\"l2\",\"elasticnet\"],\n",
    "        sparcity=sparcity, #If using LR without optimisation, this controls the sparsity in model\n",
    "        max_iter = 1000, #Increase if experiencing max iter issues\n",
    "        l1_ratio = l1_ratio, #If using elasticnet without optimisation, this controls the ratio between l1 and l2)\n",
    "        partial_scale = False, #partial_scale,\n",
    "        tune_hyper_params = True # Current implementation is very expensive, intentionally made rigid for now\n",
    "        )\n",
    "        filename = model_key\n",
    "        pkl.dump(model_trained, open(filename, 'wb'))\n",
    "        models[model_key] = model_key\n",
    "    finally: #\n",
    "        current, peak = display_cpu.stop()\n",
    "        t1 = time.time()\n",
    "        time_s = t1-t0\n",
    "        print('training complete!')\n",
    "        time.sleep(3)\n",
    "        print('projection time was ' + str(time_s) + ' seconds')\n",
    "        print(f\"Current memory usage is {current / 10**6}MB; Peak was {peak / 10**6}MB\")\n",
    "        print(f\"starting memory usage is\" +'' + str(display_cpu.starting))\n",
    "        print('peak CPU % usage = '+''+ str(display_cpu.peak_cpu))\n",
    "        print('peak CPU % usage/core = '+''+ str(display_cpu.peak_cpu_per_core))\n",
    "    model_lr= model_trained\n",
    "    adata =  load_adatas(adatas_dict, data_merge, adata_key,QC_normalise)\n",
    "else:\n",
    "    adata =  load_adatas(adatas_dict, data_merge, adata_key,QC_normalise)\n",
    "    #sc.tl.umap(adata)\n",
    "    #sc.tl.leiden(adata,resolution = 3,key_added = 'leiden_res_3_IG')\n",
    "    model = load_models(models,model_key)\n",
    "    model_lr =  model\n",
    "# run with usage logger\n",
    "import time\n",
    "t0 = time.time()\n",
    "display_cpu = DisplayCPU()\n",
    "display_cpu.start()\n",
    "try: #code here ##\n",
    "    pred_out,train_x,model_lr,adata_temp = reference_projection(adata, model_lr, dyn_std,partial_scale)\n",
    "    if freq_redist != False:\n",
    "        pred_out = freq_redist_68CI(adata,freq_redist)\n",
    "        pred_out['orig_labels'] = adata.obs[freq_redist]\n",
    "        adata.obs['consensus_clus_prediction'] = pred_out['consensus_clus_prediction']\n",
    "    adata.obs['predicted'] = pred_out['predicted']\n",
    "    adata_temp.obs = adata.obs\n",
    "    \n",
    "    # Estimate top model features for class descrimination\n",
    "    feature_importance = estimate_important_features(model_lr, 100)\n",
    "    mat = feature_importance.euler_pow_mat\n",
    "    top_loadings = feature_importance.to_n_features_long\n",
    "    \n",
    "    # Estimate dataset specific feature impact\n",
    "#     for classes in ['pDC precursor_ys_HL','AEC_ys_HL']:\n",
    "#         model_class_feature_plots(top_loadings, [str(classes)], 'e^coef')\n",
    "#         plt.show()\n",
    "finally: #\n",
    "    current, peak = display_cpu.stop()\n",
    "t1 = time.time()\n",
    "time_s = t1-t0\n",
    "print('projection complete!')\n",
    "time.sleep(3)\n",
    "print('projection time was ' + str(time_s) + ' seconds')\n",
    "print(f\"Current memory usage is {current / 10**6}MB; Peak was {peak / 10**6}MB\")\n",
    "print(f\"starting memory usage is\" +'' + str(display_cpu.starting))\n",
    "print('peak CPU % usage = '+''+ str(display_cpu.peak_cpu))\n",
    "print('peak CPU % usage/core = '+''+ str(display_cpu.peak_cpu_per_core))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-astronomy",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_prediction = \"clus_prediction_confident\"\n",
    "clusters_reassign = \"leiden_res_3_IG\"\n",
    "lr_predicted_col = 'predicted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-universal",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs['confident_calls'] = pred_out['confident_calls']\n",
    "adata.obs[cluster_prediction] = adata.obs.index\n",
    "for z in adata.obs[clusters_reassign].unique():\n",
    "    df = adata.obs\n",
    "    df = df[(df[clusters_reassign].isin([z]))]\n",
    "    df_count = pd.DataFrame(df[lr_predicted_col].value_counts())\n",
    "    freq_arranged = df_count.index\n",
    "    cat = freq_arranged[0]\n",
    "    df.loc[:,cluster_prediction] = cat\n",
    "    adata.obs.loc[adata.obs[clusters_reassign] == z, [cluster_prediction]] = cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-special",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mean_probs = pred_out.loc[:, pred_out.columns != 'predicted'].groupby('orig_labels').median()\n",
    "model_mean_probs = model_mean_probs*100\n",
    "model_mean_probs = model_mean_probs.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "crs_tbl = model_mean_probs.copy()\n",
    "# Sort df columns by rows\n",
    "index_order = list(crs_tbl.max(axis=1).sort_values(ascending=False).index)\n",
    "col_order = list(crs_tbl.max(axis=0).sort_values(ascending=False).index)\n",
    "crs_tbl = crs_tbl.loc[index_order]\n",
    "crs_tbl = crs_tbl[col_order]\n",
    "# Plot_df_heatmap(crs_tbl, cmap='coolwarm', rotation=90, vmin=20, vmax=70)\n",
    "pal = sns.diverging_palette(240, 10, n=10)\n",
    "plt.figure(figsize=(20,15))\n",
    "sns.set(font_scale=0.5)\n",
    "g = sns.heatmap(crs_tbl, cmap='viridis_r',  annot=False,vmin=0, vmax=max(np.max(crs_tbl)), linewidths=1, center=max(np.max(crs_tbl))/2, square=True, cbar_kws={\"shrink\": 0.5})\n",
    "    \n",
    "plt.ylabel(\"Original labels\")\n",
    "plt.xlabel(\"Training labels\")\n",
    "plt.savefig((model_key+'_X_lr_model_means_subclusters.pdf'),dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-tracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.set_figure_params(dpi=150, dpi_save=150,figsize=[15,15],fontsize=10)\n",
    "sc.pl.umap(adata,color = ['confident_calls','clus_prediction_confident'],wspace = 0.5,size = 10)\n",
    "sc.pl.umap(adata,color = ['IG_annot','leiden_res_3_IG'],wspace = 0.5,size = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(adata,color = ['clus_prediction_confident'],legend_loc = 'on data',wspace = 0.5,size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-moral",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs['organ'] = adata.obs['organ'].str.replace('lung','Lung')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "df = pd.DataFrame(adata.obs[adata.obs['clus_prediction_confident'].str.contains('mac')].groupby(['organ','clus_prediction_confident']).apply(len)).reset_index()\n",
    "\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-liechtenstein",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-sellers",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs['kit'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(adata[adata.obs['organ'].isin(['Lung','lung'])], color = 'kit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-rapid",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(adata,color = ['organ'],groups = ['Lung','lung'],legend_loc = 'on data',wspace = 0.5,size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-booking",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs[adata.obs['IG_annot'].str.contains('OSTEO')].groupby(['organ']).apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-vertical",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs.to_csv('./v2_hm_projected_immune_atlas_ling_meta.csv')\n",
    "pred_out.to_csv('./v2_hm_pred_out_projected_immune_atlas_ling.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-coach",
   "metadata": {},
   "source": [
    "# load original umap and check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-premium",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_umap = sc.read('/nfs/team205/ig7/work_backups/backup_210306/projects/YS/rebuttal_figs_010922/ling_adult_macs/A1_V1_LING_ADULT_IG_annot.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-oracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_umap.obs['clus_prediction_confident'] = adata.obs['clus_prediction_confident']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-chassis",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(adata_umap,color = ['kit','leiden_scVI','clus_prediction_confident'],legend_loc = 'on data',wspace = 0.5,size = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-marathon",
   "metadata": {},
   "source": [
    "### Remove clusters 3 and 9 from data due to batch-effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-maximum",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obsm = adata_umap.obsm\n",
    "adata = adata[~adata.obs['leiden_scVI'].isin(['3','9'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.write('/nfs/team205/ig7/work_backups/backup_210306/projects/YS/rebuttal_figs_010922/ling_adult_macs/A1_V3_LING_ADULT_IG_annot.h5ad')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-concord",
   "metadata": {},
   "source": [
    "### Add brain and Lung annots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-humidity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-gabriel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-focus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-immigration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-sequence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-disabled",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-renaissance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-glossary",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "sc.pl.umap(adata,color = ['clus_prediction_confident'],wspace = 0.1,save = 'Lung_atlas_projection')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-salvation",
   "metadata": {},
   "source": [
    "# View by median probabilities per classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-occupation",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mean_probs = pred_out.loc[:, pred_out.columns != 'predicted'].groupby('orig_labels').median()\n",
    "model_mean_probs = model_mean_probs*100\n",
    "model_mean_probs = model_mean_probs.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "crs_tbl = model_mean_probs.copy()\n",
    "# Sort df columns by rows\n",
    "index_order = list(crs_tbl.max(axis=1).sort_values(ascending=False).index)\n",
    "col_order = list(crs_tbl.max(axis=0).sort_values(ascending=False).index)\n",
    "crs_tbl = crs_tbl.loc[index_order]\n",
    "crs_tbl = crs_tbl[col_order]\n",
    "# Plot_df_heatmap(crs_tbl, cmap='coolwarm', rotation=90, vmin=20, vmax=70)\n",
    "pal = sns.diverging_palette(240, 10, n=10)\n",
    "plt.figure(figsize=(20,15))\n",
    "sns.set(font_scale=0.5)\n",
    "g = sns.heatmap(crs_tbl, cmap='viridis_r',  annot=False,vmin=0, vmax=max(np.max(crs_tbl)), linewidths=1, center=max(np.max(crs_tbl))/2, square=True, cbar_kws={\"shrink\": 0.5})\n",
    "\n",
    "plt.ylabel(\"Original labels\")\n",
    "plt.xlabel(\"Training labels\")\n",
    "plt.savefig((model_key+'_X_lr_model_means_subclusters.pdf'),dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-compound",
   "metadata": {},
   "source": [
    "# View by label assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=feat_use\n",
    "y = 'predicted'\n",
    "\n",
    "y_attr = adata_temp.obs[y]\n",
    "x_attr = adata_temp.obs[x]\n",
    "crs = pd.crosstab(x_attr, y_attr)\n",
    "crs_tbl = crs\n",
    "for col in crs_tbl :\n",
    "    crs_tbl[col] = crs_tbl[col].div(crs_tbl[col].sum(axis=0)).multiply(100).round(2)\n",
    "    \n",
    "index_order = list(crs_tbl.max(axis=1).sort_values(ascending=False).index)\n",
    "col_order = list(crs_tbl.max(axis=0).sort_values(ascending=False).index)\n",
    "crs_tbl = crs_tbl.loc[index_order]\n",
    "crs_tbl = crs_tbl[col_order]\n",
    "\n",
    "#plot_df_heatmap(crs_tbl, cmap='coolwarm', rotation=90, vmin=20, vmax=70)\n",
    "pal = sns.diverging_palette(240, 10, n=10)\n",
    "plt.figure(figsize=(20,15))\n",
    "sns.set(font_scale=0.8)\n",
    "g = sns.heatmap(crs_tbl, cmap='viridis_r', vmin=0, vmax=100, linewidths=1, center=50, square=True, cbar_kws={\"shrink\": 0.3})\n",
    "plt.xlabel(\"Original labels\")\n",
    "plt.ylabel(\"Predicted labels\")\n",
    "# plt.savefig(save_path + \"/LR_predictions_consensus.pdf\")\n",
    "# crs_tbl.to_csv(save_path + \"/post-freq_LR_predictions_consensus_supp_table.csv\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-california",
   "metadata": {},
   "outputs": [],
   "source": [
    "x='consensus_clus_prediction'\n",
    "y = 'predicted'\n",
    "\n",
    "y_attr = adata_temp.obs[y]\n",
    "x_attr = adata_temp.obs[x]\n",
    "crs = pd.crosstab(x_attr, y_attr)\n",
    "crs_tbl = crs\n",
    "for col in crs_tbl :\n",
    "    crs_tbl[col] = crs_tbl[col].div(crs_tbl[col].sum(axis=0)).multiply(100).round(2)\n",
    "    \n",
    "index_order = list(crs_tbl.max(axis=1).sort_values(ascending=False).index)\n",
    "col_order = list(crs_tbl.max(axis=0).sort_values(ascending=False).index)\n",
    "crs_tbl = crs_tbl.loc[index_order]\n",
    "crs_tbl = crs_tbl[col_order]\n",
    "\n",
    "#plot_df_heatmap(crs_tbl, cmap='coolwarm', rotation=90, vmin=20, vmax=70)\n",
    "pal = sns.diverging_palette(240, 10, n=10)\n",
    "plt.figure(figsize=(20,15))\n",
    "sns.set(font_scale=0.8)\n",
    "g = sns.heatmap(crs_tbl, cmap='viridis_r', vmin=0, vmax=100, linewidths=1, center=50, square=True, cbar_kws={\"shrink\": 0.3})\n",
    "plt.xlabel(\"Original labels\")\n",
    "plt.ylabel(\"Predicted labels\")\n",
    "plt.savefig((model_key+'_X_lr_model_means_subclusters.pdf'),dpi=300)\n",
    "# crs_tbl.to_csv(save_path + \"/post-freq_LR_predictions_consensus_supp_table.csv\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-directive",
   "metadata": {},
   "source": [
    "# Save predicted output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-antenna",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_out.to_csv('./A1_V1_LUNG_LUNG_adult_pred_outs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-prototype",
   "metadata": {},
   "source": [
    "# Update the integrated macs data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-liverpool",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read('/nfs/team205/ig7/work_backups/backup_210306/projects/YS/rebuttal_figs_010922/ling_adult_macs/A2_V1_130323_integrated_macs_adult_pan_organ_scored.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-acceptance",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pred_out[['predicted','consensus_clus_prediction']]\n",
    "preds.groupby(['consensus_clus_prediction']).apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-footwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.loc[~preds['consensus_clus_prediction'].isin(['Interstitial macrophages']),'consensus_clus_prediction'] = 'Alveolar macrophages'\n",
    "preds.groupby(['consensus_clus_prediction']).apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-picture",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds['LVL3'] = preds['consensus_clus_prediction']\n",
    "preds['LVL3'] = preds['LVL3'].str.replace('Alveolar macrophages','MACROPHAGES_ALVEOLAR')\n",
    "preds['LVL3'] = preds['LVL3'].str.replace('Interstitial macrophages','MACROPHAGES_INTERSTITIAL')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-assumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "# push into main\n",
    "tlf_indx = (adata.obs.loc[(~adata.obs['LVL4'].isin(['MACROPHAGE_pre_agm_hi'])) & (adata.obs.index.isin(preds.index))].index)\n",
    "preds['LVL4'] = preds['LVL3'][:]\n",
    "preds.loc[preds.index.isin(tlf_indx),'LVL4'] = 'MACROPHAGE_pre_agm_hi'\n",
    "\n",
    "adata.obs['LVL3'] = adata.obs['LVL3'].astype(str)\n",
    "adata.obs['LVL4'] = adata.obs['LVL4'].astype(str)\n",
    "adata.obs.loc[(adata.obs.index.isin(preds.index)),'LVL3'] = preds['LVL3'].astype(str)\n",
    "adata.obs.loc[(adata.obs.index.isin(preds.index)),'LVL4'] = preds['LVL4'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(adata,color = ['LVL3','LVL4'],wspace = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-premises",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = [\n",
    "'MACROPHAGE_ERY',\n",
    "'MACROPHAGE_KUPFFER_LIKE',\n",
    "'MACROPHAGE_LYVE1_HIGH',\n",
    "'MACROPHAGE_MHCII_HIGH',\n",
    "'MACROPHAGES_ALVEOLAR',\n",
    "'MACROPHAGES_INTERSTITIAL',\n",
    "'MACROPHAGE_BAMS',\n",
    "'MACROPHAGE_MICROGLIA',\n",
    "'MACROPHAGE_PERI',\n",
    "'MACROPHAGE_PROLIFERATING',\n",
    "'OSTEOCLAST']\n",
    "cols = [\n",
    "'#DB3432',\n",
    "'#FFFFFF',\n",
    "'#CEDE78',\n",
    "'#E38C4C',\n",
    "'#BD9D93',\n",
    "'#9C8897',\n",
    "'#EDCD52',\n",
    "'#4C7397',\n",
    "'#9EAFB7',\n",
    "'#EB8F50',\n",
    "'#DF5251']\n",
    "col_pal = dict(zip(cells,cols))\n",
    "adata.obs['LVL3'] = adata.obs['LVL3'].astype('category').cat.reorder_categories(cells) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-savage",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "var = \"LVL3\"\n",
    "#Create color dictionary_cell\n",
    "adata.obs[var] = adata.obs[var].astype('category')\n",
    "cells = list(adata.obs[var].cat.categories)\n",
    "col = list(range(0, len(adata.obs[var].cat.categories)))\n",
    "#col = adata_mac.uns['cell.labels_colors']\n",
    "dic = dict(zip(cells,col))\n",
    "\n",
    "#Create a mappable field\n",
    "adata.obs['num'] = adata.obs[var].astype(str)\n",
    "#map to adata_mac.obs.col to create a caterorical column\n",
    "adata.obs['num'] = adata.obs['num'].map(dic)\n",
    "\n",
    "##Map to a pallete to use with umap\n",
    "#cells_list = pd.DataFrame(adata_mac.obs[\"cell.labels\"].cat.categories)\n",
    "#cells_list['col'] = cells_list[0].map(dic)\n",
    "#col_pal = list(cells_list['col'])\n",
    "adata.obs['num'] = adata.obs['num'].astype(str)\n",
    "adata.obs[var+'_num'] = adata.obs['num'].astype(str) + \" : \" + adata.obs[var].astype(str) #FF4A46\n",
    "\n",
    "# col_pal = ['#94BFB1',     '#B49EC8',    '#E0EE70',    '#EE943E',    '#4C7BAB',    '#E78AB8',    '#AFBFCC',   \"#FF4A46\", '#FF993F',    \"#FFFF00\", \"#1CE6FF\", \"#FF34FF\", \"#FF4A46\", \"#008941\", \"#006FA6\", \"#A30059\",    \"#FFDBE5\", \"#7A4900\", \"#0000A6\", \"#63FFAC\", \"#B79762\", \"#004D43\", \"#8FB0FF\", \"#997D87\",    \"#5A0007\", \"#809693\", \"#6A3A4C\", \"#1B4400\", \"#4FC601\", \"#3B5DFF\"]\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcdefaults() #Reset matplot lb deafults as seaborne tends to mess with this\n",
    "fig, (ax1, ax2,) = plt.subplots(1,2, figsize=(10,10), gridspec_kw={'wspace':0,'width_ratios': [1,0]})\n",
    "p2 = sc.pl.umap(adata, color = (var+'_num') ,ax=ax2,show=False,title=\"\", palette= cols) #title=i\n",
    "p3 = sc.pl.umap(adata, color = \"num\",legend_loc=\"on data\",size=10,legend_fontsize='small',ax=ax1,show=False,title=\"Macs_adult\", palette= cols) #title=i\n",
    "\n",
    "fig.savefig('./'+var+\"_mac_num.pdf\",bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-humidity",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.write('/nfs/team205/ig7/work_backups/backup_210306/projects/YS/rebuttal_figs_010922/ling_adult_macs/A2_V2_130323_integrated_macs_adult_pan_organ_scored.h5ad')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-stock",
   "metadata": {},
   "source": [
    "# Derive a new space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-plain",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_key = 'organ'\n",
    "theta = 3\n",
    "\n",
    "sc.pp.highly_variable_genes(adata, batch_key = batch_key, subset=False)\n",
    "sc.pp.pca(adata, n_comps=100, use_highly_variable=True, svd_solver='arpack')\n",
    "sc.pl.pca_variance_ratio(adata, log=True,n_pcs=100)\n",
    "\n",
    "import harmonypy as hm\n",
    "# Batch correction options\n",
    "# The script will test later which Harmony values we should use \n",
    "print(\"Commencing harmony\")\n",
    "adata.obs['lr_batch'] = adata.obs[batch_key]\n",
    "batch_var = \"lr_batch\"\n",
    "# Create hm subset\n",
    "adata_hm = adata[:]\n",
    "# Set harmony variables\n",
    "data_mat = np.array(adata_hm.obsm[\"X_pca\"])\n",
    "meta_data = adata_hm.obs\n",
    "vars_use = [batch_var]\n",
    "# Run Harmony\n",
    "ho = hm.run_harmony(data_mat, meta_data, vars_use,theta=theta)\n",
    "res = (pd.DataFrame(ho.Z_corr)).T\n",
    "res.columns = ['X{}'.format(i + 1) for i in range(res.shape[1])]\n",
    "# Insert coordinates back into object\n",
    "adata_hm.obsm[\"X_pca_back\"]= adata_hm.obsm[\"X_pca\"][:]\n",
    "adata_hm.obsm[\"X_pca\"] = np.array(res)\n",
    "adata = adata_hm[:]\n",
    "del adata_hm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-verification",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.neighbors(adata,use_rep = 'X_pca')\n",
    "sc.tl.umap(adata)\n",
    "sc.pl.umap(adata,color = ['LVL3','LVL4'],wspace = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.write('/nfs/team205/ig7/work_backups/backup_210306/projects/YS/rebuttal_figs_010922/ling_adult_macs/A2_V3_HM_130323_integrated_macs_adult_pan_organ_scored.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-burner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # filter unlikely predictions\n",
    "# filtered = pred_out[np.max(pred_out.loc[:,~pred_out.columns.isin(['predicted','confident_calls','annot_celltype', 'consensus_clus_prediction', 'orig_labels','clus_prediction_confident'])],axis = 1) > 0.3]\n",
    "# adata_temp = adata[adata.obs.index.isin(filtered.index)]\n",
    "# filtered['clus_prediction_confident'] = adata_temp.obs['clus_prediction_confident']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-amsterdam",
   "metadata": {},
   "source": [
    " # Significant contributors to feature effect size per class of model\n",
    "     - Bear in mind these are only top features..\n",
    "    - assess the positive descriminators (markers) of the model\n",
    "    - â€œâ€¦provide information about the magnitude and direction of the difference between two groups or the relationship between two variables.â€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-husband",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(top_loadings['class'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-benjamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-things",
   "metadata": {},
   "outputs": [],
   "source": [
    "for classes in list(top_loadings['class'].unique()):\n",
    "    try:\n",
    "        model_class_feature_plots(top_loadings, [str(classes)], 'e^coef')\n",
    "    except:\n",
    "        skip = classes\n",
    "        print('No significant features detected, skipped {}'.format(skip))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-equipment",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "# plot_states = ['Tip cell (arterial)','HSC','SPP1+ proliferating neuron proneitors']\n",
    "markers = top_loadings[top_loadings['class'].isin(adata_temp.obs['consensus_clus_prediction'])].groupby(['class']).head(5).groupby(['class'])['feature'].agg(lambda grp: list(grp)).to_dict()\n",
    "sc.pl.dotplot(adata_temp, groupby = 'consensus_clus_prediction', var_names = markers,standard_scale='var')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-currency",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_loadings[top_loadings['class'].isin(['Lymphoid progenitor','Early erythroid (embryonic)','Pre-dermal condensate'])].groupby(['class']).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-hearts",
   "metadata": {},
   "source": [
    "# Label confidence scoring\n",
    "- Let's study label stability given K-neighborhood assignments\n",
    "\n",
    "**Author notes:** \n",
    "-  Hey! If you're reading this, I've probably messed up somewhere and you're looking for an explanation why :) \n",
    "- Code blocks marked **Prototype** are usually incomplete or a irresponsible lift from another pipeline, if the source pipeline is already distributed/published, I will leave git links associated with the module.\n",
    "- If there are no links, there should be some run notes\n",
    "\n",
    "**Run mode 2 of prototype $alpha$ $beta$ sampling via leverage-score**\n",
    "- Mode 2 was chosen as we want to define a sampling space which satisfies same KNN distribution and density instead of prioritising variability\n",
    "- Neighborhood assignment is done via majority voting\n",
    "- Posterior probability computed and sampling rate for X is determined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-rebel",
   "metadata": {},
   "source": [
    "# Running mode 2 of prototype alpha-beta sampling via leverage-score instead of NUTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-fetish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prior probability\n",
    "# define liklihood of type 1 error (FP)\n",
    "# define liklihood o type 2 error (FN)\n",
    "# For given geosketch, what is the error rate and posterior prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-parliament",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-original",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fossil-algorithm",
   "metadata": {},
   "source": [
    "# Vertice calling and association array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-feedback",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vertice association matrix\n",
    "adata.obsm['nhoods']\n",
    "# Let's now count number of cell states given a sampled neighbourhood\n",
    "nn_membership_mt = pd.DataFrame(adata.uns[\"nhood_adata\"].X.todense(),index =adata.uns[\"nhood_adata\"].obs.index,columns = adata.uns[\"nhood_adata\"].var.index )\n",
    "# Are there annotation fields with less than a single neighborhood rep?\n",
    "adata.uns[\"nhood_adata\"].obs['membership'] = nn_membership_mt.idxmax(axis = 1)\n",
    "# let's get the binarised relationships of all cells to neighborhoods\n",
    "knn_membership_mt = pd.DataFrame(adata.obsm['nhoods'].todense(),index = adata.obs.index, columns = adata.uns[\"nhood_adata\"].obs.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-maintenance",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_membership_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-zimbabwe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# areas which are under represented require re-sampling\n",
    "np.max(nn_membership_mt['ASDC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-melbourne",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_membership_mt['ASDC'][nn_membership_mt['ASDC']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-swift",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.uns[\"nhood_adata\"].obs['membership']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-phone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is there a minimum of three neighborhoods per cell state?\n",
    "undersampled = adata.uns[\"nhood_adata\"].obs.groupby(['membership']).apply(len)[adata.uns[\"nhood_adata\"].obs.groupby(['membership']).apply(len)<3]\n",
    "\n",
    "# which neighborhoods have these labels been lost to?\n",
    "lost = list(set(list(adata.uns[\"nhood_adata\"].obs['membership'].unique())) ^ set(list(adata.obs[feat_use].unique())))\n",
    "\n",
    "# How many cells are in these neighborhoods?\n",
    "adata.uns[\"nhood_adata\"].obs['count'] = knn_membership_mt.T.sum(axis = 1).values\n",
    "adata.uns[\"nhood_adata\"].obs[adata.uns[\"nhood_adata\"].obs['membership'].isin(undersampled.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-sender",
   "metadata": {},
   "outputs": [],
   "source": [
    "lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-village",
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-bracket",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_membership_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-cotton",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-calgary",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-object",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_out.to_csv('A1_V2_X_LR_pred_out_SK_to_VASC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-group",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_temp.obs.groupby(['consensus_clus_prediction']).apply(len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workhorse",
   "language": "python",
   "name": "workhorse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
